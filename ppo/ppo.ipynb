{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim), #output dim should be 1\n",
    "            #No Activation function here for now\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "env = gym.make(\"LunarLander-v3\", continuous = False)\n",
    "n_episodes = 1\n",
    "discount_factor = 0.98\n",
    "\n",
    "actor = Actor(8, 4)\n",
    "critic = Critic(8, 1)\n",
    "\n",
    "actor_lr = 0.0003\n",
    "critic_lr = 0.001\n",
    "optimizer_actor = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "optimizer_critic = torch.optim.Adam(actor.parameters(), lr=critic_lr)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "total_rewards = [] \n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    all_rewards = []\n",
    "    log_probs = []\n",
    "    all_values = []\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        action_probs = critic(state)\n",
    "        distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action)\n",
    "        \n",
    "        new_state, reward, done, truncated, _ = env.step(action=action.item())\n",
    "        \n",
    "        value = critic(state)\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        all_rewards.append(reward)\n",
    "        all_values.append(value)\n",
    "        \n",
    "        done = done or truncated\n",
    "        state = new_state\n",
    "\n",
    "    log_probs = torch.stack(log_probs)   \n",
    "    all_values = torch.cat(all_values)\n",
    "    \n",
    "    returns = []\n",
    "    G = 0\n",
    "    for i in reversed(all_rewards):\n",
    "        G = G + discount_factor*i\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(0)\n",
    "    returns = (returns - returns.mean())/ returns.std() + 1e-8\n",
    "    \n",
    "    advantages = returns - all_values\n",
    "    \n",
    "    #update critic\n",
    "    critic_loss = mse(all_values, returns)\n",
    "    optimizer_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    \n",
    "    # $update actor\n",
    "    actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "    optimizer_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward: -53.56\n",
      "Episode 10: Total Reward: -440.19\n",
      "Episode 20: Total Reward: -538.58\n",
      "Episode 30: Total Reward: -762.56\n",
      "Episode 40: Total Reward: -568.47\n",
      "Episode 50: Total Reward: -501.54\n",
      "Episode 60: Total Reward: -335.93\n",
      "Episode 70: Total Reward: -747.09\n",
      "Episode 80: Total Reward: -746.44\n",
      "Episode 90: Total Reward: -863.89\n",
      "Episode 100: Total Reward: -535.01\n",
      "Episode 110: Total Reward: -975.53\n",
      "Episode 120: Total Reward: -567.90\n",
      "Episode 130: Total Reward: -801.51\n",
      "Episode 140: Total Reward: -515.98\n",
      "Episode 150: Total Reward: -731.27\n",
      "Episode 160: Total Reward: -368.20\n",
      "Episode 170: Total Reward: -443.80\n",
      "Episode 180: Total Reward: -784.04\n",
      "Episode 190: Total Reward: -494.62\n",
      "Episode 200: Total Reward: -544.09\n",
      "Episode 210: Total Reward: -778.83\n",
      "Episode 220: Total Reward: -497.38\n",
      "Episode 230: Total Reward: -627.00\n",
      "Episode 240: Total Reward: -377.14\n",
      "Episode 250: Total Reward: -730.08\n",
      "Episode 260: Total Reward: -390.58\n",
      "Episode 270: Total Reward: -876.44\n",
      "Episode 280: Total Reward: -619.76\n",
      "Episode 290: Total Reward: -584.06\n",
      "Episode 300: Total Reward: -521.95\n",
      "Episode 310: Total Reward: -505.86\n",
      "Episode 320: Total Reward: -429.96\n",
      "Episode 330: Total Reward: -411.34\n",
      "Episode 340: Total Reward: -689.40\n",
      "Episode 350: Total Reward: -369.08\n",
      "Episode 360: Total Reward: -678.98\n",
      "Episode 370: Total Reward: -662.77\n",
      "Episode 380: Total Reward: -631.04\n",
      "Episode 390: Total Reward: -840.09\n",
      "Episode 400: Total Reward: -635.82\n",
      "Episode 410: Total Reward: -656.62\n",
      "Episode 420: Total Reward: -528.74\n",
      "Episode 430: Total Reward: -765.11\n",
      "Episode 440: Total Reward: -678.80\n",
      "Episode 450: Total Reward: -334.63\n",
      "Episode 460: Total Reward: -538.85\n",
      "Episode 470: Total Reward: -532.87\n",
      "Episode 480: Total Reward: -368.93\n",
      "Episode 490: Total Reward: -346.13\n",
      "Episode 500: Total Reward: -746.82\n",
      "Episode 510: Total Reward: -569.29\n",
      "Episode 520: Total Reward: -476.55\n",
      "Episode 530: Total Reward: -431.88\n",
      "Episode 540: Total Reward: -379.65\n",
      "Episode 550: Total Reward: -728.03\n",
      "Episode 560: Total Reward: -587.59\n",
      "Episode 570: Total Reward: -398.04\n",
      "Episode 580: Total Reward: -558.32\n",
      "Episode 590: Total Reward: -808.05\n",
      "Episode 600: Total Reward: -325.02\n",
      "Episode 610: Total Reward: -410.12\n",
      "Episode 620: Total Reward: -877.79\n",
      "Episode 630: Total Reward: -421.75\n",
      "Episode 640: Total Reward: -475.57\n",
      "Episode 650: Total Reward: -651.91\n",
      "Episode 660: Total Reward: -459.02\n",
      "Episode 670: Total Reward: -632.87\n",
      "Episode 680: Total Reward: -337.55\n",
      "Episode 690: Total Reward: -769.87\n",
      "Episode 700: Total Reward: -1148.67\n",
      "Episode 710: Total Reward: -732.02\n",
      "Episode 720: Total Reward: -399.45\n",
      "Episode 730: Total Reward: -791.56\n",
      "Episode 740: Total Reward: -761.92\n",
      "Episode 750: Total Reward: -521.94\n",
      "Episode 760: Total Reward: -422.07\n",
      "Episode 770: Total Reward: -638.48\n",
      "Episode 780: Total Reward: -518.42\n",
      "Episode 790: Total Reward: -922.91\n",
      "Episode 800: Total Reward: -492.73\n",
      "Episode 810: Total Reward: -546.94\n",
      "Episode 820: Total Reward: -626.26\n",
      "Episode 830: Total Reward: -585.40\n",
      "Episode 840: Total Reward: -436.66\n",
      "Episode 850: Total Reward: -734.20\n",
      "Episode 860: Total Reward: -598.46\n",
      "Episode 870: Total Reward: -785.89\n",
      "Episode 880: Total Reward: -388.28\n",
      "Episode 890: Total Reward: -787.70\n",
      "Episode 900: Total Reward: -458.94\n",
      "Episode 910: Total Reward: -911.44\n",
      "Episode 920: Total Reward: -814.66\n",
      "Episode 930: Total Reward: -713.02\n",
      "Episode 940: Total Reward: -871.10\n",
      "Episode 950: Total Reward: -392.24\n",
      "Episode 960: Total Reward: -761.64\n",
      "Episode 970: Total Reward: -904.59\n",
      "Episode 980: Total Reward: -570.59\n",
      "Episode 990: Total Reward: -424.49\n",
      "Episode 1000: Total Reward: -436.38\n",
      "Episode 1010: Total Reward: -378.55\n",
      "Episode 1020: Total Reward: -391.86\n",
      "Episode 1030: Total Reward: -440.95\n",
      "Episode 1040: Total Reward: -488.96\n",
      "Episode 1050: Total Reward: -517.60\n",
      "Episode 1060: Total Reward: -566.32\n",
      "Episode 1070: Total Reward: -1051.88\n",
      "Episode 1080: Total Reward: -401.91\n",
      "Episode 1090: Total Reward: -469.34\n",
      "Episode 1100: Total Reward: -378.33\n",
      "Episode 1110: Total Reward: -762.73\n",
      "Episode 1120: Total Reward: -867.69\n",
      "Episode 1130: Total Reward: -783.18\n",
      "Episode 1140: Total Reward: -975.87\n",
      "Episode 1150: Total Reward: -838.36\n",
      "Episode 1160: Total Reward: -455.09\n",
      "Episode 1170: Total Reward: -432.24\n",
      "Episode 1180: Total Reward: -747.51\n",
      "Episode 1190: Total Reward: -407.02\n",
      "Episode 1200: Total Reward: -389.65\n",
      "Episode 1210: Total Reward: -715.08\n",
      "Episode 1220: Total Reward: -416.91\n",
      "Episode 1230: Total Reward: -518.00\n",
      "Episode 1240: Total Reward: -1213.67\n",
      "Episode 1250: Total Reward: -502.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m actor(state_tensor)\n\u001b[0;32m     86\u001b[0m value \u001b[38;5;241m=\u001b[39m critic(state_tensor)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m---> 88\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     90\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\SHIVAM\\Desktop\\ReinforcementLearning\\rlenv\\lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHIVAM\\Desktop\\ReinforcementLearning\\rlenv\\lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     68\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 69\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SHIVAM\\Desktop\\ReinforcementLearning\\rlenv\\lib\\site-packages\\torch\\distributions\\constraints.py:464\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(value \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Chat-gpt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.2  # clipping range for PPO\n",
    "ACTOR_LR = 3e-4\n",
    "CRITIC_LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Actor Network (Policy)\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "# Critic Network (Value Function)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output is a single state value\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "# Function to compute advantages using GAE\n",
    "def compute_advantages(rewards, values, gamma=GAMMA):\n",
    "    \"\"\"\n",
    "    Iska explanation notes.ipynb me snippets dala hai\n",
    "    and\n",
    "    Here lambda is not there, It can be multiplied with : \n",
    "    gamma and last_advantage for BIAS & VARIANCE Trade off\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    last_advantage = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] - values[t] #also known as one step TD Error : δt^V​=rt​+γV(st+1​)−V(st​)\n",
    "        last_advantage = delta + gamma * last_advantage #A(t)GAE(γ,λ)​ = δt^V ​+ γ * λ * A(t+1)GAE(γ,λ)​ \n",
    "        advantages.insert(0, last_advantage)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    advantages = (advantages - advantages.mean())/advantages.std() + 1e+8\n",
    "    return advantages\n",
    "\n",
    "# PPO Training Loop\n",
    "for episode in range(100000):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    log_probs, values, rewards = [], [], []\n",
    "    states, actions = [], []\n",
    "    \n",
    "    # Collecting trajectories\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_probs = actor(state_tensor)\n",
    "        value = critic(state_tensor).squeeze()\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        new_state, reward, done, truncated, _ = env.step(action.item())\n",
    "        done = done or truncated\n",
    "        \n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "    # Compute returns and advantages\n",
    "    values.append(torch.tensor(0.0))  # Bootstrap last value\n",
    "    returns = compute_advantages(rewards, values)\n",
    "    values = torch.stack(values[:-1])\n",
    "    advantages = returns - values\n",
    "    \n",
    "    # PPO update\n",
    "    for _ in range(EPOCHS):\n",
    "        for i in range(0, len(states), BATCH_SIZE):\n",
    "            batch_states = torch.cat(states[i:i + BATCH_SIZE])\n",
    "            batch_actions = torch.stack(actions[i:i + BATCH_SIZE])\n",
    "            batch_log_probs = torch.stack(log_probs[i:i + BATCH_SIZE])\n",
    "            batch_advantages = advantages[i:i + BATCH_SIZE].detach()\n",
    "            batch_returns = returns[i:i + BATCH_SIZE].detach()\n",
    "            \n",
    "            # Compute new action probabilities\n",
    "            new_action_probs = actor(batch_states)\n",
    "            new_dist = Categorical(new_action_probs)\n",
    "            new_log_probs = new_dist.log_prob(batch_actions)\n",
    "            \n",
    "            # Compute probability ratio\n",
    "            ratio = torch.exp(new_log_probs - batch_log_probs.detach())\n",
    "            \n",
    "            # Clipped policy loss\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON)\n",
    "            actor_loss = -torch.min(ratio * batch_advantages, clipped_ratio * batch_advantages).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_preds = critic(batch_states).squeeze()\n",
    "            critic_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "            \n",
    "            # Update actor\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "            \n",
    "            # Update critic\n",
    "            optimizer_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            optimizer_critic.step()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Total Reward: {sum(rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPOAgent' object has no attribute 'get_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 261\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    260\u001b[0m     env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLander-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 261\u001b[0m     agent, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     evaluate(env_name, agent, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[17], line 189\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[1;34m(env_name, num_episodes, update_timestep, max_timesteps)\u001b[0m\n\u001b[0;32m    185\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_timesteps):\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# Select action\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m     action, action_log_prob, state_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m(state)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# Take action in environment\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PPOAgent' object has no attribute 'get_action'"
     ]
    }
   ],
   "source": [
    "# Chat-gpt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# PPO Network Architecture\n",
    "class PPONetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPONetwork, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Policy head\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Value head\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.feature_extractor(state)\n",
    "        action_probs = F.softmax(self.policy(features), dim=-1)\n",
    "        state_value = self.value(features)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "    \n",
    "    def get_action(self, state, action=None):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action_probs, state_value = self.forward(state)\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        action_log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action.cpu().numpy(), action_log_prob, state_value, entropy\n",
    "\n",
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.0003, gamma=0.99, clip_ratio=0.2, value_coef=0.5, entropy_coef=0.01):\n",
    "        self.network = PPONetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.action_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def store_transition(self, state, action, action_log_prob, reward, state_value, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.action_log_probs.append(action_log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.state_values.append(state_value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.action_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def compute_returns(self, next_value):\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        next_val = next_value\n",
    "        \n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            delta = self.rewards[i] + self.gamma * next_val * (1 - self.dones[i]) - self.state_values[i]\n",
    "            gae = delta + self.gamma * 0.95 * (1 - self.dones[i]) * gae\n",
    "            returns.insert(0, gae + self.state_values[i])\n",
    "            next_val = self.state_values[i]\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update(self, next_value, epochs=10, batch_size=64):\n",
    "        returns = self.compute_returns(next_value)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(self.states)).to(device)\n",
    "        actions = torch.LongTensor(np.array(self.actions)).to(device)\n",
    "        old_action_log_probs = torch.FloatTensor(np.array(self.action_log_probs)).detach().to(device)\n",
    "        returns = torch.FloatTensor(np.array(returns)).to(device)\n",
    "        values = torch.FloatTensor(np.array(self.state_values)).detach().to(device)\n",
    "        \n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        \n",
    "        dataset_size = len(states)\n",
    "        indices = np.arange(dataset_size)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, dataset_size, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, dataset_size)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_action_log_probs[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                \n",
    "                # Forward pass\n",
    "                action_probs, state_values = self.network(batch_states)\n",
    "                dist = Categorical(action_probs)\n",
    "                action_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Calculate ratios and surrogate objectives\n",
    "                ratios = torch.exp(action_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(state_values.squeeze(-1), batch_returns)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                total_entropy += entropy.item()\n",
    "        \n",
    "        self.clear_memory()\n",
    "        \n",
    "        return total_policy_loss / (dataset_size // batch_size), total_value_loss / (dataset_size // batch_size), total_entropy / (dataset_size // batch_size)\n",
    "\n",
    "# Training function\n",
    "def train_ppo(env_name, num_episodes=500, update_timestep=2000, max_timesteps=500):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = PPOAgent(state_dim, action_dim)\n",
    "    \n",
    "    # Tracking variables\n",
    "    running_reward = 0\n",
    "    avg_rewards = []\n",
    "    timestep = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            # Select action\n",
    "            action, action_log_prob, state_value, _ = agent.get_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, action_log_prob.item(), reward, state_value.item(), done)\n",
    "            \n",
    "            # Update running reward\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            timestep += 1\n",
    "            \n",
    "            # Update PPO agent\n",
    "            if timestep % update_timestep == 0:\n",
    "                _, next_value, _ = agent.get_action(next_state)[1:]\n",
    "                policy_loss, value_loss, entropy = agent.update(next_value.item())\n",
    "                print(f\"Timestep: {timestep}, Policy Loss: {policy_loss:.4f}, Value Loss: {value_loss:.4f}, Entropy: {entropy:.4f}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Calculate running reward\n",
    "        running_reward = 0.05 * episode_reward + 0.95 * running_reward\n",
    "        avg_rewards.append(running_reward)\n",
    "        \n",
    "        # Print log\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {episode_reward:.2f}, Avg Reward: {running_reward:.2f}\")\n",
    "        \n",
    "        # If solved\n",
    "        if running_reward > 200:\n",
    "            print(f\"Solved at episode {episode}!\")\n",
    "            break\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(agent.network.state_dict(), f\"ppo_{env_name}.pth\")\n",
    "    \n",
    "    return agent, avg_rewards\n",
    "\n",
    "# Function to evaluate the trained agent\n",
    "def evaluate(env_name, agent, num_episodes=10, render=False):\n",
    "    env = gym.make(env_name, render_mode=\"human\" if render else None)\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)[0]\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    avg_reward = sum(rewards) / len(rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {avg_reward:.2f}\")\n",
    "    \n",
    "    return avg_reward\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"LunarLander-v3\"\n",
    "    agent, rewards = train_ppo(env_name)\n",
    "    evaluate(env_name, agent, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
