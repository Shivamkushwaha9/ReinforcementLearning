{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 144, Reward: 239.7318, Avg Reward: 123.68\n",
      "Episode 145, Reward: 159.3239, Avg Reward: 124.49\n",
      "Episode 146, Reward: 110.4353, Avg Reward: 128.46\n",
      "Episode 147, Reward: 129.7034, Avg Reward: 129.16\n",
      "Episode 148, Reward: 217.7232, Avg Reward: 129.26\n",
      "Episode 149, Reward: 209.7757, Avg Reward: 134.41\n",
      "Episode 150, Reward: 130.7336, Avg Reward: 136.57\n",
      "Episode 151, Reward: 228.5597, Avg Reward: 141.52\n",
      "Episode 152, Reward: 118.6351, Avg Reward: 141.56\n",
      "Episode 153, Reward: 37.3959, Avg Reward: 141.28\n",
      "Episode 154, Reward: 158.4690, Avg Reward: 143.60\n",
      "Episode 155, Reward: 156.8718, Avg Reward: 144.88\n",
      "Episode 156, Reward: 25.8118, Avg Reward: 151.30\n",
      "Episode 157, Reward: 253.0285, Avg Reward: 150.69\n",
      "Episode 158, Reward: 131.3257, Avg Reward: 151.98\n",
      "Episode 159, Reward: 225.9798, Avg Reward: 152.36\n",
      "Episode 160, Reward: 239.4316, Avg Reward: 147.90\n",
      "Episode 161, Reward: 159.2472, Avg Reward: 146.76\n",
      "Episode 162, Reward: 4.4268, Avg Reward: 142.87\n",
      "Episode 163, Reward: 135.3428, Avg Reward: 142.60\n",
      "Episode 164, Reward: 56.9769, Avg Reward: 140.02\n",
      "Episode 165, Reward: 208.9511, Avg Reward: 140.08\n",
      "Episode 166, Reward: 165.2377, Avg Reward: 140.26\n",
      "Episode 167, Reward: 145.8360, Avg Reward: 141.51\n",
      "Episode 168, Reward: 27.1221, Avg Reward: 140.66\n",
      "Episode 169, Reward: 272.5801, Avg Reward: 141.91\n",
      "Episode 170, Reward: 157.1931, Avg Reward: 139.82\n",
      "Episode 171, Reward: 237.4411, Avg Reward: 140.24\n",
      "Episode 172, Reward: 135.9343, Avg Reward: 141.26\n",
      "Episode 173, Reward: 134.2572, Avg Reward: 139.00\n",
      "Episode 174, Reward: 20.9967, Avg Reward: 138.23\n",
      "Episode 175, Reward: 235.0385, Avg Reward: 139.17\n",
      "Episode 176, Reward: 53.6170, Avg Reward: 140.49\n",
      "Episode 177, Reward: 113.3608, Avg Reward: 139.63\n",
      "Episode 178, Reward: 148.9937, Avg Reward: 138.89\n",
      "Episode 179, Reward: 164.6935, Avg Reward: 136.90\n",
      "Episode 180, Reward: 133.6865, Avg Reward: 135.08\n",
      "Episode 181, Reward: 245.4082, Avg Reward: 135.34\n",
      "Episode 182, Reward: -13.8624, Avg Reward: 135.10\n",
      "Episode 183, Reward: 158.4010, Avg Reward: 136.06\n",
      "Episode 184, Reward: -11.2582, Avg Reward: 134.15\n",
      "Episode 185, Reward: 14.7952, Avg Reward: 130.58\n",
      "Episode 186, Reward: 197.5463, Avg Reward: 130.52\n",
      "Episode 187, Reward: 128.6176, Avg Reward: 131.64\n",
      "Episode 188, Reward: 162.6058, Avg Reward: 133.75\n",
      "Episode 189, Reward: 164.1164, Avg Reward: 132.65\n",
      "Episode 190, Reward: 6.5775, Avg Reward: 130.27\n",
      "Episode 191, Reward: -2.1606, Avg Reward: 127.35\n",
      "Episode 192, Reward: 145.1236, Avg Reward: 130.01\n",
      "Episode 193, Reward: 127.4177, Avg Reward: 128.38\n",
      "Episode 194, Reward: 151.1080, Avg Reward: 128.30\n",
      "Episode 195, Reward: 143.4028, Avg Reward: 133.21\n",
      "Episode 196, Reward: 40.2258, Avg Reward: 131.55\n",
      "Episode 197, Reward: 7.4669, Avg Reward: 129.91\n",
      "Episode 198, Reward: 34.6037, Avg Reward: 121.39\n",
      "Episode 199, Reward: 138.8723, Avg Reward: 121.16\n",
      "Episode 200, Reward: 161.5437, Avg Reward: 122.00\n",
      "Episode 201, Reward: -9.3402, Avg Reward: 116.99\n",
      "Episode 202, Reward: 21.4372, Avg Reward: 113.23\n",
      "Episode 203, Reward: 134.3705, Avg Reward: 114.72\n",
      "Episode 204, Reward: 163.0378, Avg Reward: 115.12\n",
      "Episode 205, Reward: 268.5103, Avg Reward: 111.00\n",
      "Episode 206, Reward: 259.3020, Avg Reward: 115.68\n",
      "Episode 207, Reward: -17.1933, Avg Reward: 115.22\n",
      "Episode 208, Reward: 276.1206, Avg Reward: 114.51\n",
      "Episode 209, Reward: -65.2158, Avg Reward: 118.89\n",
      "Episode 210, Reward: 238.9478, Avg Reward: 120.41\n",
      "Episode 211, Reward: -1.0306, Avg Reward: 115.44\n",
      "Episode 212, Reward: 267.6928, Avg Reward: 123.68\n",
      "Episode 213, Reward: -0.0701, Avg Reward: 120.46\n",
      "Episode 214, Reward: 253.0089, Avg Reward: 121.93\n",
      "Episode 215, Reward: -70.6792, Avg Reward: 114.29\n",
      "Episode 216, Reward: -12.3757, Avg Reward: 101.18\n",
      "Episode 217, Reward: 29.1064, Avg Reward: 87.55\n",
      "Episode 218, Reward: 253.4307, Avg Reward: 90.45\n",
      "Episode 219, Reward: -14.8113, Avg Reward: 82.74\n",
      "Episode 220, Reward: -14.9682, Avg Reward: 82.40\n",
      "Episode 221, Reward: 252.0325, Avg Reward: 81.26\n",
      "Episode 222, Reward: -27.9266, Avg Reward: 88.34\n",
      "Episode 223, Reward: 257.1142, Avg Reward: 93.32\n",
      "Episode 224, Reward: -9.6511, Avg Reward: 100.93\n",
      "Episode 225, Reward: 30.2837, Avg Reward: 101.91\n",
      "Episode 226, Reward: 283.5928, Avg Reward: 109.74\n",
      "Episode 227, Reward: 253.6503, Avg Reward: 123.76\n",
      "Episode 228, Reward: 263.2738, Avg Reward: 116.62\n",
      "Episode 229, Reward: 17.9109, Avg Reward: 125.89\n",
      "Episode 230, Reward: -14.4027, Avg Reward: 124.08\n",
      "Episode 231, Reward: 243.2644, Avg Reward: 137.14\n",
      "Episode 232, Reward: -32.5533, Avg Reward: 131.72\n",
      "Episode 233, Reward: -26.9660, Avg Reward: 122.31\n",
      "Episode 234, Reward: -108.4270, Avg Reward: 114.81\n",
      "Episode 235, Reward: -44.8552, Avg Reward: 122.10\n",
      "Episode 236, Reward: 275.0054, Avg Reward: 118.31\n",
      "Episode 237, Reward: 18.5880, Avg Reward: 115.00\n",
      "Episode 238, Reward: 306.8029, Avg Reward: 127.05\n",
      "Episode 239, Reward: 272.7967, Avg Reward: 131.98\n",
      "Episode 240, Reward: 229.1806, Avg Reward: 133.64\n",
      "Episode 241, Reward: 251.5699, Avg Reward: 132.94\n",
      "Episode 242, Reward: 252.0222, Avg Reward: 129.74\n",
      "Episode 243, Reward: 241.4508, Avg Reward: 142.90\n",
      "Episode 244, Reward: 243.7480, Avg Reward: 150.10\n",
      "Episode 245, Reward: 283.0759, Avg Reward: 163.83\n",
      "Episode 246, Reward: 285.3808, Avg Reward: 172.13\n",
      "Episode 247, Reward: -41.7290, Avg Reward: 161.17\n",
      "Episode 248, Reward: -102.8991, Avg Reward: 167.96\n",
      "Episode 249, Reward: 300.2173, Avg Reward: 175.65\n",
      "Episode 250, Reward: 260.5414, Avg Reward: 169.70\n",
      "Episode 251, Reward: 123.8674, Avg Reward: 167.73\n",
      "Episode 252, Reward: 126.1353, Avg Reward: 173.75\n",
      "Episode 253, Reward: 241.0992, Avg Reward: 178.25\n",
      "Episode 254, Reward: -80.9762, Avg Reward: 171.39\n",
      "Episode 255, Reward: 18.7651, Avg Reward: 181.20\n",
      "Episode 256, Reward: 255.6357, Avg Reward: 174.56\n",
      "Episode 257, Reward: 272.5974, Avg Reward: 172.09\n",
      "Episode 258, Reward: 273.5403, Avg Reward: 171.31\n",
      "Episode 259, Reward: 271.3240, Avg Reward: 176.82\n",
      "Episode 260, Reward: 78.3441, Avg Reward: 177.20\n",
      "Episode 261, Reward: 238.0307, Avg Reward: 191.59\n",
      "Episode 262, Reward: 278.5545, Avg Reward: 197.11\n",
      "Episode 263, Reward: 239.5022, Avg Reward: 200.82\n",
      "Episode 264, Reward: 119.7930, Avg Reward: 203.80\n",
      "Episode 265, Reward: 267.5140, Avg Reward: 208.02\n",
      "Episode 266, Reward: 263.9579, Avg Reward: 208.86\n",
      "Episode 267, Reward: 261.4722, Avg Reward: 218.17\n",
      "Episode 268, Reward: 259.2112, Avg Reward: 221.10\n",
      "Episode 269, Reward: 245.1738, Avg Reward: 229.78\n",
      "Episode 270, Reward: 227.6140, Avg Reward: 231.84\n",
      "Episode 271, Reward: 257.2873, Avg Reward: 229.74\n",
      "Episode 272, Reward: 260.4446, Avg Reward: 229.35\n",
      "Episode 273, Reward: 287.4847, Avg Reward: 226.43\n",
      "Episode 274, Reward: 245.3591, Avg Reward: 219.11\n",
      "Episode 275, Reward: 292.1262, Avg Reward: 220.65\n",
      "Episode 276, Reward: 277.6740, Avg Reward: 219.50\n",
      "Episode 277, Reward: 300.0650, Avg Reward: 229.09\n",
      "Episode 278, Reward: 273.5787, Avg Reward: 232.75\n",
      "Episode 279, Reward: 311.9555, Avg Reward: 233.90\n",
      "Episode 280, Reward: 41.1547, Avg Reward: 232.24\n",
      "Episode 281, Reward: 304.7148, Avg Reward: 231.32\n",
      "Episode 282, Reward: 275.9319, Avg Reward: 229.89\n",
      "Episode 283, Reward: 291.7585, Avg Reward: 224.98\n",
      "Episode 284, Reward: -81.6003, Avg Reward: 216.69\n",
      "Episode 285, Reward: 62.2757, Avg Reward: 202.39\n",
      "Episode 286, Reward: 34.3894, Avg Reward: 201.71\n",
      "Episode 287, Reward: 278.1898, Avg Reward: 184.62\n",
      "Episode 288, Reward: 261.7578, Avg Reward: 175.63\n",
      "Episode 289, Reward: -340.1380, Avg Reward: 161.36\n",
      "Episode 290, Reward: -90.1028, Avg Reward: 144.41\n",
      "Episode 291, Reward: 250.1350, Avg Reward: 146.46\n",
      "Episode 292, Reward: 272.6742, Avg Reward: 144.43\n",
      "Episode 293, Reward: 260.6320, Avg Reward: 131.03\n",
      "Episode 294, Reward: 270.0816, Avg Reward: 138.71\n",
      "Episode 295, Reward: 232.3041, Avg Reward: 141.84\n",
      "Episode 296, Reward: 234.1291, Avg Reward: 145.31\n",
      "Episode 297, Reward: 21.6691, Avg Reward: 154.61\n",
      "Episode 298, Reward: -43.6173, Avg Reward: 161.94\n",
      "Episode 299, Reward: 241.9934, Avg Reward: 167.77\n",
      "Episode 300, Reward: 275.3159, Avg Reward: 166.97\n",
      "Episode 301, Reward: 265.5490, Avg Reward: 179.04\n",
      "Episode 302, Reward: 271.1714, Avg Reward: 199.03\n",
      "Episode 303, Reward: 264.2523, Avg Reward: 203.25\n",
      "Episode 304, Reward: 273.2685, Avg Reward: 207.61\n",
      "Episode 305, Reward: 302.9210, Avg Reward: 211.25\n",
      "Episode 306, Reward: 294.5075, Avg Reward: 211.54\n",
      "Episode 307, Reward: 273.4537, Avg Reward: 214.44\n",
      "Episode 308, Reward: 249.9647, Avg Reward: 215.43\n",
      "Episode 309, Reward: 261.5196, Avg Reward: 219.40\n",
      "Episode 310, Reward: 144.1294, Avg Reward: 226.46\n",
      "Episode 311, Reward: 288.8740, Avg Reward: 237.70\n",
      "Episode 312, Reward: 258.1134, Avg Reward: 243.69\n",
      "Episode 313, Reward: 260.7540, Avg Reward: 245.95\n",
      "Episode 314, Reward: -216.1478, Avg Reward: 241.05\n",
      "Episode 315, Reward: 298.0300, Avg Reward: 245.26\n",
      "Episode 316, Reward: 282.0895, Avg Reward: 249.88\n",
      "Episode 317, Reward: 265.1090, Avg Reward: 252.79\n",
      "Episode 318, Reward: 270.0595, Avg Reward: 256.71\n",
      "Episode 319, Reward: 277.9891, Avg Reward: 256.87\n",
      "Episode 320, Reward: 288.5466, Avg Reward: 257.14\n",
      "Episode 321, Reward: 308.2908, Avg Reward: 260.22\n",
      "Episode 322, Reward: 295.0363, Avg Reward: 260.70\n",
      "Episode 323, Reward: -215.4560, Avg Reward: 259.62\n",
      "Episode 324, Reward: 291.2417, Avg Reward: 261.25\n",
      "Episode 325, Reward: 196.2828, Avg Reward: 259.36\n",
      "Episode 326, Reward: 278.4456, Avg Reward: 264.41\n",
      "Episode 327, Reward: 269.6682, Avg Reward: 262.10\n",
      "Episode 328, Reward: 98.6349, Avg Reward: 259.00\n",
      "Episode 329, Reward: 259.7472, Avg Reward: 259.08\n",
      "Episode 330, Reward: 309.0280, Avg Reward: 258.18\n",
      "Episode 331, Reward: 255.8551, Avg Reward: 258.81\n",
      "Episode 332, Reward: 283.1595, Avg Reward: 257.38\n",
      "Episode 333, Reward: 257.8435, Avg Reward: 256.82\n",
      "Episode 334, Reward: 316.2337, Avg Reward: 259.25\n",
      "Episode 335, Reward: 258.0997, Avg Reward: 256.54\n",
      "Episode 336, Reward: 263.5058, Avg Reward: 257.50\n",
      "Episode 337, Reward: 305.1315, Avg Reward: 262.53\n",
      "Episode 338, Reward: 288.7918, Avg Reward: 261.82\n",
      "Episode 339, Reward: 246.8536, Avg Reward: 261.56\n",
      "Episode 340, Reward: 303.4699, Avg Reward: 262.00\n",
      "Episode 341, Reward: 287.8708, Avg Reward: 262.95\n",
      "Episode 342, Reward: 269.7629, Avg Reward: 268.11\n",
      "Episode 343, Reward: 281.4434, Avg Reward: 267.57\n",
      "Episode 344, Reward: 297.3905, Avg Reward: 269.71\n",
      "Episode 345, Reward: 286.7419, Avg Reward: 270.25\n",
      "Episode 346, Reward: 268.0976, Avg Reward: 268.86\n",
      "Episode 347, Reward: 298.9798, Avg Reward: 270.37\n",
      "Episode 348, Reward: 286.1081, Avg Reward: 269.60\n",
      "Episode 349, Reward: 281.3091, Avg Reward: 270.33\n",
      "Episode 350, Reward: 256.6815, Avg Reward: 272.19\n",
      "Episode 351, Reward: 306.4834, Avg Reward: 273.73\n",
      "Episode 352, Reward: 270.2678, Avg Reward: 271.09\n",
      "Episode 353, Reward: 138.4775, Avg Reward: 269.42\n",
      "Episode 354, Reward: 273.0702, Avg Reward: 269.21\n",
      "Episode 355, Reward: 276.9931, Avg Reward: 267.95\n",
      "Episode 356, Reward: 312.9265, Avg Reward: 266.21\n",
      "Episode 357, Reward: 266.1162, Avg Reward: 265.02\n",
      "Episode 358, Reward: 255.6156, Avg Reward: 265.65\n",
      "Episode 359, Reward: 266.8531, Avg Reward: 265.92\n",
      "Episode 360, Reward: 264.1594, Avg Reward: 266.30\n",
      "Episode 361, Reward: 288.3699, Avg Reward: 265.61\n",
      "Episode 362, Reward: 286.2719, Avg Reward: 264.71\n",
      "Episode 363, Reward: 256.4913, Avg Reward: 262.28\n",
      "Episode 364, Reward: 293.5822, Avg Reward: 262.23\n",
      "Episode 365, Reward: 286.0842, Avg Reward: 260.09\n",
      "Episode 366, Reward: 260.6511, Avg Reward: 260.43\n",
      "Episode 367, Reward: 253.6668, Avg Reward: 262.44\n",
      "Episode 368, Reward: 254.6111, Avg Reward: 261.51\n",
      "Episode 369, Reward: 278.2803, Avg Reward: 265.02\n",
      "Episode 370, Reward: 143.8770, Avg Reward: 264.80\n",
      "Episode 371, Reward: 266.3581, Avg Reward: 265.66\n",
      "Episode 372, Reward: 276.4984, Avg Reward: 264.72\n",
      "Episode 373, Reward: 270.3937, Avg Reward: 259.35\n",
      "Episode 374, Reward: 292.1638, Avg Reward: 261.84\n",
      "Episode 375, Reward: 258.1644, Avg Reward: 261.01\n",
      "Episode 376, Reward: 260.0900, Avg Reward: 262.22\n",
      "Episode 377, Reward: 289.3696, Avg Reward: 261.93\n",
      "Episode 378, Reward: 258.7488, Avg Reward: 261.85\n",
      "Episode 379, Reward: 295.2199, Avg Reward: 263.33\n",
      "Episode 380, Reward: 278.7315, Avg Reward: 263.76\n",
      "Episode 381, Reward: 296.5725, Avg Reward: 265.23\n",
      "Episode 382, Reward: 279.9157, Avg Reward: 265.47\n",
      "Episode 383, Reward: 255.3803, Avg Reward: 263.18\n",
      "Episode 384, Reward: 281.4761, Avg Reward: 264.11\n",
      "Episode 385, Reward: 296.3369, Avg Reward: 269.76\n",
      "Episode 386, Reward: 266.1667, Avg Reward: 271.26\n",
      "Episode 387, Reward: 283.3343, Avg Reward: 270.04\n",
      "Episode 388, Reward: 260.4731, Avg Reward: 269.94\n",
      "Episode 389, Reward: 281.0312, Avg Reward: 269.54\n",
      "Episode 390, Reward: 303.5703, Avg Reward: 271.33\n",
      "Episode 391, Reward: 259.1092, Avg Reward: 270.10\n",
      "Episode 392, Reward: 253.0960, Avg Reward: 268.12\n",
      "Episode 393, Reward: 298.2076, Avg Reward: 266.31\n",
      "Episode 394, Reward: 76.3672, Avg Reward: 262.24\n",
      "Episode 395, Reward: 238.3987, Avg Reward: 259.34\n",
      "Episode 396, Reward: 264.6462, Avg Reward: 259.29\n",
      "Episode 397, Reward: 251.3147, Avg Reward: 261.40\n",
      "Episode 398, Reward: 293.3049, Avg Reward: 259.84\n",
      "Episode 399, Reward: 275.0621, Avg Reward: 258.38\n",
      "Episode 400, Reward: 277.3480, Avg Reward: 262.08\n",
      "Episode 401, Reward: 246.0463, Avg Reward: 260.75\n",
      "Episode 402, Reward: 278.2517, Avg Reward: 259.37\n",
      "Episode 403, Reward: 155.1260, Avg Reward: 259.50\n",
      "Episode 404, Reward: 270.8462, Avg Reward: 257.34\n",
      "Episode 405, Reward: 279.2768, Avg Reward: 257.83\n",
      "Episode 406, Reward: 32.4652, Avg Reward: 258.48\n",
      "Episode 407, Reward: 294.2389, Avg Reward: 259.42\n",
      "Episode 408, Reward: 291.3451, Avg Reward: 264.90\n",
      "Episode 409, Reward: 300.4544, Avg Reward: 268.84\n",
      "Episode 410, Reward: 296.8860, Avg Reward: 267.53\n",
      "Episode 411, Reward: 305.1046, Avg Reward: 267.45\n",
      "Episode 412, Reward: 276.4291, Avg Reward: 267.84\n",
      "Episode 413, Reward: 290.3114, Avg Reward: 264.23\n",
      "Episode 414, Reward: 281.2027, Avg Reward: 262.01\n",
      "Episode 415, Reward: 290.7787, Avg Reward: 266.62\n",
      "Episode 416, Reward: 255.6386, Avg Reward: 265.69\n",
      "Episode 417, Reward: 287.9273, Avg Reward: 266.93\n",
      "Episode 418, Reward: 266.2005, Avg Reward: 270.20\n",
      "Episode 419, Reward: 310.1644, Avg Reward: 268.47\n",
      "Episode 420, Reward: 284.5689, Avg Reward: 267.41\n",
      "Episode 421, Reward: 277.0496, Avg Reward: 263.94\n",
      "Episode 422, Reward: 307.1957, Avg Reward: 262.49\n",
      "Episode 423, Reward: 317.7090, Avg Reward: 263.16\n",
      "Episode 424, Reward: 267.6389, Avg Reward: 263.09\n",
      "Episode 425, Reward: 275.7300, Avg Reward: 266.06\n",
      "Episode 426, Reward: 279.8519, Avg Reward: 264.32\n",
      "Episode 427, Reward: 301.3036, Avg Reward: 263.78\n",
      "Episode 428, Reward: 288.4102, Avg Reward: 266.28\n",
      "Episode 429, Reward: 46.7711, Avg Reward: 262.39\n",
      "Episode 430, Reward: 258.0058, Avg Reward: 260.56\n",
      "Episode 431, Reward: 305.2839, Avg Reward: 258.08\n",
      "Episode 432, Reward: 263.1017, Avg Reward: 256.36\n",
      "Episode 433, Reward: 270.7153, Avg Reward: 256.10\n",
      "Episode 434, Reward: 283.4924, Avg Reward: 260.05\n",
      "Episode 435, Reward: 315.9215, Avg Reward: 259.64\n",
      "Episode 436, Reward: 140.5195, Avg Reward: 255.03\n",
      "Episode 437, Reward: 270.3256, Avg Reward: 254.99\n",
      "Episode 438, Reward: 247.7417, Avg Reward: 254.11\n",
      "Episode 439, Reward: 295.4940, Avg Reward: 254.86\n",
      "Episode 440, Reward: 258.5367, Avg Reward: 258.60\n",
      "Episode 441, Reward: 287.3097, Avg Reward: 258.16\n",
      "Episode 442, Reward: 271.0768, Avg Reward: 254.40\n",
      "Episode 443, Reward: 283.5317, Avg Reward: 254.28\n",
      "Episode 444, Reward: 249.7847, Avg Reward: 252.31\n",
      "Episode 445, Reward: 281.5364, Avg Reward: 252.90\n",
      "Episode 446, Reward: 236.8358, Avg Reward: 255.34\n",
      "Episode 447, Reward: 283.4680, Avg Reward: 253.88\n",
      "Episode 448, Reward: 268.2772, Avg Reward: 252.75\n",
      "Episode 449, Reward: 299.2398, Avg Reward: 253.12\n",
      "Episode 450, Reward: 290.3402, Avg Reward: 257.75\n",
      "Episode 451, Reward: 134.4837, Avg Reward: 257.26\n",
      "Episode 452, Reward: 289.0389, Avg Reward: 257.91\n",
      "Episode 453, Reward: 104.6836, Avg Reward: 255.60\n",
      "Episode 454, Reward: 278.9816, Avg Reward: 252.04\n",
      "Episode 455, Reward: 268.0814, Avg Reward: 250.11\n",
      "Episode 456, Reward: 150.0500, Avg Reward: 248.85\n",
      "Episode 457, Reward: 144.5566, Avg Reward: 248.13\n",
      "Episode 458, Reward: 280.8856, Avg Reward: 244.80\n",
      "Episode 459, Reward: 263.7656, Avg Reward: 246.33\n",
      "Episode 460, Reward: 281.5256, Avg Reward: 251.37\n",
      "Episode 461, Reward: 254.6524, Avg Reward: 251.30\n",
      "Episode 462, Reward: 238.5610, Avg Reward: 255.79\n",
      "Episode 463, Reward: 294.1648, Avg Reward: 257.26\n",
      "Episode 464, Reward: 299.2594, Avg Reward: 257.47\n",
      "Episode 465, Reward: 299.7999, Avg Reward: 256.29\n",
      "Episode 466, Reward: 288.1226, Avg Reward: 261.48\n",
      "Episode 467, Reward: 257.0077, Avg Reward: 263.87\n",
      "Episode 468, Reward: 278.3343, Avg Reward: 264.60\n",
      "Episode 469, Reward: 265.0218, Avg Reward: 264.03\n",
      "Episode 470, Reward: 243.4279, Avg Reward: 269.26\n",
      "Episode 471, Reward: 274.4354, Avg Reward: 270.15\n",
      "Episode 472, Reward: 252.7506, Avg Reward: 271.10\n",
      "Episode 473, Reward: 307.2530, Avg Reward: 266.23\n",
      "Episode 474, Reward: 38.0067, Avg Reward: 260.92\n",
      "Episode 475, Reward: 267.4108, Avg Reward: 260.52\n",
      "Episode 476, Reward: 300.2488, Avg Reward: 259.41\n",
      "Episode 477, Reward: 230.4421, Avg Reward: 256.46\n",
      "Episode 478, Reward: 278.3018, Avg Reward: 257.06\n",
      "Episode 479, Reward: 308.8823, Avg Reward: 256.31\n",
      "Episode 480, Reward: 285.5056, Avg Reward: 256.56\n",
      "Episode 481, Reward: 269.3585, Avg Reward: 255.42\n",
      "Episode 482, Reward: 261.3478, Avg Reward: 256.93\n",
      "Episode 483, Reward: 84.5704, Avg Reward: 253.89\n",
      "Episode 484, Reward: 310.0160, Avg Reward: 255.34\n",
      "Episode 485, Reward: 307.1223, Avg Reward: 253.10\n",
      "Episode 486, Reward: 303.4435, Avg Reward: 257.84\n",
      "Episode 487, Reward: 272.6720, Avg Reward: 259.83\n",
      "Episode 488, Reward: 149.8530, Avg Reward: 261.17\n",
      "Episode 489, Reward: 253.3165, Avg Reward: 261.64\n",
      "Episode 490, Reward: 259.6220, Avg Reward: 264.86\n",
      "Episode 491, Reward: 277.6846, Avg Reward: 265.87\n",
      "Episode 492, Reward: 290.9314, Avg Reward: 267.14\n",
      "Episode 493, Reward: 287.7506, Avg Reward: 269.53\n",
      "Episode 494, Reward: 252.9487, Avg Reward: 269.11\n",
      "Episode 495, Reward: -29.9014, Avg Reward: 265.75\n",
      "Episode 496, Reward: 262.4857, Avg Reward: 265.48\n",
      "Episode 497, Reward: 262.4083, Avg Reward: 264.00\n",
      "Episode 498, Reward: 324.6314, Avg Reward: 266.13\n",
      "Episode 499, Reward: 250.5369, Avg Reward: 264.97\n",
      "Episode 500, Reward: 257.3023, Avg Reward: 263.63\n",
      "Episode 501, Reward: 264.8862, Avg Reward: 262.20\n",
      "Episode 502, Reward: 266.0532, Avg Reward: 261.15\n",
      "Episode 503, Reward: 284.3806, Avg Reward: 263.45\n",
      "Episode 504, Reward: 252.7165, Avg Reward: 261.86\n",
      "Episode 505, Reward: 175.9365, Avg Reward: 261.10\n",
      "Episode 506, Reward: 276.3648, Avg Reward: 259.66\n",
      "Episode 507, Reward: 261.3967, Avg Reward: 258.55\n",
      "Episode 508, Reward: 282.0841, Avg Reward: 256.98\n",
      "Episode 509, Reward: 287.1731, Avg Reward: 256.75\n",
      "Episode 510, Reward: 272.7684, Avg Reward: 261.99\n",
      "Episode 511, Reward: 112.3554, Avg Reward: 258.84\n",
      "Episode 512, Reward: 250.2638, Avg Reward: 255.68\n",
      "Episode 513, Reward: 283.5446, Avg Reward: 257.42\n",
      "Episode 514, Reward: 257.9188, Avg Reward: 259.50\n",
      "Episode 515, Reward: 255.0451, Avg Reward: 256.48\n",
      "Episode 516, Reward: 249.4552, Avg Reward: 258.21\n",
      "Episode 517, Reward: 258.2099, Avg Reward: 255.91\n",
      "Episode 518, Reward: 271.7425, Avg Reward: 256.88\n",
      "Episode 519, Reward: 296.7408, Avg Reward: 256.52\n",
      "Episode 520, Reward: 278.0586, Avg Reward: 259.52\n",
      "Episode 521, Reward: 255.4377, Avg Reward: 257.62\n",
      "Episode 522, Reward: 311.9332, Avg Reward: 255.64\n",
      "Episode 523, Reward: 261.6277, Avg Reward: 253.48\n",
      "Episode 524, Reward: 82.8036, Avg Reward: 250.39\n",
      "Episode 525, Reward: 278.7596, Avg Reward: 252.04\n",
      "Episode 526, Reward: 303.6821, Avg Reward: 257.52\n",
      "Episode 527, Reward: 285.2238, Avg Reward: 255.42\n",
      "Episode 528, Reward: 248.6687, Avg Reward: 257.01\n",
      "Episode 529, Reward: 71.6510, Avg Reward: 259.96\n",
      "Episode 530, Reward: 261.4940, Avg Reward: 260.74\n",
      "Episode 531, Reward: 294.4189, Avg Reward: 262.21\n",
      "Episode 532, Reward: 254.3969, Avg Reward: 260.79\n",
      "Episode 533, Reward: 304.6450, Avg Reward: 259.36\n",
      "Episode 534, Reward: 274.3754, Avg Reward: 261.26\n",
      "Episode 535, Reward: 296.6857, Avg Reward: 265.01\n",
      "Episode 536, Reward: 290.6950, Avg Reward: 269.72\n",
      "Episode 537, Reward: 301.6466, Avg Reward: 272.34\n",
      "Episode 538, Reward: 282.7064, Avg Reward: 274.02\n",
      "Episode 539, Reward: 294.0779, Avg Reward: 273.80\n",
      "Episode 540, Reward: 282.4651, Avg Reward: 276.38\n",
      "Episode 541, Reward: 136.7024, Avg Reward: 276.82\n",
      "Episode 542, Reward: 288.3712, Avg Reward: 281.00\n",
      "Episode 543, Reward: 296.6699, Avg Reward: 278.50\n",
      "Episode 544, Reward: 290.9216, Avg Reward: 277.45\n",
      "Episode 545, Reward: 297.5440, Avg Reward: 275.15\n",
      "Episode 546, Reward: 284.2261, Avg Reward: 271.74\n",
      "Episode 547, Reward: 281.7700, Avg Reward: 271.58\n",
      "Episode 548, Reward: 306.3443, Avg Reward: 270.53\n",
      "Episode 549, Reward: 264.5710, Avg Reward: 270.73\n",
      "Episode 550, Reward: 280.1104, Avg Reward: 269.19\n",
      "Episode 551, Reward: 286.6119, Avg Reward: 268.76\n",
      "Episode 552, Reward: 296.5060, Avg Reward: 270.02\n",
      "Episode 553, Reward: 261.3974, Avg Reward: 271.51\n",
      "Episode 554, Reward: 307.4893, Avg Reward: 270.73\n",
      "Episode 555, Reward: 312.9286, Avg Reward: 272.52\n",
      "Episode 556, Reward: 298.3105, Avg Reward: 270.70\n",
      "Episode 557, Reward: 205.4814, Avg Reward: 269.91\n",
      "Episode 558, Reward: 291.1591, Avg Reward: 263.52\n",
      "Episode 559, Reward: 287.2867, Avg Reward: 262.81\n",
      "Episode 560, Reward: 279.0960, Avg Reward: 256.83\n",
      "Episode 561, Reward: 304.7743, Avg Reward: 258.87\n",
      "Episode 562, Reward: 303.2722, Avg Reward: 256.46\n",
      "Episode 563, Reward: 276.7689, Avg Reward: 255.29\n",
      "Episode 564, Reward: 282.1078, Avg Reward: 249.69\n",
      "Episode 565, Reward: 288.5178, Avg Reward: 249.49\n",
      "Episode 566, Reward: 250.4971, Avg Reward: 247.36\n",
      "Episode 567, Reward: 274.2669, Avg Reward: 249.57\n",
      "Episode 568, Reward: 274.7956, Avg Reward: 253.14\n",
      "Episode 569, Reward: 252.0347, Avg Reward: 260.25\n",
      "Episode 570, Reward: 296.8541, Avg Reward: 260.43\n",
      "Episode 571, Reward: 284.0794, Avg Reward: 265.99\n",
      "Episode 572, Reward: 247.5548, Avg Reward: 266.40\n",
      "Episode 573, Reward: 80.6077, Avg Reward: 264.53\n",
      "Episode 574, Reward: 268.6629, Avg Reward: 262.12\n",
      "Episode 575, Reward: 256.8699, Avg Reward: 268.49\n",
      "Episode 576, Reward: 303.8275, Avg Reward: 270.04\n",
      "Episode 577, Reward: 284.3631, Avg Reward: 273.43\n",
      "Episode 578, Reward: 248.9446, Avg Reward: 273.79\n",
      "Episode 579, Reward: 267.7200, Avg Reward: 271.61\n",
      "Episode 580, Reward: 255.8798, Avg Reward: 272.29\n",
      "Episode 581, Reward: 293.0193, Avg Reward: 270.51\n",
      "Episode 582, Reward: 301.5901, Avg Reward: 269.46\n",
      "Episode 583, Reward: 298.1503, Avg Reward: 271.49\n",
      "Episode 584, Reward: 295.7956, Avg Reward: 274.88\n",
      "Episode 585, Reward: 271.5244, Avg Reward: 272.44\n",
      "Episode 586, Reward: 317.1833, Avg Reward: 271.76\n",
      "Episode 587, Reward: 281.1897, Avg Reward: 272.26\n",
      "Episode 588, Reward: 248.3971, Avg Reward: 271.62\n",
      "Episode 589, Reward: 39.7283, Avg Reward: 267.18\n",
      "Episode 590, Reward: 242.8572, Avg Reward: 268.67\n",
      "Episode 591, Reward: 286.7549, Avg Reward: 268.02\n",
      "Episode 592, Reward: 285.3829, Avg Reward: 267.03\n",
      "Episode 593, Reward: 269.9148, Avg Reward: 264.01\n",
      "Episode 594, Reward: 290.9832, Avg Reward: 262.00\n",
      "Episode 595, Reward: 284.2023, Avg Reward: 263.47\n",
      "Episode 596, Reward: 263.5357, Avg Reward: 263.57\n",
      "Episode 597, Reward: 260.7809, Avg Reward: 263.03\n",
      "Episode 598, Reward: 291.3582, Avg Reward: 258.68\n",
      "Episode 599, Reward: 258.6062, Avg Reward: 262.28\n",
      "Episode 600, Reward: 288.9324, Avg Reward: 260.13\n",
      "Episode 601, Reward: 289.3665, Avg Reward: 260.55\n",
      "Episode 602, Reward: 271.5875, Avg Reward: 264.24\n",
      "Episode 603, Reward: 282.7174, Avg Reward: 263.68\n",
      "Episode 604, Reward: 287.5732, Avg Reward: 262.86\n",
      "Episode 605, Reward: 289.8646, Avg Reward: 254.48\n",
      "Episode 606, Reward: 258.0244, Avg Reward: 252.49\n",
      "Episode 607, Reward: 316.4326, Avg Reward: 254.66\n",
      "Episode 608, Reward: 293.0440, Avg Reward: 254.82\n",
      "Episode 609, Reward: 291.3632, Avg Reward: 255.02\n",
      "Episode 610, Reward: 270.6271, Avg Reward: 251.38\n",
      "Episode 611, Reward: 257.4848, Avg Reward: 250.12\n",
      "Episode 612, Reward: 252.2303, Avg Reward: 248.49\n",
      "Episode 613, Reward: 108.6823, Avg Reward: 245.93\n",
      "Episode 614, Reward: 303.2715, Avg Reward: 249.80\n",
      "Episode 615, Reward: 308.0273, Avg Reward: 251.49\n",
      "Episode 616, Reward: 280.8059, Avg Reward: 248.36\n",
      "Episode 617, Reward: 286.0557, Avg Reward: 246.72\n",
      "Episode 618, Reward: 304.1631, Avg Reward: 249.17\n",
      "Episode 619, Reward: 263.3352, Avg Reward: 259.88\n",
      "Episode 620, Reward: 271.6582, Avg Reward: 261.77\n",
      "Episode 621, Reward: 271.1965, Avg Reward: 259.44\n",
      "Episode 622, Reward: 259.5068, Avg Reward: 260.13\n",
      "Episode 623, Reward: 308.1594, Avg Reward: 260.90\n",
      "Episode 624, Reward: 296.6082, Avg Reward: 265.49\n",
      "Episode 625, Reward: 295.9510, Avg Reward: 264.94\n",
      "Episode 626, Reward: 291.2654, Avg Reward: 267.72\n",
      "Episode 627, Reward: 294.4357, Avg Reward: 255.41\n",
      "Episode 628, Reward: 311.0090, Avg Reward: 253.60\n",
      "Episode 629, Reward: 271.1736, Avg Reward: 250.10\n",
      "Episode 630, Reward: 302.4805, Avg Reward: 249.70\n",
      "Episode 631, Reward: 51.5922, Avg Reward: 246.01\n",
      "Episode 632, Reward: 57.7209, Avg Reward: 242.50\n",
      "Episode 633, Reward: 274.6549, Avg Reward: 241.17\n",
      "Episode 634, Reward: 292.5847, Avg Reward: 242.98\n",
      "Episode 635, Reward: 276.9762, Avg Reward: 238.70\n",
      "Episode 636, Reward: 278.0694, Avg Reward: 236.65\n",
      "Episode 637, Reward: 304.7302, Avg Reward: 236.12\n",
      "Episode 638, Reward: 248.8557, Avg Reward: 238.59\n",
      "Episode 639, Reward: 32.4973, Avg Reward: 237.28\n",
      "Episode 640, Reward: 309.5297, Avg Reward: 241.33\n",
      "Episode 641, Reward: 129.1363, Avg Reward: 245.43\n",
      "Episode 642, Reward: 269.0584, Avg Reward: 247.19\n",
      "Episode 643, Reward: 275.1566, Avg Reward: 248.60\n",
      "Episode 644, Reward: 256.9224, Avg Reward: 252.02\n",
      "Episode 645, Reward: 276.5867, Avg Reward: 243.65\n",
      "Episode 646, Reward: 157.8743, Avg Reward: 246.98\n",
      "Episode 647, Reward: 74.8147, Avg Reward: 246.11\n",
      "Episode 648, Reward: 293.4791, Avg Reward: 243.30\n",
      "Episode 649, Reward: 39.3955, Avg Reward: 241.26\n",
      "Episode 650, Reward: 277.6819, Avg Reward: 237.56\n",
      "Episode 651, Reward: 284.5538, Avg Reward: 238.85\n",
      "Episode 652, Reward: 257.8750, Avg Reward: 234.74\n",
      "Episode 653, Reward: -20.4876, Avg Reward: 226.34\n",
      "Episode 654, Reward: 264.2626, Avg Reward: 211.74\n",
      "Episode 655, Reward: -4.7328, Avg Reward: 206.41\n",
      "Episode 656, Reward: 275.0219, Avg Reward: 204.28\n",
      "Episode 657, Reward: 295.7296, Avg Reward: 211.48\n",
      "Episode 658, Reward: -15.2523, Avg Reward: 211.44\n",
      "Episode 659, Reward: 126.0652, Avg Reward: 209.91\n",
      "Episode 660, Reward: 284.6638, Avg Reward: 212.46\n",
      "Episode 661, Reward: 242.6104, Avg Reward: 215.75\n",
      "Episode 662, Reward: 265.1629, Avg Reward: 215.87\n",
      "Episode 663, Reward: 293.2520, Avg Reward: 214.81\n",
      "Episode 664, Reward: 296.9169, Avg Reward: 218.09\n",
      "Episode 665, Reward: 292.0462, Avg Reward: 223.36\n",
      "Episode 666, Reward: 237.5704, Avg Reward: 223.83\n",
      "Episode 667, Reward: 240.5177, Avg Reward: 224.40\n",
      "Episode 668, Reward: 123.2690, Avg Reward: 225.79\n",
      "Episode 669, Reward: 290.0834, Avg Reward: 229.70\n",
      "Episode 670, Reward: 279.5727, Avg Reward: 230.72\n",
      "Episode 671, Reward: 311.3934, Avg Reward: 236.33\n",
      "Episode 672, Reward: 284.2504, Avg Reward: 234.54\n",
      "Episode 673, Reward: 299.8298, Avg Reward: 230.77\n",
      "Episode 674, Reward: 271.0617, Avg Reward: 229.50\n",
      "Episode 675, Reward: 279.1540, Avg Reward: 232.28\n",
      "Episode 676, Reward: 236.9080, Avg Reward: 230.96\n",
      "Episode 677, Reward: 285.8101, Avg Reward: 235.37\n",
      "Episode 678, Reward: 137.2610, Avg Reward: 235.44\n",
      "Episode 679, Reward: 297.3667, Avg Reward: 238.58\n",
      "Episode 680, Reward: 268.7069, Avg Reward: 240.09\n",
      "Episode 681, Reward: 262.0675, Avg Reward: 237.70\n",
      "Episode 682, Reward: 259.0244, Avg Reward: 242.04\n",
      "Episode 683, Reward: 301.8749, Avg Reward: 249.72\n",
      "Episode 684, Reward: 256.7308, Avg Reward: 253.38\n",
      "Episode 685, Reward: 287.6068, Avg Reward: 253.46\n",
      "Episode 686, Reward: 271.7547, Avg Reward: 252.92\n",
      "Episode 687, Reward: 286.7764, Avg Reward: 255.40\n",
      "Episode 688, Reward: 270.5578, Avg Reward: 255.18\n",
      "Episode 689, Reward: 266.2588, Avg Reward: 257.94\n",
      "Episode 690, Reward: 285.6165, Avg Reward: 257.99\n",
      "Episode 691, Reward: 269.9594, Avg Reward: 257.90\n",
      "Episode 692, Reward: 153.8917, Avg Reward: 258.12\n",
      "Episode 693, Reward: 264.3264, Avg Reward: 259.76\n",
      "Episode 694, Reward: 289.1395, Avg Reward: 260.35\n",
      "Episode 695, Reward: 137.0600, Avg Reward: 259.17\n",
      "Episode 696, Reward: 270.2921, Avg Reward: 256.25\n",
      "Episode 697, Reward: 252.7169, Avg Reward: 257.13\n",
      "Episode 698, Reward: 295.6958, Avg Reward: 258.75\n",
      "Episode 699, Reward: 152.5042, Avg Reward: 256.57\n",
      "Episode 700, Reward: 309.4930, Avg Reward: 257.18\n",
      "Episode 701, Reward: 258.0776, Avg Reward: 259.42\n",
      "Episode 702, Reward: 184.1868, Avg Reward: 257.55\n",
      "Episode 703, Reward: 292.5624, Avg Reward: 260.31\n",
      "Episode 704, Reward: 264.2189, Avg Reward: 262.49\n",
      "Episode 705, Reward: 298.3464, Avg Reward: 261.87\n",
      "Episode 706, Reward: 259.6544, Avg Reward: 260.84\n",
      "Episode 707, Reward: 327.1322, Avg Reward: 259.48\n",
      "Episode 708, Reward: 269.3567, Avg Reward: 259.74\n",
      "Episode 709, Reward: 272.9378, Avg Reward: 264.72\n",
      "Episode 710, Reward: 271.2505, Avg Reward: 265.10\n",
      "Episode 711, Reward: 253.9764, Avg Reward: 265.54\n",
      "Episode 712, Reward: 270.6052, Avg Reward: 265.43\n",
      "Episode 713, Reward: 258.9134, Avg Reward: 265.03\n",
      "Episode 714, Reward: 251.3855, Avg Reward: 263.42\n",
      "Episode 715, Reward: 262.6067, Avg Reward: 264.23\n",
      "Episode 716, Reward: 256.2679, Avg Reward: 260.27\n",
      "Episode 717, Reward: 263.8048, Avg Reward: 262.95\n",
      "Episode 718, Reward: 287.4508, Avg Reward: 262.60\n",
      "Episode 719, Reward: 294.9544, Avg Reward: 263.46\n",
      "Episode 720, Reward: 309.5719, Avg Reward: 266.05\n",
      "Episode 721, Reward: 284.7799, Avg Reward: 268.33\n",
      "Episode 722, Reward: 299.6362, Avg Reward: 272.82\n",
      "Episode 723, Reward: 302.5849, Avg Reward: 272.30\n",
      "Episode 724, Reward: 43.1715, Avg Reward: 271.25\n",
      "Episode 725, Reward: 280.8219, Avg Reward: 272.02\n",
      "Episode 726, Reward: 291.2088, Avg Reward: 271.60\n",
      "Episode 727, Reward: 251.5235, Avg Reward: 274.74\n",
      "Episode 728, Reward: 269.5609, Avg Reward: 272.81\n",
      "Episode 729, Reward: 258.5195, Avg Reward: 269.43\n",
      "Episode 730, Reward: 272.5716, Avg Reward: 270.02\n",
      "Episode 731, Reward: 269.3699, Avg Reward: 265.78\n",
      "Episode 732, Reward: 289.6804, Avg Reward: 265.34\n",
      "Episode 733, Reward: 258.5388, Avg Reward: 255.90\n",
      "Episode 734, Reward: 290.9714, Avg Reward: 256.54\n",
      "Episode 735, Reward: 312.5179, Avg Reward: 253.05\n",
      "Episode 736, Reward: 286.7381, Avg Reward: 243.90\n",
      "Episode 737, Reward: -371.4243, Avg Reward: 238.13\n",
      "Episode 738, Reward: 277.2567, Avg Reward: 236.16\n",
      "Episode 739, Reward: -174.1992, Avg Reward: 234.76\n",
      "Episode 740, Reward: 295.7016, Avg Reward: 238.77\n",
      "Episode 741, Reward: 295.8959, Avg Reward: 235.96\n",
      "Episode 742, Reward: 292.7015, Avg Reward: 233.90\n",
      "Episode 743, Reward: 68.3837, Avg Reward: 231.07\n",
      "Episode 744, Reward: 292.0950, Avg Reward: 241.37\n",
      "Episode 745, Reward: 274.6650, Avg Reward: 240.35\n",
      "Episode 746, Reward: -292.2223, Avg Reward: 236.63\n",
      "Episode 747, Reward: 268.4453, Avg Reward: 249.91\n",
      "Episode 748, Reward: 305.7125, Avg Reward: 257.33\n",
      "Episode 749, Reward: 303.5866, Avg Reward: 259.64\n",
      "Episode 750, Reward: 260.6035, Avg Reward: 264.77\n",
      "Episode 751, Reward: 249.1344, Avg Reward: 261.94\n",
      "Episode 752, Reward: 304.3614, Avg Reward: 264.48\n",
      "Episode 753, Reward: 24.4656, Avg Reward: 268.41\n",
      "Episode 754, Reward: 285.5659, Avg Reward: 269.73\n",
      "Episode 755, Reward: 270.6543, Avg Reward: 268.73\n",
      "Episode 756, Reward: 264.4212, Avg Reward: 263.63\n",
      "Episode 757, Reward: 302.7342, Avg Reward: 271.01\n",
      "Episode 758, Reward: 273.1483, Avg Reward: 271.66\n",
      "Episode 759, Reward: 309.1277, Avg Reward: 265.94\n",
      "Episode 760, Reward: 293.7446, Avg Reward: 265.10\n",
      "Episode 761, Reward: 57.4102, Avg Reward: 264.11\n",
      "Episode 762, Reward: 287.6495, Avg Reward: 262.38\n",
      "Episode 763, Reward: 54.8555, Avg Reward: 249.77\n",
      "Episode 764, Reward: 313.1377, Avg Reward: 251.42\n",
      "Episode 765, Reward: 308.6431, Avg Reward: 249.28\n",
      "Episode 766, Reward: 293.1697, Avg Reward: 248.10\n",
      "Episode 767, Reward: 292.7169, Avg Reward: 240.28\n",
      "Episode 768, Reward: -22.3145, Avg Reward: 234.49\n",
      "Episode 769, Reward: 321.5999, Avg Reward: 237.87\n",
      "Episode 770, Reward: 249.4506, Avg Reward: 235.30\n",
      "Episode 771, Reward: -38.6586, Avg Reward: 232.66\n",
      "Episode 772, Reward: 273.7701, Avg Reward: 229.61\n",
      "Episode 773, Reward: 265.4732, Avg Reward: 240.37\n",
      "Episode 774, Reward: 249.9071, Avg Reward: 244.99\n",
      "Episode 775, Reward: 288.3581, Avg Reward: 245.65\n",
      "Episode 776, Reward: 308.4160, Avg Reward: 246.56\n",
      "Episode 777, Reward: 258.6062, Avg Reward: 256.15\n",
      "Episode 778, Reward: 264.7794, Avg Reward: 261.89\n",
      "Episode 779, Reward: 266.4466, Avg Reward: 263.68\n",
      "Episode 780, Reward: 280.6266, Avg Reward: 266.21\n",
      "Episode 781, Reward: 296.0407, Avg Reward: 267.76\n",
      "Episode 782, Reward: 302.7753, Avg Reward: 273.11\n",
      "Episode 783, Reward: 269.3851, Avg Reward: 276.72\n",
      "Episode 784, Reward: 282.3642, Avg Reward: 278.01\n",
      "Episode 785, Reward: 137.3941, Avg Reward: 276.17\n",
      "Episode 786, Reward: 320.9392, Avg Reward: 274.36\n",
      "Episode 787, Reward: 288.8508, Avg Reward: 273.60\n",
      "Episode 788, Reward: 290.4818, Avg Reward: 275.05\n",
      "Episode 789, Reward: 298.8376, Avg Reward: 275.28\n",
      "Episode 790, Reward: 293.9997, Avg Reward: 274.88\n",
      "Episode 791, Reward: 294.4741, Avg Reward: 279.30\n",
      "Episode 792, Reward: 275.8788, Avg Reward: 279.54\n",
      "Episode 793, Reward: 323.2996, Avg Reward: 277.14\n",
      "Episode 794, Reward: 272.6465, Avg Reward: 276.36\n",
      "Episode 795, Reward: 289.4179, Avg Reward: 278.68\n",
      "Episode 796, Reward: 263.8866, Avg Reward: 275.47\n",
      "Episode 797, Reward: 310.2151, Avg Reward: 274.42\n",
      "Episode 798, Reward: 284.3432, Avg Reward: 273.21\n",
      "Episode 799, Reward: 286.5204, Avg Reward: 272.08\n",
      "Episode 800, Reward: 279.1484, Avg Reward: 272.38\n",
      "Episode 801, Reward: 263.1114, Avg Reward: 266.65\n",
      "Episode 802, Reward: 266.8362, Avg Reward: 264.15\n",
      "Episode 803, Reward: 307.5653, Avg Reward: 263.98\n",
      "Episode 804, Reward: 58.4385, Avg Reward: 254.90\n",
      "Episode 805, Reward: 278.4468, Avg Reward: 252.56\n",
      "Episode 806, Reward: 260.4115, Avg Reward: 255.18\n",
      "Episode 807, Reward: 251.4784, Avg Reward: 248.56\n",
      "Episode 808, Reward: 311.8933, Avg Reward: 248.22\n",
      "Episode 809, Reward: 282.2360, Avg Reward: 239.45\n",
      "Episode 810, Reward: 305.9938, Avg Reward: 232.55\n",
      "Episode 811, Reward: 280.0080, Avg Reward: 229.07\n",
      "Episode 812, Reward: 279.8755, Avg Reward: 231.09\n",
      "Episode 813, Reward: 290.9647, Avg Reward: 228.25\n",
      "Episode 814, Reward: 271.4463, Avg Reward: 230.49\n",
      "Episode 815, Reward: 264.6168, Avg Reward: 239.47\n",
      "Episode 816, Reward: 272.5929, Avg Reward: 242.87\n",
      "Episode 817, Reward: 293.8623, Avg Reward: 252.87\n",
      "Episode 818, Reward: 307.8479, Avg Reward: 254.57\n",
      "Episode 819, Reward: 288.3626, Avg Reward: 261.72\n",
      "Episode 820, Reward: 306.2744, Avg Reward: 268.23\n",
      "Episode 821, Reward: 274.1757, Avg Reward: 279.15\n",
      "Episode 822, Reward: 268.6717, Avg Reward: 279.52\n",
      "Episode 823, Reward: 284.2587, Avg Reward: 281.36\n",
      "Episode 824, Reward: 299.6669, Avg Reward: 282.99\n",
      "Episode 825, Reward: 290.3953, Avg Reward: 282.78\n",
      "Episode 826, Reward: 310.4806, Avg Reward: 283.05\n",
      "Episode 827, Reward: 275.8042, Avg Reward: 282.07\n",
      "Episode 828, Reward: 299.7229, Avg Reward: 282.12\n",
      "Episode 829, Reward: 294.4704, Avg Reward: 283.33\n",
      "Episode 830, Reward: 296.4143, Avg Reward: 281.55\n",
      "Episode 831, Reward: 281.3526, Avg Reward: 280.31\n",
      "Episode 832, Reward: 286.3897, Avg Reward: 281.44\n",
      "Episode 833, Reward: 268.3913, Avg Reward: 282.70\n",
      "Episode 834, Reward: 307.3777, Avg Reward: 282.92\n",
      "Episode 835, Reward: 300.3079, Avg Reward: 283.25\n",
      "Episode 836, Reward: 317.3165, Avg Reward: 284.49\n",
      "Episode 837, Reward: 277.7460, Avg Reward: 284.14\n",
      "Episode 838, Reward: 289.5454, Avg Reward: 284.01\n",
      "Episode 839, Reward: 297.6716, Avg Reward: 285.81\n",
      "Episode 840, Reward: 265.9596, Avg Reward: 286.19\n",
      "Episode 841, Reward: 267.4231, Avg Reward: 277.90\n",
      "Episode 842, Reward: 294.2467, Avg Reward: 278.69\n",
      "Episode 843, Reward: 282.9991, Avg Reward: 275.91\n",
      "Episode 844, Reward: 299.3617, Avg Reward: 272.76\n",
      "Episode 845, Reward: 311.0129, Avg Reward: 271.58\n",
      "Episode 846, Reward: 253.0487, Avg Reward: 271.41\n",
      "Episode 847, Reward: 281.7617, Avg Reward: 270.83\n",
      "Episode 848, Reward: 314.4993, Avg Reward: 268.77\n",
      "Episode 849, Reward: 286.9928, Avg Reward: 269.48\n",
      "Episode 850, Reward: 269.5116, Avg Reward: 274.97\n",
      "Episode 851, Reward: 264.7872, Avg Reward: 274.67\n",
      "Episode 852, Reward: 268.6689, Avg Reward: 275.10\n",
      "Episode 853, Reward: 252.6103, Avg Reward: 278.17\n",
      "Episode 854, Reward: 280.7627, Avg Reward: 275.84\n",
      "Episode 855, Reward: 272.7441, Avg Reward: 274.13\n",
      "Episode 856, Reward: 254.5431, Avg Reward: 272.77\n",
      "Episode 857, Reward: 319.7222, Avg Reward: 273.49\n",
      "Episode 858, Reward: 265.9075, Avg Reward: 268.57\n",
      "Episode 859, Reward: 286.7395, Avg Reward: 267.40\n",
      "Episode 860, Reward: 283.7100, Avg Reward: 266.08\n",
      "Episode 861, Reward: 293.1257, Avg Reward: 262.35\n",
      "Episode 862, Reward: 292.3531, Avg Reward: 262.01\n",
      "Episode 863, Reward: 300.3676, Avg Reward: 261.40\n",
      "Episode 864, Reward: -244.7475, Avg Reward: 257.18\n",
      "Episode 865, Reward: -247.6900, Avg Reward: 252.11\n",
      "Episode 866, Reward: 293.0701, Avg Reward: 253.10\n",
      "Episode 867, Reward: 314.4280, Avg Reward: 255.73\n",
      "Episode 868, Reward: 273.9659, Avg Reward: 250.48\n",
      "Episode 869, Reward: 310.4492, Avg Reward: 244.26\n",
      "Episode 870, Reward: 284.9579, Avg Reward: 239.55\n",
      "Episode 871, Reward: 280.7367, Avg Reward: 239.37\n",
      "Episode 872, Reward: 44.4465, Avg Reward: 240.22\n",
      "Episode 873, Reward: 309.6874, Avg Reward: 242.80\n",
      "Episode 874, Reward: 281.3864, Avg Reward: 234.27\n",
      "Episode 875, Reward: 294.6183, Avg Reward: 242.50\n",
      "Episode 876, Reward: 296.9033, Avg Reward: 250.21\n",
      "Episode 877, Reward: 280.2565, Avg Reward: 245.06\n",
      "Episode 878, Reward: 276.5533, Avg Reward: 248.07\n",
      "Episode 879, Reward: 289.7399, Avg Reward: 252.37\n",
      "Episode 880, Reward: 255.7304, Avg Reward: 262.42\n",
      "Episode 881, Reward: 135.7136, Avg Reward: 260.91\n",
      "Episode 882, Reward: 302.8632, Avg Reward: 260.35\n",
      "Episode 883, Reward: 272.5527, Avg Reward: 260.50\n",
      "Episode 884, Reward: 289.7554, Avg Reward: 268.46\n",
      "Episode 885, Reward: 299.5608, Avg Reward: 266.84\n",
      "Episode 886, Reward: -283.2266, Avg Reward: 257.88\n",
      "Episode 887, Reward: 258.8473, Avg Reward: 258.87\n",
      "Episode 888, Reward: 277.5797, Avg Reward: 260.38\n",
      "Episode 889, Reward: 162.9348, Avg Reward: 263.57\n",
      "Episode 890, Reward: 288.2048, Avg Reward: 256.15\n",
      "Episode 891, Reward: 297.6566, Avg Reward: 258.30\n",
      "Episode 892, Reward: -227.5817, Avg Reward: 248.81\n",
      "Episode 893, Reward: 310.0058, Avg Reward: 240.38\n",
      "Episode 894, Reward: 270.7313, Avg Reward: 236.24\n",
      "Episode 895, Reward: 266.4626, Avg Reward: 232.20\n",
      "Episode 896, Reward: 274.5518, Avg Reward: 236.62\n",
      "Episode 897, Reward: -15.9575, Avg Reward: 232.45\n",
      "Episode 898, Reward: 315.7445, Avg Reward: 228.88\n",
      "Episode 899, Reward: 304.8691, Avg Reward: 226.44\n",
      "Episode 900, Reward: 281.5440, Avg Reward: 225.99\n",
      "Episode 901, Reward: 262.5678, Avg Reward: 226.71\n",
      "Episode 902, Reward: 293.6544, Avg Reward: 238.48\n",
      "Episode 903, Reward: 276.4037, Avg Reward: 243.33\n",
      "Episode 904, Reward: 284.3796, Avg Reward: 240.97\n",
      "Episode 905, Reward: 300.0196, Avg Reward: 242.27\n",
      "Episode 906, Reward: 294.8787, Avg Reward: 248.12\n",
      "Episode 907, Reward: 277.5967, Avg Reward: 254.80\n",
      "Episode 908, Reward: 281.0323, Avg Reward: 254.05\n",
      "Episode 909, Reward: 245.5189, Avg Reward: 248.29\n",
      "Episode 910, Reward: 255.2548, Avg Reward: 251.61\n",
      "Episode 911, Reward: 271.7984, Avg Reward: 244.63\n",
      "Episode 912, Reward: 276.2575, Avg Reward: 247.93\n",
      "Episode 913, Reward: 318.8825, Avg Reward: 242.68\n",
      "Episode 914, Reward: 284.6349, Avg Reward: 248.40\n",
      "Episode 915, Reward: 262.7498, Avg Reward: 233.39\n",
      "Episode 916, Reward: 300.7966, Avg Reward: 233.29\n",
      "Episode 917, Reward: 310.4908, Avg Reward: 228.47\n",
      "Episode 918, Reward: 273.5719, Avg Reward: 226.87\n",
      "Episode 919, Reward: 265.7349, Avg Reward: 213.39\n",
      "Episode 920, Reward: -150.1962, Avg Reward: 202.88\n",
      "Episode 921, Reward: 266.5530, Avg Reward: 205.41\n",
      "Episode 922, Reward: 259.1544, Avg Reward: 205.77\n",
      "Episode 923, Reward: 296.6719, Avg Reward: 204.32\n",
      "Episode 924, Reward: 259.3604, Avg Reward: 201.29\n",
      "Episode 925, Reward: -253.7038, Avg Reward: 195.06\n",
      "Episode 926, Reward: 304.4065, Avg Reward: 195.18\n",
      "Episode 927, Reward: -106.2815, Avg Reward: 182.54\n",
      "Episode 928, Reward: 267.0219, Avg Reward: 190.64\n",
      "Episode 929, Reward: 277.6846, Avg Reward: 190.09\n",
      "Episode 930, Reward: 295.3485, Avg Reward: 191.61\n",
      "Episode 931, Reward: 265.0310, Avg Reward: 187.71\n",
      "Episode 932, Reward: -371.8102, Avg Reward: 173.60\n",
      "Episode 933, Reward: 286.9247, Avg Reward: 190.34\n",
      "Episode 934, Reward: 286.5032, Avg Reward: 189.29\n",
      "Episode 935, Reward: 278.1322, Avg Reward: 188.85\n",
      "Episode 936, Reward: 279.8866, Avg Reward: 182.35\n",
      "Episode 937, Reward: 312.0233, Avg Reward: 187.86\n",
      "Episode 938, Reward: 7.2618, Avg Reward: 180.42\n",
      "Episode 939, Reward: 255.6204, Avg Reward: 181.07\n",
      "Episode 940, Reward: -180.6146, Avg Reward: 178.19\n",
      "Episode 941, Reward: 62.1198, Avg Reward: 179.56\n",
      "Episode 942, Reward: 270.5607, Avg Reward: 191.50\n",
      "Episode 943, Reward: 308.4968, Avg Reward: 199.87\n",
      "Episode 944, Reward: -94.9657, Avg Reward: 199.21\n",
      "Episode 945, Reward: 252.1166, Avg Reward: 204.53\n",
      "Episode 946, Reward: 262.8723, Avg Reward: 205.77\n",
      "Episode 947, Reward: 242.0193, Avg Reward: 211.03\n",
      "Episode 948, Reward: -1.9030, Avg Reward: 213.74\n",
      "Episode 949, Reward: 295.6490, Avg Reward: 220.28\n",
      "Episode 950, Reward: 286.1521, Avg Reward: 230.02\n",
      "Episode 951, Reward: 257.5108, Avg Reward: 227.47\n",
      "Episode 952, Reward: 262.9421, Avg Reward: 234.07\n",
      "Episode 953, Reward: 244.2750, Avg Reward: 234.55\n",
      "Episode 954, Reward: 263.1968, Avg Reward: 240.34\n",
      "Episode 955, Reward: 38.8607, Avg Reward: 238.22\n",
      "Episode 956, Reward: 299.6317, Avg Reward: 235.17\n",
      "Episode 957, Reward: 297.2609, Avg Reward: 227.68\n",
      "Episode 958, Reward: 242.1391, Avg Reward: 222.43\n",
      "Episode 959, Reward: 299.3543, Avg Reward: 223.28\n",
      "Episode 960, Reward: 298.3368, Avg Reward: 228.85\n",
      "Episode 961, Reward: 276.5493, Avg Reward: 223.15\n",
      "Episode 962, Reward: 278.2061, Avg Reward: 230.06\n",
      "Episode 963, Reward: 291.2898, Avg Reward: 234.00\n",
      "Episode 964, Reward: 267.8086, Avg Reward: 233.93\n",
      "Episode 965, Reward: 275.3024, Avg Reward: 235.25\n",
      "Episode 966, Reward: 276.7230, Avg Reward: 241.18\n",
      "Episode 967, Reward: 279.2396, Avg Reward: 244.22\n",
      "Episode 968, Reward: 263.6749, Avg Reward: 246.66\n",
      "Episode 969, Reward: 261.3571, Avg Reward: 259.02\n",
      "Episode 970, Reward: 264.9517, Avg Reward: 265.26\n",
      "Episode 971, Reward: 294.2963, Avg Reward: 266.13\n",
      "Episode 972, Reward: 273.6709, Avg Reward: 265.13\n",
      "Episode 973, Reward: 278.9606, Avg Reward: 270.63\n",
      "Episode 974, Reward: 265.8782, Avg Reward: 272.98\n",
      "Episode 975, Reward: 291.4627, Avg Reward: 271.51\n",
      "Episode 976, Reward: 274.0483, Avg Reward: 274.36\n",
      "Episode 977, Reward: 308.7664, Avg Reward: 274.61\n",
      "Episode 978, Reward: 281.0662, Avg Reward: 274.26\n",
      "Episode 979, Reward: 307.4653, Avg Reward: 274.51\n",
      "Episode 980, Reward: 275.4125, Avg Reward: 277.05\n",
      "Episode 981, Reward: 274.8998, Avg Reward: 276.36\n",
      "Episode 982, Reward: 299.1012, Avg Reward: 275.87\n",
      "Episode 983, Reward: 262.5573, Avg Reward: 276.31\n",
      "Episode 984, Reward: 278.9077, Avg Reward: 276.47\n",
      "Episode 985, Reward: 287.5315, Avg Reward: 276.85\n",
      "Episode 986, Reward: 283.5261, Avg Reward: 276.88\n",
      "Episode 987, Reward: 280.8026, Avg Reward: 276.68\n",
      "Episode 988, Reward: 141.9487, Avg Reward: 273.86\n",
      "Episode 989, Reward: 289.5491, Avg Reward: 273.20\n",
      "Episode 990, Reward: 160.4545, Avg Reward: 271.06\n",
      "Episode 991, Reward: 269.6311, Avg Reward: 268.67\n",
      "Episode 992, Reward: 306.4828, Avg Reward: 268.85\n",
      "Episode 993, Reward: 251.4970, Avg Reward: 268.56\n",
      "Episode 994, Reward: 293.1993, Avg Reward: 268.43\n",
      "Episode 995, Reward: 274.8933, Avg Reward: 267.00\n",
      "Episode 996, Reward: 182.0628, Avg Reward: 265.52\n",
      "Episode 997, Reward: 166.7750, Avg Reward: 263.79\n",
      "Episode 998, Reward: 303.8493, Avg Reward: 263.15\n",
      "Episode 999, Reward: 169.5389, Avg Reward: 261.79\n",
      "Episode 1000, Reward: 192.7186, Avg Reward: 259.69\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "# import torch.utils.data.dataloader\n",
    "import config\n",
    "\n",
    "#Policy Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, HIDDEN_DIM = config.HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(HIDDEN_DIM, action_dim)\n",
    "        self.critic = nn.Linear(HIDDEN_DIM, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "    \n",
    "    def act(self, x):\n",
    "        state = torch.FloatTensor(x)\n",
    "        logits, value = self.forward(state)\n",
    "        probs = F.softmax(logits, dim=-1) \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "\n",
    "class PPOTrainer():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        self.policy = ActorCritic(self.state_dim, self.action_dim, config.HIDDEN_DIM)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=config.LR)\n",
    "        self.ep_rewards = []\n",
    "        self.best_avg = -np.inf\n",
    "    \n",
    "    def compute_advantage(self, rewards, next_values, values, dones):\n",
    "        deltas = rewards + config.GAMMA * next_values * (1 - dones) - values\n",
    "        advantages = np.zeros_like(deltas)\n",
    "        last_advantage = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            advantages[t] = deltas[t] + config.GAE_LAMBDA * config.GAMMA * (1-dones[t]) * last_advantage\n",
    "            last_advantage = advantages[t]\n",
    "        return (advantages - advantages.mean())/(advantages.std() + 1e-8)\n",
    "    \n",
    "    # log_probs, returns, advantages, states, actions\n",
    "    def update(self, old_log_probs, returns, advantages, states, old_actions):\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "        old_actions = torch.LongTensor(old_actions)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(old_log_probs, old_actions,returns, advantages, states)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        for _ in range(config.NUM_EPOCHS):\n",
    "            for batch in loader:\n",
    "                old_lp, a, ret, adv, s = batch\n",
    "                \n",
    "                logits, values = self.policy(s)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                dist = Categorical(probs)\n",
    "                new_log_prob = dist.log_prob(a)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                ratio = (new_log_prob - old_lp).exp()\n",
    "                surr1 = ratio * adv\n",
    "                surr2 = torch.clamp(ratio, 1 - config.CLIP_EPS, 1 + config.CLIP_EPS) * adv\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                value_loss = F.mse_loss(values.squeeze(), ret)\n",
    "                \n",
    "                loss = policy_loss + value_loss * config.VALUE_COEF - entropy * config.ENTROPY_COEF\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        state, _ = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for episode in range(config.MAX_EPISODES):\n",
    "            states = [] \n",
    "            rewards = [] \n",
    "            actions = [] \n",
    "            dones = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            \n",
    "            for step in range(config.NUM_STEPS):\n",
    "                action, log_prob, value = self.policy.act(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                done = done or truncated\n",
    "                \n",
    "                states.append(state)\n",
    "                rewards.append(reward)\n",
    "                actions.append(action)\n",
    "                dones.append(done)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    state, _ = self.env.reset()\n",
    "                    self.ep_rewards.append(episode_reward)\n",
    "                    episode_reward = 0 \n",
    "                    \n",
    "            next_states = torch.FloatTensor(np.array([s for s in states]))\n",
    "            with torch.no_grad():\n",
    "                _, next_values = self.policy(next_states)\n",
    "            next_values = next_values.cpu().numpy().flatten()\n",
    "            next_values = np.append(next_values[1:], 0)\n",
    "            next_values[dones] = 0\n",
    "            \n",
    "            #Cpnverting to numpy arrays\n",
    "            rewards = np.array(rewards)\n",
    "            values = np.array(values)\n",
    "            dones = np.array(dones).astype(np.float32)\n",
    "            \n",
    "            advantages = self.compute_advantage(rewards, next_values, values, dones)\n",
    "            returns = advantages + values\n",
    "            \n",
    "            self.update(log_probs, returns, advantages, states, actions)\n",
    "            \n",
    "            avg_reward = np.mean(self.ep_rewards[-100:]) if len(self.ep_rewards) >= 100 else np.mean(self.ep_rewards)\n",
    "            if avg_reward > self.best_avg:\n",
    "                self.best_avg = avg_reward\n",
    "            print(f\"Episode {episode+1}, Reward: {self.ep_rewards[-1]:.4f}, Avg Reward: {avg_reward:.2f}\")\n",
    "            \n",
    "        torch.save(self.policy.state_dict(), 'ppo_model.pt')\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"LunarLander-v3\", continuous=False)\n",
    "    trainer = PPOTrainer(env)\n",
    "    trainer.train()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_model(policy, filename='ppo_model.pt'):\n",
    "    \"\"\"\n",
    "    Load a trained policy network\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        policy.load_state_dict(torch.load(filename))\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No model found at {filename}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict loaded successfully from ppo_model.pt\n",
      "Episode 1: Total Reward = 272.81, Steps = 255\n",
      "Episode 2: Total Reward = 296.44, Steps = 301\n",
      "Episode 3: Total Reward = 277.91, Steps = 239\n",
      "Episode 4: Total Reward = 268.14, Steps = 270\n",
      "Episode 5: Total Reward = 266.59, Steps = 243\n",
      "Inference finished.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import os\n",
    "\n",
    "class MockConfig:\n",
    "    NUM_STEPS = 2048          # Number of steps per environment per update\n",
    "    BATCH_SIZE = 64           # Mini-batch size for updates\n",
    "    NUM_EPOCHS = 10           # Number of optimization epochs per update\n",
    "    GAMMA = 0.99              # Discount factor\n",
    "    GAE_LAMBDA = 0.95         # GAE parameter\n",
    "    CLIP_EPS = 0.2            # PPO clip parameter\n",
    "    LR = 3e-4                 # Learning rate\n",
    "    HIDDEN_DIM = 256          # Network hidden layer size\n",
    "    ENTROPY_COEF = 0.01       # Entropy coefficient\n",
    "    VALUE_COEF = 0.5          # Value loss coefficient\n",
    "    MAX_EPISODES = 300        # Maximum training episodes\n",
    "\n",
    "config = MockConfig()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, HIDDEN_DIM = config.HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(HIDDEN_DIM, action_dim)\n",
    "        self.critic = nn.Linear(HIDDEN_DIM, 1) # Critic not used in inference, but part of the model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "    \n",
    "    # To use deterministic action (argmax) or sampling based on preference\n",
    "    def act_inference(self, x, deterministic=True):\n",
    "        state = torch.FloatTensor(x).unsqueeze(0) # To add batch dimension\n",
    "        with torch.no_grad(): \n",
    "            logits, _ = self.forward(state)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(probs, dim=-1).item()\n",
    "            else:\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample().item()\n",
    "        return action\n",
    "\n",
    "\n",
    "def load_model(policy, filename='ppo_model.pt'):\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            state_dict = torch.load(filename)\n",
    "            # Load it into the policy network\n",
    "            policy.load_state_dict(state_dict)\n",
    "            # Setting the model to evaluation mode\n",
    "            policy.eval()\n",
    "            print(f\"Model state_dict loaded successfully from {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from {filename}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"Error: No model found at {filename}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"LunarLander-v3\" \n",
    "    env = gym.make(env_name, continuous=False, render_mode=\"human\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Instantiating the policy network\n",
    "    policy = ActorCritic(state_dim, action_dim, config.HIDDEN_DIM)\n",
    "\n",
    "    # Load the trained weights\n",
    "    model_loaded = load_model(policy, 'ppo_model.pt')\n",
    "\n",
    "    if model_loaded:\n",
    "        num_episodes = 5 # Number of episodes to run for testing\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            truncated = False\n",
    "            total_reward = 0\n",
    "            step_count = 0\n",
    "\n",
    "            while not done and not truncated:\n",
    "                # Choose deterministic=True for the 'best' action,\n",
    "                # or deterministic=False to sample like during training.\n",
    "                action = policy.act_inference(state, deterministic=True)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {total_reward:.2f}, Steps = {step_count}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Could not load the model. Exiting inference.\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"Inference finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Parameters\n",
    "CLIP_EPS = 0.2              #PPO clip parameter\n",
    "NUM_EPOCHS = 10             #number of times the ppo will be updated\n",
    "HIDDEN_DIM = 256            #N/w Hidden layer sizeeee\n",
    "MAX_EPISODES = 500          #Max Episode to run\n",
    "MAX_TIMESTEPS = 1500        #Max No of timesteps in 1 Ep\n",
    "LR = 3e-4                   #Learning rate for the N/W\n",
    "GAE_LAMBDA = 0.95           #GAE Parameter\n",
    "GAMMA = 0.99                #Discount Factoor\n",
    "BATCH_SIZE = 64             #Batch size for dataloadear\n",
    "ENTROPY_COEF = 0.01         # Entropy coefficient\n",
    "VALUE_COEF = 0.5            # Value loss coefficient\n",
    "\n",
    "#policy-n/w\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim): # state_dim, action_dim -> 8,4 (For lunarlander)\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(HIDDEN_DIM, action_dim)\n",
    "        self.critic = nn.Linear(HIDDEN_DIM, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "    \n",
    "    def act(self, x):\n",
    "        state = torch.FloatTensor(x)\n",
    "        logits, value = self.forward(state)\n",
    "        probs = F.softmax(logits, dim=-1) \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "#Generalized advantage estimation\n",
    "def compute_gae(rewards, next_values, dones, values):\n",
    "    #δₜ = rₜ + γ * V(s_{t+1}) - V(sₜ)\n",
    "    deltas = rewards + GAMMA * next_values * (1- dones) - values\n",
    "    advantages = np.zeros_like(deltas)\n",
    "    lastadv = 0\n",
    "    for t in reversed(range(len(deltas))):\n",
    "        #A _t = δₜ + γ * λ * (1-dones) * A_t+1 \n",
    "        advantages[t] = deltas[t] + GAMMA * GAE_LAMBDA * (1 - dones[t]) * lastadv\n",
    "        lastadv = advantages[t]\n",
    "    return (advantages - advantages.mean() ) / (advantages.std() + 1e-8)\n",
    "    \n",
    "\n",
    "def update(states, returns, advantages, old_log_probs, actions, policy, optimizer):\n",
    "    states = torch.FloatTensor(np.array(states))\n",
    "    actions = torch.LongTensor(actions)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    advantages = torch.FloatTensor(advantages)\n",
    "    old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(states, actions, returns, advantages, old_log_probs)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    #Update\n",
    "    for _ in range(NUM_EPOCHS):\n",
    "        for btc in loader:\n",
    "            s, a, ret, adv, old_pb = btc\n",
    "            \n",
    "            logits, values = policy(s) #latest new val, came from acting in env with old states\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            dist = Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(a) \n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            ratio = (new_log_probs - old_pb).exp()\n",
    "            surr1 = ratio*adv\n",
    "            surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS)*adv\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(values.squeeze(), ret)\n",
    "            loss = policy_loss + value_loss*VALUE_COEF - entropy*ENTROPY_COEF \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "best_avg = -np.inf\n",
    "ep_rewards = []\n",
    "def train():\n",
    "    global best_avg\n",
    "    env = gym.make(\"LunarLander-v3\", continuous=False)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    policy = ActorCritic(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        actions = []\n",
    "        dones = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        states = []\n",
    "        values = []\n",
    "        \n",
    "        for step in range(MAX_TIMESTEPS):\n",
    "            state = torch.FloatTensor(state)\n",
    "            action, log_prob, value = policy.act(state)\n",
    "            next_state, reward, done , truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "                ep_rewards.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "        \n",
    "        \n",
    "        next_states = torch.FloatTensor(np.array([s for s in states]))\n",
    "        with torch.no_grad():\n",
    "            _, next_values = policy(next_states)\n",
    "        next_values = np.append(next_values[1:], 0)\n",
    "        next_values[dones] = 0\n",
    "        \n",
    "        #Converting to numoy arrays\n",
    "        rewards = np.array(rewards)\n",
    "        values = np.array(values)\n",
    "        dones = np.array(dones).astype(np.float32)\n",
    "        \n",
    "        advantages = compute_gae(rewards, next_values, dones, values)\n",
    "        returns = advantages + values\n",
    "        \n",
    "        #To update policy\n",
    "        update(states, returns, advantages, log_probs, actions, policy, optimizer)\n",
    "        \n",
    "        avg_reward = np.mean(ep_rewards[-100:]) if len(ep_rewards) >= 100 else np.mean(ep_rewards)\n",
    "        if avg_reward > best_avg:\n",
    "            best_avg = avg_reward\n",
    "        print(f\"Episode {episode+1}, Reward: {ep_rewards[-1]}, Avg Reward: {avg_reward:.2f}\")\n",
    "        \n",
    "    env.close()\n",
    "        \n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Actor-Critic Networks\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Actor network (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def act(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "# PPO Agent\n",
    "class PPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,              # 8\n",
    "        action_dim,             # 1\n",
    "        lr_actor=0.0003,\n",
    "        lr_critic=0.001,\n",
    "        gamma=0.99,\n",
    "        K_epochs=4,\n",
    "        eps_clip=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "        \n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())       #copying the params of policy instance\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action, action_logprob = self.policy_old.act(state)\n",
    "        \n",
    "        return action.item(), action_logprob.item()\n",
    "    \n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        old_states = torch.stack(memory.states).detach().to(device)\n",
    "        old_actions = torch.tensor(memory.actions, dtype=torch.int64).detach().to(device)\n",
    "        old_logprobs = torch.tensor(memory.logprobs, dtype=torch.float32).detach().to(device)\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (π_θ / π_θ_old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            \n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            \n",
    "            # Final loss of clipped objective PPO\n",
    "            value_loss = self.MseLoss(state_values, rewards)\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            entropy_loss = -dist_entropy.mean()\n",
    "            \n",
    "            loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "            \n",
    "            #Take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "# Memory for storing transitions\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    env_name = \"LunarLander-v3\"\n",
    "    env = gym.make(env_name, continuous = False)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0] # 8\n",
    "    action_dim = env.action_space.n # 1\n",
    "    \n",
    "    max_episodes = 5000        # max training episodes\n",
    "    max_timesteps = 1500       # max timesteps in one episode\n",
    "    update_timestep = 2000     # update policy every n timesteps\n",
    "    save_interval = 500        # to save model every n episodes\n",
    "    \n",
    "    # PPO hyperparameters\n",
    "    K_epochs = 8               # update policy for K epochs\n",
    "    eps_clip = 0.2             # clip parameter for PPO\n",
    "    gamma = 0.98               # discount factor\n",
    "    lr_actor = 0.0003          # learning rate for actor\n",
    "    lr_critic = 0.001          # learning rate for critic \n",
    "    \n",
    "    # Initialize PPO agent\n",
    "    ppo = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
    "    memory = Memory()\n",
    "    \n",
    "    # Logging variables\n",
    "    running_reward = 0\n",
    "    avg_length = 0\n",
    "    timestep = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            timestep += 1\n",
    "            \n",
    "            # Select action from policy\n",
    "            action, logprob = ppo.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Save in memory\n",
    "            memory.states.append(torch.FloatTensor(state).to(device))\n",
    "            memory.actions.append(action)\n",
    "            memory.logprobs.append(logprob)\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            # Update if its time\n",
    "            if timestep % update_timestep == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "                timestep = 0\n",
    "            \n",
    "            running_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        avg_length += t\n",
    "        \n",
    "        # Print average reward and length every 10 episodes\n",
    "        if i_episode % 10 == 0:\n",
    "            avg_length = avg_length/10\n",
    "            running_reward = running_reward/10\n",
    "            \n",
    "            print(f'Episode {i_episode} \\t Avg length: {avg_length:.2f} \\t Reward: {running_reward:.2f}')\n",
    "            running_reward = 0\n",
    "            avg_length = 0\n",
    "            \n",
    "        # Save model\n",
    "        if i_episode % save_interval == 0:\n",
    "            torch.save(ppo.policy.state_dict(), f'./PPO_LunarLander_{i_episode}.pth')\n",
    "            \n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting bad results w this implementation, Don't know why -> at least for nowwwww\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95  \n",
    "EPSILON = 0.2  # clipping range for PPO -> usually 0.1 or 0.2 rhta\n",
    "ACTOR_LR = 3e-4\n",
    "CRITIC_LR = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 4\n",
    "NUM_EPISODES = 1000\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "#Policy network as usual\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "#value function\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "\n",
    "#Computing GAE\n",
    "def compute_gae(rewards, values, gamma=GAMMA, lam=GAE_LAMBDA):\n",
    "    \"\"\"\n",
    "    Iska explanation notes.ipynb me snippets dala hai\n",
    "    and\n",
    "    Here lambda is a hyperparameter, It can be multiplied with : \n",
    "    gamma and last_advantage for BIAS & VARIANCE Trade off\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    last_advantage = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t+1] - values[t]        #also known as one step TD Error : δt^V​=rt​+γV(st+1​)−V(st​)\n",
    "        last_advantage = delta + gamma * lam * last_advantage       #A(t)GAE(γ,λ)​ = δt^V ​+ γ * λ * A(t+1)GAE(γ,λ)\n",
    "        advantages.insert(0, last_advantage)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    # Calculate : advantages = returns - values\n",
    "    # Calculate : returns = advantages + values\n",
    "    returns = advantages + torch.tensor(values[:-1], dtype=torch.float32)\n",
    "    return advantages, returns\n",
    "\n",
    "# Training loop\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    if(episode > 900):\n",
    "        env = gym.make(\"LunarLander-v3\", continuous=False, render_mode='human')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    \n",
    "    # Collect trajectory\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_probs = actor(state_tensor)\n",
    "        value = critic(state_tensor).squeeze()\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.item()) \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    # appending final state value for GAE calculation\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        final_value = critic(state_tensor).squeeze()\n",
    "    values.append(final_value.item())\n",
    "    \n",
    "    # Compute advantages and returns\n",
    "    advantages, returns = compute_gae(rewards, values)\n",
    "    \n",
    "    # Convert lists to tensors for batch processing\n",
    "    # Adding detach to ensure no gradient flow\n",
    "    old_states = torch.cat(states).detach()         #torch.Size([104, 8])\n",
    "    old_actions = torch.stack(actions).detach()     #torch.Size([104, 1]) = [[n1], [n2], [n3],... [n_steps]]\n",
    "    old_log_probs = torch.stack(log_probs).detach() #torch.Size([104, 1]) = [[n1], [n2], [n3],... [n_steps]]\n",
    "    # print(old_actions)\n",
    "   \n",
    "    # PPO update\n",
    "    for _ in range(EPOCHS):\n",
    "        # Process in mini-batches\n",
    "        indices = np.arange(len(old_states))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, len(old_states), BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, len(old_states))\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            batch_states = old_states[batch_indices]\n",
    "            batch_actions = old_actions[batch_indices]\n",
    "            batch_log_probs = old_log_probs[batch_indices]\n",
    "            batch_advantages = advantages[batch_indices]\n",
    "            batch_returns = returns[batch_indices]\n",
    "            \n",
    "            # Actor update\n",
    "            new_action_probs = actor(batch_states)\n",
    "            new_dist = Categorical(new_action_probs)\n",
    "            new_log_probs = new_dist.log_prob(batch_actions)\n",
    "            \n",
    "            # Calculate policy ratio and surrogate loss\n",
    "            ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "            surr1 = ratio * batch_advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - EPSILON, 1.0 + EPSILON) * batch_advantages\n",
    "            \n",
    "            # Negative because we're minimizing, but we want to maximize the objective\n",
    "            entropy = new_dist.entropy().mean()\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "            \n",
    "            # Update actor\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor.parameters(), 1)\n",
    "            optimizer_actor.step()\n",
    "            \n",
    "            # Critic update - completely separate calculation\n",
    "            critic_values = critic(batch_states).squeeze()\n",
    "            critic_loss = nn.MSELoss()(critic_values, batch_returns)\n",
    "            \n",
    "            # Update critic\n",
    "            optimizer_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), 1)\n",
    "            optimizer_critic.step()\n",
    "            \n",
    "    episode_rewards.append(episode_reward)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f\"Episode {episode+1}/{NUM_EPISODES}, Avg Reward (last 10): {avg_reward:.2f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting bad results ;-;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
