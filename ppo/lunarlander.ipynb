{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In descrete space -> the actions will be either 0,1,2,3\n",
    "#In continous space -> the actions will be for main and side engines only with [1,1] jaha 1,1 ki values flunctuate hogi\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "               enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.Softmax(dim=-1)\n",
    "            #nn.Softmax(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "model = SimpleModel()\n",
    "\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    # Convert observation to tensor\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Get action probabilities\n",
    "    action_probs = model(obs_tensor)\n",
    "    \n",
    "    # Sample action based on probabilities\n",
    "    action_dist = torch.distributions.Categorical(action_probs)\n",
    "    action = action_dist.sample()\n",
    "    \n",
    "    # Prepare target (one-hot encoded)\n",
    "    y_target = torch.zeros_like(action_probs)      # [[0,0,0,0]] \n",
    "    y_target[0, action] = 1.0                      # [[0,0,1,0]] -> one hot encoded for loss calculation\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(action_probs, y_target)\n",
    "    \n",
    "    # Compute gradient\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Take action in environment\n",
    "    action_int = int(action.item())\n",
    "    obs, reward, done, truncated, info = env.step(action_int)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = [param.grad.clone() for param in model.parameters()]\n",
    "    \n",
    "    return obs, reward, done, truncated, grads\n",
    "\n",
    "\n",
    "\n",
    "def play_multiple_episodes(env, n_max_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_max_episodes):\n",
    "        curr_rews = []\n",
    "        curr_grds = []\n",
    "        observation, info = env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            obs, reward, done, truncated, grads = play_one_step(env, observation, model, loss_fn)\n",
    "            curr_rews.append(reward)\n",
    "            curr_grds.append(grads)\n",
    "        all_rewards.append(curr_rews) \n",
    "        all_grads.append(curr_grds) \n",
    "    return all_rewards, all_grads\n",
    "\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for i in range(len(rewards)-2, -1, -1):\n",
    "        discounted[i] += discounted[i+1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(single_rew, discount_factor) for single_rew in all_rewards]\n",
    "    flattened_rewards = np.concatenate(all_discounted_rewards)\n",
    "    mean = flattened_rewards.mean()\n",
    "    std = flattened_rewards.std()\n",
    "    return [(rew - mean)/std for rew in all_discounted_rewards]\n",
    "\n",
    "\n",
    "n_iterations = 200\n",
    "n_episodes = 15\n",
    "n_steps_per_ep = 1000\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for iter in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes, n_steps_per_ep, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    \n",
    "    all_mean_grads = []\n",
    "    \n",
    "    for var_index, param in enumerate(model.parameters()):\n",
    "        mean_grad = torch.mean(\n",
    "            torch.stack([\n",
    "                final_rew * all_grads[episode_index][step][var_index]\n",
    "                for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                for step, final_rew in enumerate(final_rewards)\n",
    "            ])\n",
    "            ,dim=0\n",
    "        )\n",
    "        all_mean_grads.append(mean_grad)\n",
    "    \n",
    "    for param, mean_grads in zip(model.parameters(), all_mean_grads):\n",
    "        param.grad = mean_grads\n",
    "    optimizer.step()  # Update model parameters\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean -> -109.884\n",
      "Max -> 60\n",
      "Min -> -296\n"
     ]
    }
   ],
   "source": [
    "# Let's test the model\n",
    "\n",
    "def nn_policy(obs):\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "    action_probs = model(obs_tensor)\n",
    "    action_dist = torch.distributions.Categorical(action_probs)\n",
    "    action = action_dist.sample()\n",
    "    return int(action.item())\n",
    "\n",
    "total = []\n",
    "for episode in range(500): #running for 500 episodes\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    episode_reward = 0\n",
    "    for step in range(200): #Each episode runs for 200 steps\n",
    "        action = nn_policy(obs)\n",
    "        obs, curr_reward, done, truncated, info  = env.step(action=action)\n",
    "        episode_reward += int(curr_reward)\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    total.append(episode_reward)\n",
    "    \n",
    "print(\"Mean ->\",np.mean(total))\n",
    "print(\"Max ->\",max(total))\n",
    "print(\"Min ->\",min(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Looks like the agent is as clueless as I'm ;-;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Reward: -289.83\n",
      "Episode 10, Average Reward: -221.74\n",
      "Episode 20, Average Reward: -197.19\n",
      "Episode 30, Average Reward: -191.02\n",
      "Episode 40, Average Reward: -191.14\n",
      "Episode 50, Average Reward: -180.78\n",
      "Episode 60, Average Reward: -170.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 79\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env_name, episodes)\u001b[0m\n\u001b[0;32m     76\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 79\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     81\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_reward(reward)\n",
      "Cell \u001b[1;32mIn[65], line 35\u001b[0m, in \u001b[0;36mREINFORCE.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     34\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(state)\n\u001b[1;32m---> 35\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m action \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs\u001b[38;5;241m.\u001b[39mappend(distribution\u001b[38;5;241m.\u001b[39mlog_prob(action))\n",
      "File \u001b[1;32mc:\\Users\\SHIVAM\\Desktop\\ReinforcementLearning\\rlenv\\lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHIVAM\\Desktop\\ReinforcementLearning\\rlenv\\lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     68\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 69\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "#Policy Network or model\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# REINFORCE Algorithm\n",
    "class REINFORCE:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99):\n",
    "        self.policy = PolicyNetwork(input_dim, output_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action_probs = self.policy(state)\n",
    "        distribution = torch.distributions.Categorical(action_probs) \n",
    "        action = distribution.sample()\n",
    "        self.log_probs.append(distribution.log_prob(action))\n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \n",
    "        discounted_rewards = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            discounted_rewards.insert(0, G)\n",
    "        \n",
    "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        loss = 0\n",
    "        for log_prob, G in zip(self.log_probs, discounted_rewards):\n",
    "            loss += -log_prob * G\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train(env_name=\"LunarLander-v3\", episodes=1000):\n",
    "    env = gym.make(env_name, render_mode=None)\n",
    "    input_dim = env.observation_space.shape[0]    #8\n",
    "    output_dim = env.action_space.n               #4\n",
    "    agent = REINFORCE(input_dim, output_dim)\n",
    "    \n",
    "    reward_history = deque(maxlen=100)\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        agent.update_policy()\n",
    "        reward_history.append(total_reward)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Average Reward: {np.mean(reward_history):.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(env_name=\"LunarLander-v3\", episodes=5):\n",
    "    env = gym.make(env_name, render_mode=\"human\")  # Enable rendering\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    agent = REINFORCE(input_dim, output_dim)  # Load trained model if available\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.select_action(state)  # Choose action\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        print(f\"Episode {episode}: Total Reward = {total_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()  # Train the agent\n",
    "    visualize()  # Visualize after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -415.17, Average Reward: -415.17\n",
      " Max : 120, Average : 120.0\n",
      "Episode 10, Reward: -120.87, Average Reward: -179.62\n",
      " Max : 122, Average : 89.2\n",
      "Episode 20, Reward: -114.67, Average Reward: -178.25\n",
      " Max : 119, Average : 91.0\n",
      "Episode 30, Reward: -96.58, Average Reward: -169.98\n",
      " Max : 113, Average : 89.1\n",
      "Episode 40, Reward: -111.59, Average Reward: -160.74\n",
      " Max : 125, Average : 84.8\n",
      "Episode 50, Reward: -216.12, Average Reward: -168.44\n",
      " Max : 119, Average : 92.5\n",
      "Episode 60, Reward: -59.10, Average Reward: -166.02\n",
      " Max : 119, Average : 91.9\n",
      "Episode 70, Reward: -244.84, Average Reward: -171.93\n",
      " Max : 128, Average : 96.3\n",
      "Episode 80, Reward: -219.43, Average Reward: -171.01\n",
      " Max : 128, Average : 104.4\n",
      "Episode 90, Reward: -69.79, Average Reward: -167.02\n",
      " Max : 99, Average : 77.3\n",
      "Episode 100, Reward: -279.90, Average Reward: -164.26\n",
      " Max : 168, Average : 106.1\n",
      "Episode 110, Reward: -132.23, Average Reward: -168.20\n",
      " Max : 146, Average : 100.3\n",
      "Episode 120, Reward: -281.51, Average Reward: -165.81\n",
      " Max : 119, Average : 99.8\n",
      "Episode 130, Reward: -97.32, Average Reward: -162.18\n",
      " Max : 148, Average : 98.3\n",
      "Episode 140, Reward: -130.92, Average Reward: -160.41\n",
      " Max : 137, Average : 109.8\n",
      "Episode 150, Reward: -103.98, Average Reward: -154.08\n",
      " Max : 133, Average : 105.3\n",
      "Episode 160, Reward: -324.58, Average Reward: -152.43\n",
      " Max : 187, Average : 122.9\n",
      "Episode 170, Reward: -152.47, Average Reward: -143.95\n",
      " Max : 167, Average : 117.4\n",
      "Episode 180, Reward: -58.78, Average Reward: -138.80\n",
      " Max : 165, Average : 106.9\n",
      "Episode 190, Reward: -132.95, Average Reward: -137.04\n",
      " Max : 157, Average : 120.8\n",
      "Episode 200, Reward: -70.54, Average Reward: -131.95\n",
      " Max : 1000, Average : 218.8\n",
      "Episode 210, Reward: -215.71, Average Reward: -129.42\n",
      " Max : 178, Average : 147.0\n",
      "Episode 220, Reward: -103.05, Average Reward: -125.41\n",
      " Max : 185, Average : 125.9\n",
      "Episode 230, Reward: -57.83, Average Reward: -120.68\n",
      " Max : 157, Average : 116.1\n",
      "Episode 240, Reward: -100.13, Average Reward: -120.72\n",
      " Max : 223, Average : 138.2\n",
      "Episode 250, Reward: -61.29, Average Reward: -114.14\n",
      " Max : 204, Average : 128.4\n",
      "Episode 260, Reward: -87.23, Average Reward: -105.56\n",
      " Max : 205, Average : 129.9\n",
      "Episode 270, Reward: -68.82, Average Reward: -102.44\n",
      " Max : 225, Average : 146.7\n",
      "Episode 280, Reward: -20.51, Average Reward: -98.00\n",
      " Max : 204, Average : 150.6\n",
      "Episode 290, Reward: -73.94, Average Reward: -98.01\n",
      " Max : 233, Average : 151.8\n",
      "Episode 300, Reward: -52.05, Average Reward: -93.40\n",
      " Max : 1000, Average : 233.3\n",
      "Episode 310, Reward: -30.29, Average Reward: -81.88\n",
      " Max : 273, Average : 162.1\n",
      "Episode 320, Reward: -31.52, Average Reward: -74.88\n",
      " Max : 167, Average : 124.5\n",
      "Episode 330, Reward: -31.91, Average Reward: -74.15\n",
      " Max : 172, Average : 108.4\n",
      "Episode 340, Reward: 1.22, Average Reward: -66.53\n",
      " Max : 173, Average : 109.7\n",
      "Episode 350, Reward: 4.30, Average Reward: -65.23\n",
      " Max : 221, Average : 134.0\n",
      "Episode 360, Reward: -35.80, Average Reward: -66.83\n",
      " Max : 728, Average : 202.3\n",
      "Episode 370, Reward: -29.89, Average Reward: -64.80\n",
      " Max : 166, Average : 126.4\n",
      "Episode 380, Reward: -32.68, Average Reward: -65.18\n",
      " Max : 185, Average : 138.6\n",
      "Episode 390, Reward: -53.39, Average Reward: -59.85\n",
      " Max : 238, Average : 159.5\n",
      "Episode 400, Reward: -9.18, Average Reward: -57.94\n",
      " Max : 244, Average : 155.4\n",
      "Episode 410, Reward: -53.79, Average Reward: -58.90\n",
      " Max : 273, Average : 174.7\n",
      "Episode 420, Reward: -165.13, Average Reward: -63.73\n",
      " Max : 1000, Average : 311.2\n",
      "Episode 430, Reward: -7.03, Average Reward: -66.31\n",
      " Max : 341, Average : 226.4\n",
      "Episode 440, Reward: -47.12, Average Reward: -66.43\n",
      " Max : 1000, Average : 267.1\n",
      "Episode 450, Reward: -81.78, Average Reward: -65.34\n",
      " Max : 190, Average : 146.3\n",
      "Episode 460, Reward: -50.44, Average Reward: -62.47\n",
      " Max : 839, Average : 210.1\n",
      "Episode 470, Reward: 23.26, Average Reward: -56.42\n",
      " Max : 1000, Average : 225.6\n",
      "Episode 480, Reward: -101.62, Average Reward: -54.96\n",
      " Max : 1000, Average : 475.1\n",
      "Episode 490, Reward: -72.95, Average Reward: -54.09\n",
      " Max : 1000, Average : 548.1\n",
      "Episode 500, Reward: -314.86, Average Reward: -69.15\n",
      " Max : 1000, Average : 404.6\n",
      "Episode 510, Reward: 20.13, Average Reward: -70.91\n",
      " Max : 1000, Average : 474.0\n",
      "Episode 520, Reward: -19.40, Average Reward: -61.98\n",
      " Max : 1000, Average : 470.8\n",
      "Episode 530, Reward: -2.54, Average Reward: -50.61\n",
      " Max : 212, Average : 150.8\n",
      "Episode 540, Reward: -29.75, Average Reward: -47.37\n",
      " Max : 211, Average : 145.0\n",
      "Episode 550, Reward: -0.53, Average Reward: -45.92\n",
      " Max : 201, Average : 146.4\n",
      "Episode 560, Reward: 46.41, Average Reward: -40.33\n",
      " Max : 1000, Average : 422.0\n",
      "Episode 570, Reward: 30.87, Average Reward: -42.27\n",
      " Max : 294, Average : 188.9\n",
      "Episode 580, Reward: -147.76, Average Reward: -42.34\n",
      " Max : 1000, Average : 324.3\n",
      "Episode 590, Reward: -149.18, Average Reward: -40.16\n",
      " Max : 1000, Average : 417.7\n",
      "Episode 600, Reward: -39.31, Average Reward: -24.06\n",
      " Max : 1000, Average : 579.2\n",
      "Episode 610, Reward: -45.98, Average Reward: -20.12\n",
      " Max : 311, Average : 190.5\n",
      "Episode 620, Reward: -141.34, Average Reward: -26.24\n",
      " Max : 260, Average : 178.3\n",
      "Episode 630, Reward: 38.60, Average Reward: -32.26\n",
      " Max : 1000, Average : 240.7\n",
      "Episode 640, Reward: -51.97, Average Reward: -35.35\n",
      " Max : 227, Average : 178.6\n",
      "Episode 650, Reward: -4.52, Average Reward: -28.16\n",
      " Max : 1000, Average : 413.5\n",
      "Episode 660, Reward: 36.72, Average Reward: -23.87\n",
      " Max : 1000, Average : 249.7\n",
      "Episode 670, Reward: 126.08, Average Reward: -13.26\n",
      " Max : 1000, Average : 588.8\n",
      "Episode 680, Reward: 42.80, Average Reward: -3.02\n",
      " Max : 1000, Average : 503.8\n",
      "Episode 690, Reward: 103.22, Average Reward: 2.66\n",
      " Max : 1000, Average : 539.9\n",
      "Episode 700, Reward: -36.55, Average Reward: 6.70\n",
      " Max : 1000, Average : 601.7\n",
      "Episode 710, Reward: -22.09, Average Reward: 13.43\n",
      " Max : 1000, Average : 638.4\n",
      "Episode 720, Reward: -17.17, Average Reward: 24.71\n",
      " Max : 1000, Average : 758.0\n",
      "Episode 730, Reward: -5.86, Average Reward: 34.20\n",
      " Max : 1000, Average : 504.1\n",
      "Episode 740, Reward: 49.82, Average Reward: 39.73\n",
      " Max : 206, Average : 181.8\n",
      "Episode 750, Reward: 44.31, Average Reward: 43.97\n",
      " Max : 676, Average : 277.5\n",
      "Episode 760, Reward: 43.25, Average Reward: 45.81\n",
      " Max : 1000, Average : 376.6\n",
      "Episode 770, Reward: 271.42, Average Reward: 55.55\n",
      " Max : 574, Average : 387.0\n",
      "Episode 780, Reward: -5.21, Average Reward: 68.62\n",
      " Max : 498, Average : 348.7\n",
      "Episode 790, Reward: 24.03, Average Reward: 74.02\n",
      " Max : 626, Average : 319.7\n",
      "Episode 800, Reward: 257.04, Average Reward: 91.32\n",
      " Max : 644, Average : 440.5\n",
      "Episode 810, Reward: 181.80, Average Reward: 100.56\n",
      " Max : 542, Average : 411.4\n",
      "Episode 820, Reward: -55.31, Average Reward: 114.22\n",
      " Max : 521, Average : 382.3\n",
      "Episode 830, Reward: 260.69, Average Reward: 120.00\n",
      " Max : 522, Average : 376.3\n",
      "Episode 840, Reward: -24.81, Average Reward: 124.39\n",
      " Max : 474, Average : 272.8\n",
      "Episode 850, Reward: 271.31, Average Reward: 124.42\n",
      " Max : 444, Average : 303.0\n",
      "Episode 860, Reward: 267.22, Average Reward: 134.24\n",
      " Max : 460, Average : 337.6\n",
      "Episode 870, Reward: 231.97, Average Reward: 131.56\n",
      " Max : 369, Average : 280.1\n",
      "Episode 880, Reward: 24.90, Average Reward: 129.16\n",
      " Max : 429, Average : 311.1\n",
      "Episode 890, Reward: 228.40, Average Reward: 135.82\n",
      " Max : 542, Average : 349.3\n",
      "Episode 900, Reward: 264.36, Average Reward: 128.08\n",
      " Max : 506, Average : 317.7\n",
      "Episode 910, Reward: 23.47, Average Reward: 134.96\n",
      " Max : 377, Average : 251.0\n",
      "Episode 920, Reward: 46.78, Average Reward: 125.90\n",
      " Max : 558, Average : 219.0\n",
      "Episode 930, Reward: -9.18, Average Reward: 116.65\n",
      " Max : 244, Average : 183.4\n",
      "Episode 940, Reward: 24.35, Average Reward: 122.07\n",
      " Max : 356, Average : 225.6\n",
      "Episode 950, Reward: 2.08, Average Reward: 120.12\n",
      " Max : 354, Average : 211.9\n",
      "Episode 960, Reward: 43.57, Average Reward: 111.75\n",
      " Max : 350, Average : 229.1\n",
      "Episode 970, Reward: 269.75, Average Reward: 113.61\n",
      " Max : 447, Average : 289.7\n",
      "Episode 980, Reward: 271.89, Average Reward: 115.49\n",
      " Max : 474, Average : 328.8\n",
      "Episode 990, Reward: -236.98, Average Reward: 113.55\n",
      " Max : 526, Average : 359.2\n",
      "Episode 1000, Reward: 241.06, Average Reward: 119.74\n",
      " Max : 480, Average : 352.0\n",
      "Episode 1010, Reward: -24.48, Average Reward: 108.18\n",
      " Max : 398, Average : 250.5\n",
      "Episode 1020, Reward: 27.32, Average Reward: 109.39\n",
      " Max : 333, Average : 237.9\n",
      "Episode 1030, Reward: 269.91, Average Reward: 118.17\n",
      " Max : 458, Average : 244.9\n",
      "Episode 1040, Reward: -12.11, Average Reward: 116.75\n",
      " Max : 310, Average : 229.0\n",
      "Episode 1050, Reward: 24.36, Average Reward: 121.94\n",
      " Max : 422, Average : 260.3\n",
      "Episode 1060, Reward: 251.35, Average Reward: 122.59\n",
      " Max : 383, Average : 252.6\n",
      "Episode 1070, Reward: 32.94, Average Reward: 121.88\n",
      " Max : 345, Average : 260.5\n",
      "Episode 1080, Reward: 27.47, Average Reward: 120.11\n",
      " Max : 385, Average : 284.1\n",
      "Episode 1090, Reward: -26.41, Average Reward: 116.78\n",
      " Max : 356, Average : 254.0\n",
      "Episode 1100, Reward: 20.59, Average Reward: 113.56\n",
      " Max : 344, Average : 266.2\n",
      "Episode 1110, Reward: 213.61, Average Reward: 127.75\n",
      " Max : 517, Average : 357.0\n",
      "Episode 1120, Reward: -30.87, Average Reward: 126.46\n",
      " Max : 599, Average : 342.2\n",
      "Episode 1130, Reward: 284.86, Average Reward: 130.70\n",
      " Max : 349, Average : 277.6\n",
      "Episode 1140, Reward: 254.26, Average Reward: 133.66\n",
      " Max : 649, Average : 300.3\n",
      "Episode 1150, Reward: -27.00, Average Reward: 132.26\n",
      " Max : 481, Average : 311.2\n",
      "Episode 1160, Reward: -71.69, Average Reward: 133.99\n",
      " Max : 674, Average : 479.6\n",
      "Episode 1170, Reward: 148.13, Average Reward: 129.91\n",
      " Max : 762, Average : 587.7\n",
      "Episode 1180, Reward: 138.44, Average Reward: 128.02\n",
      " Max : 676, Average : 434.4\n",
      "Episode 1190, Reward: 262.55, Average Reward: 130.73\n",
      " Max : 586, Average : 392.6\n",
      "Episode 1200, Reward: 9.76, Average Reward: 129.14\n",
      " Max : 514, Average : 269.9\n",
      "Episode 1210, Reward: 258.63, Average Reward: 116.09\n",
      " Max : 315, Average : 232.7\n",
      "Episode 1220, Reward: 34.93, Average Reward: 123.18\n",
      " Max : 439, Average : 279.0\n",
      "Episode 1230, Reward: -14.46, Average Reward: 118.28\n",
      " Max : 307, Average : 214.5\n",
      "Episode 1240, Reward: 50.10, Average Reward: 114.78\n",
      " Max : 322, Average : 196.5\n",
      "Episode 1250, Reward: 15.70, Average Reward: 112.09\n",
      " Max : 324, Average : 189.5\n",
      "Episode 1260, Reward: 255.49, Average Reward: 114.90\n",
      " Max : 265, Average : 191.8\n",
      "Episode 1270, Reward: 236.85, Average Reward: 116.94\n",
      " Max : 315, Average : 217.3\n",
      "Episode 1280, Reward: 251.58, Average Reward: 121.95\n",
      " Max : 390, Average : 252.8\n",
      "Episode 1290, Reward: -14.06, Average Reward: 127.89\n",
      " Max : 404, Average : 240.3\n",
      "Episode 1300, Reward: 233.11, Average Reward: 134.49\n",
      " Max : 437, Average : 257.7\n",
      "Episode 1310, Reward: 70.34, Average Reward: 134.63\n",
      " Max : 232, Average : 164.4\n",
      "Episode 1320, Reward: 49.41, Average Reward: 134.90\n",
      " Max : 445, Average : 270.4\n",
      "Episode 1330, Reward: 232.31, Average Reward: 140.60\n",
      " Max : 448, Average : 264.2\n",
      "Episode 1340, Reward: 217.45, Average Reward: 141.27\n",
      " Max : 302, Average : 205.4\n",
      "Episode 1350, Reward: 10.06, Average Reward: 150.60\n",
      " Max : 299, Average : 212.6\n",
      "Episode 1360, Reward: 263.33, Average Reward: 155.39\n",
      " Max : 416, Average : 272.7\n",
      "Episode 1370, Reward: 256.60, Average Reward: 157.70\n",
      " Max : 417, Average : 330.4\n",
      "Episode 1380, Reward: -60.58, Average Reward: 154.52\n",
      " Max : 459, Average : 331.0\n",
      "Episode 1390, Reward: -173.02, Average Reward: 152.28\n",
      " Max : 506, Average : 354.3\n",
      "Episode 1400, Reward: 204.94, Average Reward: 144.49\n",
      " Max : 493, Average : 343.5\n",
      "Episode 1410, Reward: 154.49, Average Reward: 149.95\n",
      " Max : 695, Average : 408.7\n",
      "Episode 1420, Reward: 227.70, Average Reward: 155.32\n",
      " Max : 475, Average : 343.8\n",
      "Episode 1430, Reward: 16.30, Average Reward: 142.20\n",
      " Max : 348, Average : 167.1\n",
      "Episode 1440, Reward: 16.52, Average Reward: 132.65\n",
      " Max : 232, Average : 144.0\n",
      "Episode 1450, Reward: 264.16, Average Reward: 120.07\n",
      " Max : 366, Average : 173.0\n",
      "Episode 1460, Reward: 33.36, Average Reward: 108.68\n",
      " Max : 274, Average : 183.1\n",
      "Episode 1470, Reward: 5.61, Average Reward: 101.64\n",
      " Max : 243, Average : 191.3\n",
      "Episode 1480, Reward: 246.27, Average Reward: 103.19\n",
      " Max : 328, Average : 258.8\n",
      "Episode 1490, Reward: 224.65, Average Reward: 97.98\n",
      " Max : 486, Average : 306.7\n",
      "Episode 1500, Reward: -60.88, Average Reward: 99.09\n",
      " Max : 467, Average : 370.3\n",
      "Episode 1510, Reward: -11.53, Average Reward: 93.15\n",
      " Max : 492, Average : 270.4\n",
      "Episode 1520, Reward: 299.69, Average Reward: 83.64\n",
      " Max : 281, Average : 230.5\n",
      "Episode 1530, Reward: -7.02, Average Reward: 97.81\n",
      " Max : 307, Average : 243.2\n",
      "Episode 1540, Reward: 2.12, Average Reward: 113.19\n",
      " Max : 318, Average : 236.8\n",
      "Episode 1550, Reward: 46.04, Average Reward: 117.97\n",
      " Max : 374, Average : 220.1\n",
      "Episode 1560, Reward: 5.11, Average Reward: 124.28\n",
      " Max : 333, Average : 242.5\n",
      "Episode 1570, Reward: -40.45, Average Reward: 119.62\n",
      " Max : 270, Average : 185.4\n",
      "Episode 1580, Reward: 264.21, Average Reward: 114.56\n",
      " Max : 402, Average : 233.1\n",
      "Episode 1590, Reward: -86.38, Average Reward: 123.39\n",
      " Max : 442, Average : 353.5\n",
      "Episode 1600, Reward: 251.49, Average Reward: 136.86\n",
      " Max : 741, Average : 352.5\n",
      "Episode 1610, Reward: -39.07, Average Reward: 139.91\n",
      " Max : 609, Average : 385.4\n",
      "Episode 1620, Reward: -64.86, Average Reward: 144.48\n",
      " Max : 682, Average : 402.2\n",
      "Episode 1630, Reward: 32.37, Average Reward: 141.30\n",
      " Max : 432, Average : 261.4\n",
      "Episode 1640, Reward: -69.52, Average Reward: 129.44\n",
      " Max : 497, Average : 262.9\n",
      "Episode 1650, Reward: 272.47, Average Reward: 128.37\n",
      " Max : 410, Average : 223.1\n",
      "Episode 1660, Reward: -48.26, Average Reward: 118.10\n",
      " Max : 435, Average : 282.3\n",
      "Episode 1670, Reward: -150.80, Average Reward: 115.41\n",
      " Max : 505, Average : 349.5\n",
      "Episode 1680, Reward: 256.01, Average Reward: 122.74\n",
      " Max : 467, Average : 366.3\n",
      "Episode 1690, Reward: 264.51, Average Reward: 114.81\n",
      " Max : 308, Average : 224.3\n",
      "Episode 1700, Reward: 26.55, Average Reward: 105.84\n",
      " Max : 292, Average : 214.4\n",
      "Episode 1710, Reward: 43.95, Average Reward: 109.68\n",
      " Max : 290, Average : 178.4\n",
      "Episode 1720, Reward: 224.86, Average Reward: 107.45\n",
      " Max : 336, Average : 219.7\n",
      "Episode 1730, Reward: 15.44, Average Reward: 103.52\n",
      " Max : 281, Average : 188.6\n",
      "Episode 1740, Reward: 19.50, Average Reward: 101.43\n",
      " Max : 205, Average : 155.2\n",
      "Episode 1750, Reward: 27.26, Average Reward: 108.07\n",
      " Max : 320, Average : 262.5\n",
      "Episode 1760, Reward: 229.62, Average Reward: 120.74\n",
      " Max : 331, Average : 292.2\n",
      "Episode 1770, Reward: 257.76, Average Reward: 137.36\n",
      " Max : 396, Average : 310.2\n",
      "Episode 1780, Reward: 18.46, Average Reward: 136.88\n",
      " Max : 483, Average : 318.2\n",
      "Episode 1790, Reward: 208.38, Average Reward: 141.63\n",
      " Max : 507, Average : 394.3\n",
      "Episode 1800, Reward: 239.68, Average Reward: 133.58\n",
      " Max : 780, Average : 488.6\n",
      "Episode 1810, Reward: -38.86, Average Reward: 125.45\n",
      " Max : 549, Average : 333.8\n",
      "Episode 1820, Reward: 232.36, Average Reward: 123.11\n",
      " Max : 507, Average : 383.3\n",
      "Episode 1830, Reward: 44.85, Average Reward: 120.15\n",
      " Max : 461, Average : 308.9\n",
      "Episode 1840, Reward: 242.73, Average Reward: 137.01\n",
      " Max : 414, Average : 339.2\n",
      "Episode 1850, Reward: -166.87, Average Reward: 130.24\n",
      " Max : 639, Average : 397.5\n",
      "Episode 1860, Reward: 237.38, Average Reward: 127.48\n",
      " Max : 544, Average : 386.4\n",
      "Episode 1870, Reward: 15.61, Average Reward: 125.37\n",
      " Max : 545, Average : 413.0\n",
      "Episode 1880, Reward: -257.93, Average Reward: 115.67\n",
      " Max : 752, Average : 400.2\n",
      "Episode 1890, Reward: 284.24, Average Reward: 124.13\n",
      " Max : 437, Average : 346.2\n",
      "Episode 1900, Reward: 271.35, Average Reward: 140.95\n",
      " Max : 473, Average : 305.2\n",
      "Episode 1910, Reward: -20.11, Average Reward: 143.43\n",
      " Max : 707, Average : 287.2\n",
      "Episode 1920, Reward: -6.76, Average Reward: 152.18\n",
      " Max : 716, Average : 325.7\n",
      "Episode 1930, Reward: -23.55, Average Reward: 158.48\n",
      " Max : 306, Average : 231.4\n",
      "Episode 1940, Reward: 196.16, Average Reward: 159.09\n",
      " Max : 386, Average : 273.5\n",
      "Episode 1950, Reward: 267.03, Average Reward: 168.97\n",
      " Max : 377, Average : 271.2\n",
      "Episode 1960, Reward: 170.68, Average Reward: 171.01\n",
      " Max : 482, Average : 380.2\n",
      "Episode 1970, Reward: 255.83, Average Reward: 168.45\n",
      " Max : 611, Average : 421.8\n",
      "Episode 1980, Reward: -38.97, Average Reward: 165.44\n",
      " Max : 507, Average : 329.1\n",
      "Episode 1990, Reward: -26.01, Average Reward: 152.41\n",
      " Max : 679, Average : 379.7\n",
      "Episode 2000, Reward: 306.36, Average Reward: 140.75\n",
      " Max : 475, Average : 332.4\n",
      "Episode 2010, Reward: 266.99, Average Reward: 147.21\n",
      " Max : 360, Average : 252.7\n",
      "Episode 2020, Reward: 0.03, Average Reward: 141.96\n",
      " Max : 416, Average : 283.9\n",
      "Episode 2030, Reward: 3.87, Average Reward: 142.11\n",
      " Max : 413, Average : 272.9\n",
      "Episode 2040, Reward: 35.72, Average Reward: 138.27\n",
      " Max : 349, Average : 240.2\n",
      "Episode 2050, Reward: 252.42, Average Reward: 131.70\n",
      " Max : 448, Average : 223.7\n",
      "Episode 2060, Reward: 40.13, Average Reward: 127.58\n",
      " Max : 302, Average : 215.6\n",
      "Episode 2070, Reward: 216.60, Average Reward: 124.21\n",
      " Max : 278, Average : 196.7\n",
      "Episode 2080, Reward: -8.83, Average Reward: 127.73\n",
      " Max : 349, Average : 207.1\n",
      "Episode 2090, Reward: 188.99, Average Reward: 130.23\n",
      " Max : 468, Average : 254.8\n",
      "Episode 2100, Reward: 14.78, Average Reward: 126.69\n",
      " Max : 351, Average : 230.0\n",
      "Episode 2110, Reward: 252.72, Average Reward: 133.50\n",
      " Max : 310, Average : 265.9\n",
      "Episode 2120, Reward: -18.05, Average Reward: 137.25\n",
      " Max : 375, Average : 285.8\n",
      "Episode 2130, Reward: 10.75, Average Reward: 135.75\n",
      " Max : 831, Average : 345.9\n",
      "Episode 2140, Reward: -3.74, Average Reward: 139.92\n",
      " Max : 498, Average : 337.6\n",
      "Episode 2150, Reward: 179.42, Average Reward: 140.58\n",
      " Max : 615, Average : 354.5\n",
      "Episode 2160, Reward: 209.46, Average Reward: 141.85\n",
      " Max : 949, Average : 507.1\n",
      "Episode 2170, Reward: 79.21, Average Reward: 140.38\n",
      " Max : 1000, Average : 749.3\n",
      "Episode 2180, Reward: 258.27, Average Reward: 137.05\n",
      " Max : 1000, Average : 557.1\n",
      "Episode 2190, Reward: 197.84, Average Reward: 132.51\n",
      " Max : 1000, Average : 523.2\n",
      "Episode 2200, Reward: 188.85, Average Reward: 131.01\n",
      " Max : 927, Average : 556.4\n",
      "Episode 2210, Reward: -151.69, Average Reward: 115.28\n",
      " Max : 1000, Average : 564.3\n",
      "Episode 2220, Reward: 51.66, Average Reward: 107.64\n",
      " Max : 634, Average : 354.9\n",
      "Episode 2230, Reward: -48.88, Average Reward: 109.24\n",
      " Max : 400, Average : 306.9\n",
      "Episode 2240, Reward: 221.02, Average Reward: 106.96\n",
      " Max : 451, Average : 292.6\n",
      "Episode 2250, Reward: 224.79, Average Reward: 102.70\n",
      " Max : 373, Average : 261.6\n",
      "Episode 2260, Reward: 227.06, Average Reward: 107.75\n",
      " Max : 346, Average : 269.1\n",
      "Episode 2270, Reward: -30.28, Average Reward: 116.16\n",
      " Max : 568, Average : 291.1\n",
      "Episode 2280, Reward: 234.92, Average Reward: 118.99\n",
      " Max : 301, Average : 220.2\n",
      "Episode 2290, Reward: 15.11, Average Reward: 116.55\n",
      " Max : 298, Average : 205.3\n",
      "Episode 2300, Reward: 4.36, Average Reward: 121.02\n",
      " Max : 513, Average : 276.0\n",
      "Episode 2310, Reward: 224.77, Average Reward: 127.52\n",
      " Max : 468, Average : 304.3\n",
      "Episode 2320, Reward: 257.80, Average Reward: 130.53\n",
      " Max : 624, Average : 335.6\n",
      "Episode 2330, Reward: -51.22, Average Reward: 122.28\n",
      " Max : 533, Average : 321.9\n",
      "Episode 2340, Reward: 20.03, Average Reward: 116.54\n",
      " Max : 695, Average : 437.9\n",
      "Episode 2350, Reward: -111.45, Average Reward: 126.91\n",
      " Max : 626, Average : 413.4\n",
      "Episode 2360, Reward: 244.26, Average Reward: 125.65\n",
      " Max : 474, Average : 367.3\n",
      "Episode 2370, Reward: 219.06, Average Reward: 128.70\n",
      " Max : 435, Average : 331.1\n",
      "Episode 2380, Reward: 242.85, Average Reward: 137.34\n",
      " Max : 462, Average : 402.7\n",
      "Episode 2390, Reward: 156.58, Average Reward: 148.61\n",
      " Max : 701, Average : 518.7\n",
      "Episode 2400, Reward: 181.92, Average Reward: 146.78\n",
      " Max : 1000, Average : 632.4\n",
      "Episode 2410, Reward: 206.55, Average Reward: 151.09\n",
      " Max : 610, Average : 463.9\n",
      "Episode 2420, Reward: 215.66, Average Reward: 143.58\n",
      " Max : 939, Average : 562.3\n",
      "Episode 2430, Reward: 234.37, Average Reward: 154.85\n",
      " Max : 593, Average : 396.6\n",
      "Episode 2440, Reward: -47.03, Average Reward: 155.59\n",
      " Max : 474, Average : 327.8\n",
      "Episode 2450, Reward: 231.89, Average Reward: 153.37\n",
      " Max : 480, Average : 321.7\n",
      "Episode 2460, Reward: 235.45, Average Reward: 150.46\n",
      " Max : 591, Average : 339.2\n",
      "Episode 2470, Reward: 268.72, Average Reward: 146.52\n",
      " Max : 461, Average : 313.8\n",
      "Episode 2480, Reward: -47.58, Average Reward: 131.40\n",
      " Max : 306, Average : 246.1\n",
      "Episode 2490, Reward: 197.31, Average Reward: 124.03\n",
      " Max : 639, Average : 317.0\n",
      "Episode 2500, Reward: 243.79, Average Reward: 125.91\n",
      " Max : 561, Average : 295.3\n",
      "Episode 2510, Reward: 189.72, Average Reward: 124.86\n",
      " Max : 449, Average : 300.4\n",
      "Episode 2520, Reward: -5.75, Average Reward: 126.11\n",
      " Max : 476, Average : 320.2\n",
      "Episode 2530, Reward: -51.79, Average Reward: 119.20\n",
      " Max : 502, Average : 312.1\n",
      "Episode 2540, Reward: -74.35, Average Reward: 121.40\n",
      " Max : 437, Average : 300.6\n",
      "Episode 2550, Reward: 229.50, Average Reward: 121.03\n",
      " Max : 503, Average : 345.2\n",
      "Episode 2560, Reward: -19.93, Average Reward: 124.86\n",
      " Max : 878, Average : 374.7\n",
      "Episode 2570, Reward: 269.71, Average Reward: 130.22\n",
      " Max : 467, Average : 344.9\n",
      "Episode 2580, Reward: 249.99, Average Reward: 145.56\n",
      " Max : 456, Average : 288.2\n",
      "Episode 2590, Reward: 233.80, Average Reward: 151.26\n",
      " Max : 441, Average : 289.4\n",
      "Episode 2600, Reward: -10.83, Average Reward: 150.56\n",
      " Max : 368, Average : 239.2\n",
      "Episode 2610, Reward: 5.12, Average Reward: 140.10\n",
      " Max : 257, Average : 198.7\n",
      "Episode 2620, Reward: -52.77, Average Reward: 147.08\n",
      " Max : 379, Average : 272.2\n",
      "Episode 2630, Reward: 222.34, Average Reward: 149.22\n",
      " Max : 435, Average : 281.5\n",
      "Episode 2640, Reward: -63.78, Average Reward: 152.27\n",
      " Max : 847, Average : 355.6\n",
      "Episode 2650, Reward: 2.57, Average Reward: 147.65\n",
      " Max : 566, Average : 304.9\n",
      "Episode 2660, Reward: 30.14, Average Reward: 143.66\n",
      " Max : 459, Average : 283.7\n",
      "Episode 2670, Reward: 28.30, Average Reward: 136.22\n",
      " Max : 404, Average : 258.8\n",
      "Episode 2680, Reward: 306.48, Average Reward: 130.94\n",
      " Max : 638, Average : 319.8\n",
      "Episode 2690, Reward: 185.37, Average Reward: 124.61\n",
      " Max : 426, Average : 309.9\n",
      "Episode 2700, Reward: -2.16, Average Reward: 118.95\n",
      " Max : 407, Average : 259.8\n",
      "Episode 2710, Reward: 259.47, Average Reward: 125.62\n",
      " Max : 452, Average : 280.3\n",
      "Episode 2720, Reward: 242.82, Average Reward: 123.55\n",
      " Max : 569, Average : 295.8\n",
      "Episode 2730, Reward: 258.22, Average Reward: 130.10\n",
      " Max : 438, Average : 294.9\n",
      "Episode 2740, Reward: 287.33, Average Reward: 128.66\n",
      " Max : 365, Average : 284.5\n",
      "Episode 2750, Reward: 229.00, Average Reward: 136.81\n",
      " Max : 476, Average : 304.8\n",
      "Episode 2760, Reward: 264.34, Average Reward: 139.84\n",
      " Max : 495, Average : 262.0\n",
      "Episode 2770, Reward: -150.32, Average Reward: 133.26\n",
      " Max : 408, Average : 228.8\n",
      "Episode 2780, Reward: 9.01, Average Reward: 137.21\n",
      " Max : 391, Average : 264.4\n",
      "Episode 2790, Reward: -32.39, Average Reward: 139.11\n",
      " Max : 466, Average : 240.2\n",
      "Episode 2800, Reward: 28.43, Average Reward: 146.91\n",
      " Max : 490, Average : 252.0\n",
      "Episode 2810, Reward: -4.57, Average Reward: 146.52\n",
      " Max : 288, Average : 212.2\n",
      "Episode 2820, Reward: 230.76, Average Reward: 144.29\n",
      " Max : 452, Average : 252.5\n",
      "Episode 2830, Reward: 244.15, Average Reward: 142.87\n",
      " Max : 343, Average : 262.6\n",
      "Episode 2840, Reward: 0.07, Average Reward: 141.84\n",
      " Max : 680, Average : 326.3\n",
      "Episode 2850, Reward: 10.94, Average Reward: 135.00\n",
      " Max : 334, Average : 244.2\n",
      "Episode 2860, Reward: 253.19, Average Reward: 136.72\n",
      " Max : 501, Average : 295.1\n",
      "Episode 2870, Reward: 243.33, Average Reward: 140.36\n",
      " Max : 400, Average : 260.7\n",
      "Episode 2880, Reward: 231.90, Average Reward: 143.80\n",
      " Max : 444, Average : 265.9\n",
      "Episode 2890, Reward: -47.87, Average Reward: 146.77\n",
      " Max : 465, Average : 274.4\n",
      "Episode 2900, Reward: 180.90, Average Reward: 146.75\n",
      " Max : 382, Average : 254.5\n",
      "Episode 2910, Reward: -27.26, Average Reward: 144.90\n",
      " Max : 481, Average : 245.0\n",
      "Episode 2920, Reward: -27.90, Average Reward: 145.81\n",
      " Max : 360, Average : 265.0\n",
      "Episode 2930, Reward: 318.46, Average Reward: 145.60\n",
      " Max : 399, Average : 280.4\n",
      "Episode 2940, Reward: 34.18, Average Reward: 143.04\n",
      " Max : 433, Average : 227.1\n",
      "Episode 2950, Reward: -70.80, Average Reward: 140.06\n",
      " Max : 378, Average : 239.6\n",
      "Episode 2960, Reward: -13.60, Average Reward: 134.00\n",
      " Max : 290, Average : 198.2\n",
      "Episode 2970, Reward: 59.17, Average Reward: 128.46\n",
      " Max : 370, Average : 167.2\n",
      "Episode 2980, Reward: -33.91, Average Reward: 119.61\n",
      " Max : 297, Average : 193.0\n",
      "Episode 2990, Reward: 268.45, Average Reward: 113.02\n",
      " Max : 478, Average : 262.2\n",
      "Episode 3000, Reward: 264.38, Average Reward: 119.98\n",
      " Max : 400, Average : 293.7\n",
      "Episode 3010, Reward: 233.15, Average Reward: 127.88\n",
      " Max : 450, Average : 342.9\n",
      "Episode 3020, Reward: 220.43, Average Reward: 132.84\n",
      " Max : 480, Average : 370.8\n",
      "Episode 3030, Reward: 13.62, Average Reward: 131.79\n",
      " Max : 401, Average : 307.8\n",
      "Episode 3040, Reward: 209.15, Average Reward: 143.29\n",
      " Max : 394, Average : 309.8\n",
      "Episode 3050, Reward: 10.35, Average Reward: 148.58\n",
      " Max : 548, Average : 315.8\n",
      "Episode 3060, Reward: 251.56, Average Reward: 160.57\n",
      " Max : 578, Average : 355.5\n",
      "Episode 3070, Reward: 224.05, Average Reward: 174.46\n",
      " Max : 446, Average : 375.3\n",
      "Episode 3080, Reward: 155.12, Average Reward: 174.46\n",
      " Max : 501, Average : 356.5\n",
      "Episode 3090, Reward: 241.61, Average Reward: 183.19\n",
      " Max : 555, Average : 410.9\n",
      "Episode 3100, Reward: 291.21, Average Reward: 181.59\n",
      " Max : 721, Average : 407.6\n",
      "Episode 3110, Reward: 260.82, Average Reward: 184.18\n",
      " Max : 469, Average : 314.0\n",
      "Episode 3120, Reward: 25.33, Average Reward: 186.99\n",
      " Max : 392, Average : 304.5\n",
      "Episode 3130, Reward: 269.94, Average Reward: 192.18\n",
      " Max : 514, Average : 302.2\n",
      "Episode 3140, Reward: 29.52, Average Reward: 180.92\n",
      " Max : 308, Average : 230.7\n",
      "Episode 3150, Reward: 277.52, Average Reward: 182.74\n",
      " Max : 363, Average : 244.1\n",
      "Episode 3160, Reward: 276.60, Average Reward: 172.46\n",
      " Max : 505, Average : 270.4\n",
      "Episode 3170, Reward: 34.13, Average Reward: 170.74\n",
      " Max : 427, Average : 225.6\n",
      "Episode 3180, Reward: 270.86, Average Reward: 174.99\n",
      " Max : 306, Average : 227.6\n",
      "Episode 3190, Reward: 243.81, Average Reward: 175.98\n",
      " Max : 332, Average : 245.1\n",
      "Episode 3200, Reward: 266.82, Average Reward: 179.21\n",
      " Max : 323, Average : 249.5\n",
      "Episode 3210, Reward: 263.38, Average Reward: 181.38\n",
      " Max : 259, Average : 233.1\n",
      "Episode 3220, Reward: 200.11, Average Reward: 180.22\n",
      " Max : 448, Average : 290.5\n",
      "Episode 3230, Reward: -57.00, Average Reward: 170.28\n",
      " Max : 450, Average : 266.3\n",
      "Episode 3240, Reward: -58.90, Average Reward: 160.12\n",
      " Max : 374, Average : 255.3\n",
      "Episode 3250, Reward: 235.73, Average Reward: 157.79\n",
      " Max : 543, Average : 348.6\n",
      "Episode 3260, Reward: 265.84, Average Reward: 152.50\n",
      " Max : 443, Average : 327.9\n",
      "Episode 3270, Reward: 272.54, Average Reward: 149.75\n",
      " Max : 555, Average : 309.9\n",
      "Episode 3280, Reward: -25.86, Average Reward: 145.21\n",
      " Max : 568, Average : 284.9\n",
      "Episode 3290, Reward: 271.77, Average Reward: 142.55\n",
      " Max : 440, Average : 251.2\n",
      "Episode 3300, Reward: 38.06, Average Reward: 119.95\n",
      " Max : 428, Average : 200.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m probs \u001b[38;5;241m=\u001b[39m policy(state_tensor)\n\u001b[0;32m     52\u001b[0m m \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[1;32m---> 53\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[0;32m     55\u001b[0m log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4),\n",
    "            nn.Softmax(dim=-1)            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Initialize environment and network\n",
    "env = gym.make('LunarLander-v3')  # Corrected to standard Gymnasium environment\n",
    "# env = TimeLimit(env, max_episode_steps=200)\n",
    "policy = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "num_episodes = 10000  # Total episodes to train\n",
    "\n",
    "episode_rewards = []  # Track rewards for monitoring\n",
    "\n",
    "\n",
    "total_step = []\n",
    "# Training loop\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # Properly unpack the tuple returned by reset()\n",
    "    log_probs = []  # Store log probabilities of actions\n",
    "    rewards = []    # Store rewards\n",
    "    done = False\n",
    "\n",
    "    # Generate an episode\n",
    "    stepcount=0\n",
    "    while not done:\n",
    "        stepcount += 1\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        probs = policy(state_tensor)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Properly handle the five values returned by env.step()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated  # Episode ends if either terminated or truncated\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "    total_step.append(stepcount)\n",
    "\n",
    "    # Compute discounted returns\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # Compute baseline and advantages\n",
    "    baseline = returns.mean()\n",
    "    advantages = returns - baseline\n",
    "    # Normalize advantages for stability\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # Compute policy loss\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    loss = -(log_probs * advantages).sum()\n",
    "\n",
    "    # Update policy\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record and print progress\n",
    "    total_reward = sum(rewards)\n",
    "    episode_rewards.append(total_reward)\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        print(f\"Episode {episode}, Reward: {total_reward:.2f}, Average Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "        print(f\" Max : {max(total_step)}, Average : {np.mean(total_step)}\")\n",
    "        total_step=[]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
