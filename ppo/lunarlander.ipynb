{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = torch.softmax(, dim=0)\n",
    "# print(res)\n",
    "res = torch.tensor([1.56,1.34,1.76,1.11])\n",
    "torch.distributions.Categorical(res).sample().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In descrete space -> the actions will be either 0,1,2,3\n",
    "#In continous space -> the actions will be for main and side engines only with [1,1] jaha 1,1 ki values flunctuate hogi\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "               enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            # nn.Softmax(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model):\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "    action_proba = torch.softmax(model(obs_tensor), dim=-1)\n",
    "    \n",
    "    action_dist = torch.distributions.Categorical(action_proba)\n",
    "    action = action_dist.sample()\n",
    "    \n",
    "    obs, reward, done, truncated, info = env.step(action.item())\n",
    "    \n",
    "    loss = -action_dist.log_prob(action) * reward\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    grads = [param.grad.clone() for param in model.parameters()]\n",
    "    \n",
    "    return obs, reward, done, truncated, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_max_episodes, n_max_steps, model):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_max_episodes):\n",
    "        curr_rews = []\n",
    "        curr_grds = []\n",
    "        observation, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(env, observation, model)\n",
    "            curr_rews.append(reward)\n",
    "            curr_grds.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        all_rewards.append(curr_rews) \n",
    "        all_grads.append(curr_grds) \n",
    "    return all_rewards, all_grads\n",
    "\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for i in range(len(rewards)-2, -1, -1):\n",
    "        discounted[i] += discounted[i+1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(single_rew, discount_factor) for single_rew in all_rewards]\n",
    "    flattened_rewards = np.concatenate(all_discounted_rewards)\n",
    "    mean = flattened_rewards.mean()\n",
    "    std = flattened_rewards.std()\n",
    "    return [(rew - mean)/std for rew in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes = 10\n",
    "n_steps_per_ep = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for iter in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes, n_steps_per_ep, model)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    \n",
    "    all_mean_grads = []\n",
    "    \n",
    "    for var_index, param in enumerate(model.parameters()):\n",
    "        mean_grad = torch.mean(\n",
    "            torch.stack([\n",
    "                final_rew * all_grads[episode_index][step][var_index]\n",
    "                for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                for step, final_rew in enumerate(final_rewards)\n",
    "            ])\n",
    "            ,dim=0\n",
    "        )\n",
    "        all_mean_grads.append(mean_grad)\n",
    "    \n",
    "    for param, mean_grads in zip(model.parameters(), all_mean_grads):\n",
    "        param.grad = mean_grads\n",
    "    optimizer.step()  # Update model parameters\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, model, n_episodes=5, render=True):\n",
    "    \"\"\"Test the trained agent in the environment.\"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():  # Disable gradient computation\n",
    "                action_proba = torch.softmax(model(obs_tensor), dim=-1)\n",
    "                action = torch.argmax(action_proba).item()  # Select highest probability action\n",
    "            \n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if render:\n",
    "                env.render()  # Render the environment\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    env.close()  # Close rendering window if open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -446.0374871103684\n",
      "Episode 2: Total Reward = -495.74198552372303\n",
      "Episode 3: Total Reward = -547.561237786754\n",
      "Episode 4: Total Reward = -1186.8334686374408\n",
      "Episode 5: Total Reward = -411.84532148829226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHIVAM\\Desktop\\ReinforcementLearning\\rlenv\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:672: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "test_agent(env, model, n_episodes=5, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m): \u001b[38;5;66;03m#Each episode runs for 200 steps\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnn_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     obs, curr_reward, done, truncated, info  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m=\u001b[39maction)\n\u001b[0;32m     19\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(curr_reward)\n",
      "Cell \u001b[1;32mIn[146], line 6\u001b[0m, in \u001b[0;36mnn_policy\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m      4\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m prob \u001b[38;5;241m=\u001b[39m model(obs_tensor)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[43mprob\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# Let's test the model\n",
    "\n",
    "def nn_policy(obs):\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "    prob = model(obs_tensor)\n",
    "    if(prob>0.5):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "total = []\n",
    "model.eval()\n",
    "for episode in range(500): #running for 500 episodes\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    episode_reward = 0\n",
    "    for step in range(200): #Each episode runs for 200 steps\n",
    "        action = nn_policy(obs)\n",
    "        obs, curr_reward, done, truncated, info  = env.step(action=action)\n",
    "        episode_reward += int(curr_reward)\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    total.append(episode_reward)\n",
    "    \n",
    "print(\"Mean ->\",np.mean(total))\n",
    "print(\"Max ->\",max(total))\n",
    "print(\"Min ->\",min(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4,5), # 4 Input and 5 output features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,1),  \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x) \n",
    "    \n",
    "model = Simplemodel()\n",
    "\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0) # eg -> [[1,2,3,4]]\n",
    "    left_prob = model(obs_tensor)\n",
    "    action = (torch.rand(1) > left_prob).float() \n",
    "    y_target = torch.ones_like(left_prob) - action\n",
    "    loss = loss_fn(left_prob, y_target)\n",
    "    model.zero_grad() \n",
    "    loss.backward() # Backpropagation\n",
    "    action_int = int(action.item())\n",
    "    obs, reward, done, truncated, info= env.step(action_int)\n",
    "    grads = [param.grad.clone() for param in model.parameters()]\n",
    "    return obs, reward, done, truncated, grads\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = [] # [[...],[...],[...],[...],[...],[...]]\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        curr_rewards = []\n",
    "        curr_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, rewards, done, truncated, grads = play_one_step(env,obs,model,loss_fn)\n",
    "            curr_grads.append(grads)     \n",
    "            if done or truncated:\n",
    "                break\n",
    "        all_rewards.append(curr_rewards)\n",
    "        all_grads.append(curr_grads)\n",
    "    return all_rewards, all_grads \n",
    "\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)              \n",
    "    for step in range(len(rewards)-2, -1, -1):\n",
    "        discounted[step] += discounted[step+1] * discount_factor\n",
    "    return discounted                           #[1,2,3...200]\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(reward, discount_factor) for reward in all_rewards] #[[...],[...],[...],[...],[...],[...]]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    mean = flat_rewards.mean()\n",
    "    std = flat_rewards.std()\n",
    "    return [(discounted_rewards-mean)/std for discounted_rewards in all_discounted_rewards]  #[[...],[...],[...],[...],[...],[...]]\n",
    "    \n",
    "\n",
    "#Model configurations\n",
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCELoss()  # Binary cross-entropy loss\n",
    "# loss_fn = nn.MSELoss()  # Binary cross-entropy loss\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor) # [[...],[...],[...],[...],[...],[...]]\n",
    "    all_mean_grads = [] \n",
    "    for var_index, param in enumerate(model.parameters()):\n",
    "        mean_grads = torch.mean(\n",
    "            torch.stack([\n",
    "                final_reward * all_grads[episode_index][step][var_index]\n",
    "                for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                for step, final_reward in enumerate(final_rewards)\n",
    "            ]),\n",
    "            dim=0\n",
    "        )\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    # Apply gradients manually\n",
    "    for param, mean_grad in zip(model.parameters(), all_mean_grads):\n",
    "        param.grad = mean_grad  # Set gradients manually\n",
    "    optimizer.step()  # Update model parameters\n",
    "    optimizer.zero_grad()  # Clear accumulated gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
