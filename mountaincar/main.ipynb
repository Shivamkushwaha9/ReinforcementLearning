{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b197ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# ======================== Actor and Critic ===========================\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, action_dim), nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat([state, action], dim=1))\n",
    "\n",
    "# ========================== Replay Buffer ============================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        return map(np.array, zip(*transitions))\n",
    "\n",
    "# ========================== DDPG Agent ===============================\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        self.max_action = max_action\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "    def select_action(self, state, noise=0.1):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = self.actor(state).detach().numpy()[0]\n",
    "        return np.clip(action + noise * np.random.randn(*action.shape), -self.max_action, self.max_action)\n",
    "\n",
    "    def train(self, batch_size=64, gamma=0.99, tau=0.005):\n",
    "        if len(self.buffer.buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor_target(next_states)\n",
    "            target_q = self.critic_target(next_states, next_actions)\n",
    "            target_q = rewards + (1 - dones) * gamma * target_q\n",
    "\n",
    "        # Critic update\n",
    "        current_q = self.critic(states, actions)\n",
    "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor update\n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft update\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "# ============================== Main =================================\n",
    "env = gym.make(\"Pendulum-v1\")  # Change to any continuous env\n",
    "agent = DDPGAgent(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n",
    "\n",
    "episodes = 0 #200\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        agent.buffer.push((state, action, reward, next_state, float(done)))\n",
    "        agent.train()\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "    print(f\"Episode {ep}, Reward: {ep_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57b152cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32281303]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Pendulum-v1\")  # Change to any continuous env\n",
    "agent = DDPGAgent(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n",
    "state, _ = env.reset()\n",
    "action = agent.select_action(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a43983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from p constructor\n",
      "Another Hello from p constructor\n",
      "Nonchaltant shit\n",
      "Hello from parent greet\n"
     ]
    }
   ],
   "source": [
    "class parent:\n",
    "    def __init__(self):\n",
    "        print(\"Hello from p constructor\")\n",
    "        print(\"Another Hello from p constructor\")\n",
    "        \n",
    "    def greet(self):\n",
    "        print(\"Hello from parent greet\")\n",
    "        \n",
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Nonchaltant shit\")\n",
    "    # super().__init__()\n",
    "    def greet(self):\n",
    "        super().greet()\n",
    "        # print(\"Hii\")\n",
    "        \n",
    "obj = child()\n",
    "obj.greet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992d7077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello before func\n",
      "4\n",
      "Hello after func\n"
     ]
    }
   ],
   "source": [
    "def decor(func):\n",
    "    def wrapper(arg):\n",
    "        print(\"Hello before func\")\n",
    "        ret = func(arg)\n",
    "        print(\"Hello after func\")\n",
    "        return ret\n",
    "    return wrapper\n",
    "\n",
    "@decor\n",
    "def fun(arg):\n",
    "    print(arg*arg)\n",
    "fun(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994de520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'my_expensive_function' took 2.0011 seconds to run.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function '{func.__name__}' took {end_time - start_time:.4f} seconds to run.\")\n",
    "        return None\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def my_expensive_function():\n",
    "    time.sleep(2)\n",
    "    return \"Done!\"\n",
    "\n",
    "output = my_expensive_function()\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
