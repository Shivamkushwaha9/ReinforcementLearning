{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim), #output dim should be 1\n",
    "            #No Activation function here for now\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(rewards, discount_factor):\n",
    "    discounted_rewards = []\n",
    "    G = 0\n",
    "    for i in reversed(rewards):\n",
    "        G = i + discount_factor * G\n",
    "        discounted_rewards.insert(0, G)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "    \n",
    "    return (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e+9)\n",
    "\n",
    "#Continous false abhi ke liye, Might try continuous space too after implementing AC\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False)\n",
    "acid = env.observation_space.shape[0]\n",
    "acod = env.action_space.n\n",
    "actor = Actor(acid, acod)\n",
    "critic = Critic(acid, 1)\n",
    "gamma= 0.99\n",
    "optimizer_actor = torch.optim.Adam(actor.parameters(), lr=0.0003)\n",
    "optimizer_critic = torch.optim.Adam(critic.parameters(), lr=0.001)\n",
    "n_episodes = 5000\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "total_rewards= []\n",
    "for episode in range(n_episodes):\n",
    "    # if(episode>1500):\n",
    "    #     env = gym.make(\"LunarLander-v3\", continuous=False, render_mode='human')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    log_probs = [] #[tensor([-1.5057], grad_fn=<SqueezeBackward1>), tensor([-1.3106], grad_fn=<SqueezeBackward1>),..........(steps count tak)]\n",
    "    values = []    #[tensor([[-0.1408]], grad_fn=<AddmmBackward0>), tensor([[-0.1340]], grad_fn=<AddmmBackward0>),..........(steps count tak)]\n",
    "    rewards = []   #[np.float64(-0.9220499698757294), np.float64(-1.5198324317463403),..........(steps count tak)]\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Actor policy\n",
    "        action_probs = actor(state)\n",
    "        distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action)\n",
    "        \n",
    "        # Critic value\n",
    "        value = critic(state)\n",
    "        \n",
    "        # Take action\n",
    "        new_state, reward, done, truncated, _ = env.step(action.item())\n",
    "        \n",
    "        # Store data\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        done = done or truncated\n",
    "        state = new_state\n",
    "    \n",
    "    # print(log_probs) #[tensor([-1.5057], grad_fn=<SqueezeBackward1>), tensor([-1.3106], grad_fn=<SqueezeBackward1>),..........(steps count tak)]\n",
    "    # print(values)    #[tensor([[-0.1408]], grad_fn=<AddmmBackward0>), tensor([[-0.1340]], grad_fn=<AddmmBackward0>),..........(steps count tak)]\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    log_probs = torch.stack(log_probs)  #[[n1], [n2], [n3],... [n_steps]]\n",
    "    values = torch.cat(values)          #[[n1], [n2], [n3],... [n_steps]]\n",
    "    \n",
    "    \n",
    "    # Calculate returns and advantages\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)  #[[n1], [n2], [n3],... [n_steps]]\n",
    "    returns = (returns - returns.mean())/returns.std() + 1e-9\n",
    "    \n",
    "    \n",
    "    # Calculate advantages (returns - values)\n",
    "    # [[n1], [n2], [n3],..[n_steps]] - [[n1], [n2], [n3],..[n_steps]]\n",
    "    advantages = returns - values  #A(s,a)=Q(s,a)âˆ’V(s)\n",
    "    \n",
    "    # print(len(rewards))\n",
    "    # print(advantages.shape)\n",
    "    \n",
    "    # Update critic\n",
    "    critic_loss = mse(values, returns)\n",
    "    optimizer_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    \n",
    "    # Update actor\n",
    "    actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "    optimizer_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "\n",
    "    \n",
    "    total_rewards.append(sum(rewards))\n",
    "    if(episode%100==0):\n",
    "        recent_rewards = total_rewards[-100] if(len(total_rewards)) >= 10 else total_rewards\n",
    "        avg_rewards = np.mean(recent_rewards)\n",
    "        print(f\"Episdoe : {episode} ::::::: Average reward {avg_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a policy network and a critic (value function)\n",
    "torch.save({\n",
    "    'policy_state_dict': actor.state_dict(),\n",
    "    'critic_state_dict': critic.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_actor.state_dict(),  # Optional\n",
    "    'optimizer_state_dict_critic': optimizer_critic.state_dict(),  # Optional\n",
    "}, 'model_checkpoint.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
