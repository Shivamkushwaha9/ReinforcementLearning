{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[ 4.67533303e+00  4.29228833e+00  5.49539000e+00  4.58529499e+00]\n",
      " [ 2.47983150e-01  1.35288005e-03  5.92236733e+00 -6.77918488e-02]\n",
      " [-3.93238000e-02 -3.61489009e-02  1.70303288e+00  4.80690858e-01]\n",
      " [-4.57390000e-02 -2.97010000e-02 -2.88100000e-02  1.48363951e-01]\n",
      " [ 4.60093369e+00  5.58326330e+00  6.21710000e+00  5.05158574e+00]\n",
      " [ 8.72202184e-01  1.71502883e+00  7.01673654e+00  1.47300862e+00]\n",
      " [ 2.13741706e-01  6.23153636e-01  7.73785021e+00 -2.07910000e-02]\n",
      " [-2.97010000e-02  7.20810422e-01  8.59129432e+00  9.85348164e-01]\n",
      " [ 5.22316247e+00  7.01900000e+00  3.25874087e+00  5.11374720e+00]\n",
      " [ 5.57876186e+00  7.91000000e+00  3.79603638e+00  5.87889432e+00]\n",
      " [ 6.04890140e+00  8.90000000e+00  6.42814031e+00  6.78282606e+00]\n",
      " [ 6.72081050e+00  8.25808822e+00  1.00000000e+01  7.57443207e+00]\n",
      " [ 5.49438140e+00 -2.88100000e-02  5.93994777e-01 -2.97010000e-02]\n",
      " [-1.40805550e-02  6.11094040e+00 -1.00000000e-02  4.32006317e-01]\n",
      " [ 7.88087761e+00  3.43900000e+00  6.61031693e-01  5.21497607e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Q-learning algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        - num_states: Number of states in the environment\n",
    "        - num_actions: Number of possible actions\n",
    "        - learning_rate: Alpha, how much we update our estimate each step\n",
    "        - discount_factor: Gamma, how much we value future rewards\n",
    "        - exploration_rate: Epsilon, probability of random exploration\n",
    "        \"\"\"\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if random.random() < self.exploration_rate:\n",
    "            # Explore: random action\n",
    "            return random.randint(0, self.q_table.shape[1] - 1)\n",
    "        else:\n",
    "            # Exploit: best known action\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update Q-value using the Bellman equation\n",
    "        Q(s,a) = Q(s,a) + α * (r + γ * max_a' Q(s',a') - Q(s,a))\n",
    "        \"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_STATES = 16  # 4x4 grid\n",
    "    NUM_ACTIONS = 4  # up, down, left, right\n",
    "    \n",
    "    q_agent = QLearning(NUM_STATES, NUM_ACTIONS)\n",
    "    \n",
    "    for episode in range(1000):\n",
    "        state = 0  # start state\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action\n",
    "            action = q_agent.get_action(state)\n",
    "            \n",
    "            # Simulate environment (in a real scenario, this would come from the environment)\n",
    "            # For this example, let's say:\n",
    "            # - Action 0=up, 1=right, 2=down, 3=left\n",
    "            # - State is represented as 0-15 (4x4 grid)\n",
    "            # - Reaching state 15 is the goal with reward +10\n",
    "            # - Hitting walls keeps in same state with -1 reward\n",
    "            \n",
    "            # Simple environment dynamics\n",
    "            if state == 15:  # goal state\n",
    "                reward = 0\n",
    "                done = True\n",
    "                next_state = state\n",
    "            else:\n",
    "                if action == 0:  # up\n",
    "                    next_state = state - 4 if state >= 4 else state\n",
    "                elif action == 1:  # right\n",
    "                    next_state = state + 1 if (state + 1) % 4 != 0 else state\n",
    "                elif action == 2:  # down\n",
    "                    next_state = state + 4 if state < 12 else state\n",
    "                else:  # left\n",
    "                    next_state = state - 1 if state % 4 != 0 else state\n",
    "                \n",
    "                reward = 10 if next_state == 15 else -0.1  # small penalty for each step\n",
    "            \n",
    "            # Update Q-value\n",
    "            q_agent.update_q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    \n",
    "    # Print learned Q-table\n",
    "    print(\"Learned Q-table:\")\n",
    "    print(q_agent.q_table)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
