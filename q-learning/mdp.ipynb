{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " transition_probabilities = [  # shape=[s, a, s']\n",
    "                                [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "                                [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "                                [None, [0.8, 0.1, 0.1], None]\n",
    "                            ]\n",
    " rewards = [  # shape=[s, a, s']\n",
    "            [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "            [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "            [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
    "        ]\n",
    " # shape = [s]\n",
    " possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-value Iteration Algorithm :)\n",
    "Q_{k+1}(s,a) = ∑_{s'} T(s,a,s') * [R(s,a,s') + γ * max_{a'} Q_k(s',a')] for all s,a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.full((3,3), -np.inf)\n",
    "for state, action in enumerate(possible_actions):\n",
    "    q_values[state][action] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.90\n",
    "for iteration in range(50):\n",
    "    q_prev = q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            q_values[s][a] = np.sum([transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * q_prev[sp].max()) for sp in range(3)]) \n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "#creating the gym env\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4,32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32,32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "# list(model.parameters())\n",
    "shapes = [param.shape for param in model.parameters()]\n",
    "print(shapes)\n",
    "\n",
    "input_shape = [4]  # == env.observation_space.shape\n",
    "n_outputs = 2  # == env.action_space.n\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if(np.random.rand()<epsilon):\n",
    "        return np.random.randint(model.model[-1].out_features) # (model.model[-1].out_features) = 2 \n",
    "    else:\n",
    "        Q_val = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "        # return Q_val.argmax(dim=1).item()\n",
    "        return Q_val\n",
    "state = np.array([0.1, 0.2, 0.3, 0.8])  # Example state\n",
    "action = epsilon_greedy_policy(state, epsilon=0.1)\n",
    "print(action)\n",
    "\n",
    "from collections import deque\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "#Each expereince consists of -> [s, a, r, s', done, truncated]\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    \"\"\"\n",
    "    It will just sample expereince from replay buffer\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): self_explanatory\n",
    "\n",
    "    Returns:\n",
    "        Here \n",
    "            1. We could directly return BATCH which would have following shape (batch_size, 6) for example:\n",
    "            \n",
    "                [\n",
    "                    ([1, 2, 3, 4], 0, 1.0, [5, 6, 7, 8], False, False),    # Experience 1\n",
    "                    ([17, 18, 19, 20], 0, -1.0, [21, 22, 23, 24], False, True), # Experience 2\n",
    "                    ([33, 34, 35, 36], 0, 0.0, [37, 38, 39, 40], True, True)    # Experience 3\n",
    "                ]\n",
    "            \n",
    "            2. We are here returning all s together, then all a together then all rewards and so on, eg:\n",
    "            \n",
    "                states      : [[1, 2, 3, 4], [17, 18, 19, 20], [33, 34, 35, 36]]\n",
    "                actions\t    : [0, 0, 0]\n",
    "                rewards\t    : [1.0, -1.0, 0.0]\n",
    "                next_states\t: [[5, 6, 7, 8], [21, 22, 23, 24], [37, 38, 39, 40]]\n",
    "                dones       : [False, False, True]\n",
    "                truncated   : [False, True, True]\n",
    "                \n",
    "    \"\"\"\n",
    "    \n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    return [np.array([experience[field_index] for experience in batch]) for field_index in range(6)]\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    #taking action in a particular environemnt\n",
    "    next_state, reward, done, truncated, info = env.step(action=action)\n",
    "    #entering elements in deque\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    return next_state, reward, done, truncated, info  \n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    \n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "    truncateds = torch.FloatTensor(truncateds)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #******************* Vaiii this is not trainable-> Just predicting ********************\n",
    "        next_Q_values = model(next_states)\n",
    "        \n",
    "        max_next_Q_values, _ = next_Q_values.max(dim=1)\n",
    "        \n",
    "        # Ensure dones and truncateds are boolean tensors\n",
    "        if not dones.dtype == torch.bool:\n",
    "            dones = dones.bool()\n",
    "        if not truncateds.dtype == torch.bool:\n",
    "            truncateds = truncateds.bool()\n",
    "            \n",
    "        # Now perform the logical OR and convert to float\n",
    "        terminal = dones | truncateds\n",
    "        runs = 1.0 - terminal.float()\n",
    "        \n",
    "        target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "        target_Q_values = target_Q_values.unsqueeze(1)\n",
    "    \n",
    "    mask = F.one_hot(actions, num_classes=n_outputs)\n",
    "    \n",
    "    # This one is trainable here\n",
    "    all_Q_values = model(states)\n",
    "    Q_values = torch.sum(all_Q_values * mask, dim=1, keepdims=True)\n",
    "    \n",
    "    loss = loss_fn(Q_values, target_Q_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "for episode in range(600):\n",
    "    obs, info = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
