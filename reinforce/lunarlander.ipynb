{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Average Reward: -290.14\n",
      "Loss :  tensor(1.2699, grad_fn=<AddBackward0>)\n",
      "Episode 10 Average Reward: -218.79\n",
      "Loss :  tensor(1.2367, grad_fn=<AddBackward0>)\n",
      "Episode 20 Average Reward: -184.33\n",
      "Loss :  tensor(-0.2658, grad_fn=<AddBackward0>)\n",
      "Episode 30 Average Reward: -193.45\n",
      "Loss :  tensor(-1.1242, grad_fn=<AddBackward0>)\n",
      "Episode 40 Average Reward: -188.20\n",
      "Loss :  tensor(0.6439, grad_fn=<AddBackward0>)\n",
      "Episode 50 Average Reward: -182.53\n",
      "Loss :  tensor(0.6738, grad_fn=<AddBackward0>)\n",
      "Episode 60 Average Reward: -183.64\n",
      "Loss :  tensor(-1.2811, grad_fn=<AddBackward0>)\n",
      "Episode 70 Average Reward: -182.06\n",
      "Loss :  tensor(0.2771, grad_fn=<AddBackward0>)\n",
      "Episode 80 Average Reward: -177.81\n",
      "Loss :  tensor(1.4707, grad_fn=<AddBackward0>)\n",
      "Episode 90 Average Reward: -177.93\n",
      "Loss :  tensor(-0.4967, grad_fn=<AddBackward0>)\n",
      "Episode 100 Average Reward: -178.74\n",
      "Loss :  tensor(1.3633, grad_fn=<AddBackward0>)\n",
      "Episode 110 Average Reward: -167.99\n",
      "Loss :  tensor(-0.5920, grad_fn=<AddBackward0>)\n",
      "Episode 120 Average Reward: -137.09\n",
      "Loss :  tensor(0.4275, grad_fn=<AddBackward0>)\n",
      "Episode 130 Average Reward: -162.08\n",
      "Loss :  tensor(0.0791, grad_fn=<AddBackward0>)\n",
      "Episode 140 Average Reward: -139.63\n",
      "Loss :  tensor(0.0562, grad_fn=<AddBackward0>)\n",
      "Episode 150 Average Reward: -154.30\n",
      "Loss :  tensor(-1.1008, grad_fn=<AddBackward0>)\n",
      "Episode 160 Average Reward: -140.35\n",
      "Loss :  tensor(-1.7559, grad_fn=<AddBackward0>)\n",
      "Episode 170 Average Reward: -214.82\n",
      "Loss :  tensor(-5.4239, grad_fn=<AddBackward0>)\n",
      "Episode 180 Average Reward: -116.80\n",
      "Loss :  tensor(2.5083, grad_fn=<AddBackward0>)\n",
      "Episode 190 Average Reward: -162.65\n",
      "Loss :  tensor(-1.8824, grad_fn=<AddBackward0>)\n",
      "Episode 200 Average Reward: -128.38\n",
      "Loss :  tensor(-3.6652, grad_fn=<AddBackward0>)\n",
      "Episode 210 Average Reward: -195.31\n",
      "Loss :  tensor(0.9287, grad_fn=<AddBackward0>)\n",
      "Episode 220 Average Reward: -103.11\n",
      "Loss :  tensor(0.8631, grad_fn=<AddBackward0>)\n",
      "Episode 230 Average Reward: -201.62\n",
      "Loss :  tensor(-1.8886, grad_fn=<AddBackward0>)\n",
      "Episode 240 Average Reward: -91.65\n",
      "Loss :  tensor(-1.4915, grad_fn=<AddBackward0>)\n",
      "Episode 250 Average Reward: -80.50\n",
      "Loss :  tensor(1.6699, grad_fn=<AddBackward0>)\n",
      "Episode 260 Average Reward: -132.60\n",
      "Loss :  tensor(1.2552, grad_fn=<AddBackward0>)\n",
      "Episode 270 Average Reward: -148.24\n",
      "Loss :  tensor(-2.9926, grad_fn=<AddBackward0>)\n",
      "Episode 280 Average Reward: -120.82\n",
      "Loss :  tensor(-2.9463, grad_fn=<AddBackward0>)\n",
      "Episode 290 Average Reward: -107.38\n",
      "Loss :  tensor(-0.7685, grad_fn=<AddBackward0>)\n",
      "Episode 300 Average Reward: -121.69\n",
      "Loss :  tensor(-2.6917, grad_fn=<AddBackward0>)\n",
      "Episode 310 Average Reward: -92.82\n",
      "Loss :  tensor(-3.5304, grad_fn=<AddBackward0>)\n",
      "Episode 320 Average Reward: -95.15\n",
      "Loss :  tensor(-7.1887, grad_fn=<AddBackward0>)\n",
      "Episode 330 Average Reward: -114.54\n",
      "Loss :  tensor(-0.8129, grad_fn=<AddBackward0>)\n",
      "Episode 340 Average Reward: -75.21\n",
      "Loss :  tensor(-0.9386, grad_fn=<AddBackward0>)\n",
      "Episode 350 Average Reward: -123.38\n",
      "Loss :  tensor(-8.8451, grad_fn=<AddBackward0>)\n",
      "Episode 360 Average Reward: -124.17\n",
      "Loss :  tensor(23.7843, grad_fn=<AddBackward0>)\n",
      "Episode 370 Average Reward: -148.08\n",
      "Loss :  tensor(0.8246, grad_fn=<AddBackward0>)\n",
      "Episode 380 Average Reward: -167.37\n",
      "Loss :  tensor(4.0473, grad_fn=<AddBackward0>)\n",
      "Episode 390 Average Reward: -179.49\n",
      "Loss :  tensor(10.1096, grad_fn=<AddBackward0>)\n",
      "Episode 400 Average Reward: -168.16\n",
      "Loss :  tensor(2.0011, grad_fn=<AddBackward0>)\n",
      "Episode 410 Average Reward: -126.37\n",
      "Loss :  tensor(-10.7917, grad_fn=<AddBackward0>)\n",
      "Episode 420 Average Reward: -74.37\n",
      "Loss :  tensor(5.1083, grad_fn=<AddBackward0>)\n",
      "Episode 430 Average Reward: -34.33\n",
      "Loss :  tensor(-3.1419, grad_fn=<AddBackward0>)\n",
      "Episode 440 Average Reward: -105.19\n",
      "Loss :  tensor(9.5850, grad_fn=<AddBackward0>)\n",
      "Episode 450 Average Reward: -111.68\n",
      "Loss :  tensor(6.1892, grad_fn=<AddBackward0>)\n",
      "Episode 460 Average Reward: -78.60\n",
      "Loss :  tensor(-0.6978, grad_fn=<AddBackward0>)\n",
      "Episode 470 Average Reward: -76.02\n",
      "Loss :  tensor(-6.2275, grad_fn=<AddBackward0>)\n",
      "Episode 480 Average Reward: -48.14\n",
      "Loss :  tensor(-9.4498, grad_fn=<AddBackward0>)\n",
      "Episode 490 Average Reward: -47.99\n",
      "Loss :  tensor(-10.5732, grad_fn=<AddBackward0>)\n",
      "Episode 500 Average Reward: -45.49\n",
      "Loss :  tensor(-28.9830, grad_fn=<AddBackward0>)\n",
      "Episode 510 Average Reward: -24.23\n",
      "Loss :  tensor(0.1437, grad_fn=<AddBackward0>)\n",
      "Episode 520 Average Reward: -100.70\n",
      "Loss :  tensor(7.9763, grad_fn=<AddBackward0>)\n",
      "Episode 530 Average Reward: -34.62\n",
      "Loss :  tensor(1.4280, grad_fn=<AddBackward0>)\n",
      "Episode 540 Average Reward: -38.48\n",
      "Loss :  tensor(12.3254, grad_fn=<AddBackward0>)\n",
      "Episode 550 Average Reward: -30.17\n",
      "Loss :  tensor(-2.7117, grad_fn=<AddBackward0>)\n",
      "Episode 560 Average Reward: -60.44\n",
      "Loss :  tensor(9.0889, grad_fn=<AddBackward0>)\n",
      "Episode 570 Average Reward: -48.99\n",
      "Loss :  tensor(1.4501, grad_fn=<AddBackward0>)\n",
      "Episode 580 Average Reward: -55.61\n",
      "Loss :  tensor(-5.0086, grad_fn=<AddBackward0>)\n",
      "Episode 590 Average Reward: -17.76\n",
      "Loss :  tensor(-5.0685, grad_fn=<AddBackward0>)\n",
      "Episode 600 Average Reward: 2.31\n",
      "Loss :  tensor(-10.1212, grad_fn=<AddBackward0>)\n",
      "Episode 610 Average Reward: -64.76\n",
      "Loss :  tensor(-1.5910, grad_fn=<AddBackward0>)\n",
      "Episode 620 Average Reward: -69.85\n",
      "Loss :  tensor(-9.4767, grad_fn=<AddBackward0>)\n",
      "Episode 630 Average Reward: 1.72\n",
      "Loss :  tensor(1.0117, grad_fn=<AddBackward0>)\n",
      "Episode 640 Average Reward: -7.88\n",
      "Loss :  tensor(-10.9277, grad_fn=<AddBackward0>)\n",
      "Episode 650 Average Reward: -19.72\n",
      "Loss :  tensor(-6.0736, grad_fn=<AddBackward0>)\n",
      "Episode 660 Average Reward: -3.47\n",
      "Loss :  tensor(7.0082, grad_fn=<AddBackward0>)\n",
      "Episode 670 Average Reward: -14.28\n",
      "Loss :  tensor(-7.5584, grad_fn=<AddBackward0>)\n",
      "Episode 680 Average Reward: -16.66\n",
      "Loss :  tensor(-5.2445, grad_fn=<AddBackward0>)\n",
      "Episode 690 Average Reward: -29.39\n",
      "Loss :  tensor(-4.2250, grad_fn=<AddBackward0>)\n",
      "Episode 700 Average Reward: -29.18\n",
      "Loss :  tensor(-0.0766, grad_fn=<AddBackward0>)\n",
      "Episode 710 Average Reward: -6.18\n",
      "Loss :  tensor(-4.2322, grad_fn=<AddBackward0>)\n",
      "Episode 720 Average Reward: -16.59\n",
      "Loss :  tensor(-10.1316, grad_fn=<AddBackward0>)\n",
      "Episode 730 Average Reward: -14.78\n",
      "Loss :  tensor(-18.8664, grad_fn=<AddBackward0>)\n",
      "Episode 740 Average Reward: -25.68\n",
      "Loss :  tensor(-35.8003, grad_fn=<AddBackward0>)\n",
      "Episode 750 Average Reward: 8.13\n",
      "Loss :  tensor(-10.8564, grad_fn=<AddBackward0>)\n",
      "Episode 760 Average Reward: -35.31\n",
      "Loss :  tensor(-14.9085, grad_fn=<AddBackward0>)\n",
      "Episode 770 Average Reward: 27.59\n",
      "Loss :  tensor(-80.1236, grad_fn=<AddBackward0>)\n",
      "Episode 780 Average Reward: 19.99\n",
      "Loss :  tensor(2.3629, grad_fn=<AddBackward0>)\n",
      "Episode 790 Average Reward: -1.08\n",
      "Loss :  tensor(-8.2819, grad_fn=<AddBackward0>)\n",
      "Episode 800 Average Reward: 19.58\n",
      "Loss :  tensor(-34.6210, grad_fn=<AddBackward0>)\n",
      "Episode 810 Average Reward: 25.97\n",
      "Loss :  tensor(1.1951, grad_fn=<AddBackward0>)\n",
      "Episode 820 Average Reward: 33.68\n",
      "Loss :  tensor(-32.2205, grad_fn=<AddBackward0>)\n",
      "Episode 830 Average Reward: 16.90\n",
      "Loss :  tensor(-54.2964, grad_fn=<AddBackward0>)\n",
      "Episode 840 Average Reward: 43.60\n",
      "Loss :  tensor(-48.6737, grad_fn=<AddBackward0>)\n",
      "Episode 850 Average Reward: -10.81\n",
      "Loss :  tensor(26.2485, grad_fn=<AddBackward0>)\n",
      "Episode 860 Average Reward: 15.01\n",
      "Loss :  tensor(6.4407, grad_fn=<AddBackward0>)\n",
      "Episode 870 Average Reward: 7.90\n",
      "Loss :  tensor(-41.5099, grad_fn=<AddBackward0>)\n",
      "Episode 880 Average Reward: 64.71\n",
      "Loss :  tensor(-37.1529, grad_fn=<AddBackward0>)\n",
      "Episode 890 Average Reward: 50.03\n",
      "Loss :  tensor(-22.0534, grad_fn=<AddBackward0>)\n",
      "Episode 900 Average Reward: 80.06\n",
      "Loss :  tensor(33.9158, grad_fn=<AddBackward0>)\n",
      "Episode 910 Average Reward: 53.07\n",
      "Loss :  tensor(52.3870, grad_fn=<AddBackward0>)\n",
      "Episode 920 Average Reward: 19.92\n",
      "Loss :  tensor(4.6194, grad_fn=<AddBackward0>)\n",
      "Episode 930 Average Reward: 22.15\n",
      "Loss :  tensor(-53.8554, grad_fn=<AddBackward0>)\n",
      "Episode 940 Average Reward: 68.83\n",
      "Loss :  tensor(-12.1503, grad_fn=<AddBackward0>)\n",
      "Episode 950 Average Reward: 75.60\n",
      "Loss :  tensor(-12.3832, grad_fn=<AddBackward0>)\n",
      "Episode 960 Average Reward: 87.83\n",
      "Loss :  tensor(-32.6220, grad_fn=<AddBackward0>)\n",
      "Episode 970 Average Reward: 72.77\n",
      "Loss :  tensor(-64.6087, grad_fn=<AddBackward0>)\n",
      "Episode 980 Average Reward: 61.78\n",
      "Loss :  tensor(-60.6746, grad_fn=<AddBackward0>)\n",
      "Episode 990 Average Reward: 79.57\n",
      "Loss :  tensor(-19.5566, grad_fn=<AddBackward0>)\n",
      "Episode 1000 Average Reward: 65.68\n",
      "Loss :  tensor(-67.2265, grad_fn=<AddBackward0>)\n",
      "Episode 1010 Average Reward: 103.90\n",
      "Loss :  tensor(-54.6744, grad_fn=<AddBackward0>)\n",
      "Episode 1020 Average Reward: 39.37\n",
      "Loss :  tensor(-10.6548, grad_fn=<AddBackward0>)\n",
      "Episode 1030 Average Reward: 89.76\n",
      "Loss :  tensor(27.6602, grad_fn=<AddBackward0>)\n",
      "Episode 1040 Average Reward: 79.57\n",
      "Loss :  tensor(18.5216, grad_fn=<AddBackward0>)\n",
      "Episode 1050 Average Reward: 89.45\n",
      "Loss :  tensor(25.5832, grad_fn=<AddBackward0>)\n",
      "Episode 1060 Average Reward: 77.10\n",
      "Loss :  tensor(-15.0247, grad_fn=<AddBackward0>)\n",
      "Episode 1070 Average Reward: 95.98\n",
      "Loss :  tensor(-16.8646, grad_fn=<AddBackward0>)\n",
      "Episode 1080 Average Reward: 98.46\n",
      "Loss :  tensor(20.2040, grad_fn=<AddBackward0>)\n",
      "Episode 1090 Average Reward: 47.86\n",
      "Loss :  tensor(-69.7396, grad_fn=<AddBackward0>)\n",
      "Episode 1100 Average Reward: 92.69\n",
      "Loss :  tensor(-72.4526, grad_fn=<AddBackward0>)\n",
      "Episode 1110 Average Reward: 91.06\n",
      "Loss :  tensor(-53.0483, grad_fn=<AddBackward0>)\n",
      "Episode 1120 Average Reward: 93.75\n",
      "Loss :  tensor(-68.6504, grad_fn=<AddBackward0>)\n",
      "Episode 1130 Average Reward: 72.73\n",
      "Loss :  tensor(-32.2974, grad_fn=<AddBackward0>)\n",
      "Episode 1140 Average Reward: 110.14\n",
      "Loss :  tensor(-54.4970, grad_fn=<AddBackward0>)\n",
      "Episode 1150 Average Reward: 126.73\n",
      "Loss :  tensor(-14.4025, grad_fn=<AddBackward0>)\n",
      "Episode 1160 Average Reward: 53.72\n",
      "Loss :  tensor(9.8381, grad_fn=<AddBackward0>)\n",
      "Episode 1170 Average Reward: 58.44\n",
      "Loss :  tensor(-8.5769, grad_fn=<AddBackward0>)\n",
      "Episode 1180 Average Reward: 93.83\n",
      "Loss :  tensor(7.0336, grad_fn=<AddBackward0>)\n",
      "Episode 1190 Average Reward: 96.38\n",
      "Loss :  tensor(-35.5956, grad_fn=<AddBackward0>)\n",
      "Episode 1200 Average Reward: 84.55\n",
      "Loss :  tensor(4.4094, grad_fn=<AddBackward0>)\n",
      "Episode 1210 Average Reward: 98.56\n",
      "Loss :  tensor(-113.4016, grad_fn=<AddBackward0>)\n",
      "Episode 1220 Average Reward: 50.19\n",
      "Loss :  tensor(-90.2712, grad_fn=<AddBackward0>)\n",
      "Episode 1230 Average Reward: 54.89\n",
      "Loss :  tensor(-185.0016, grad_fn=<AddBackward0>)\n",
      "Episode 1240 Average Reward: 7.71\n",
      "Loss :  tensor(-126.1500, grad_fn=<AddBackward0>)\n",
      "Episode 1250 Average Reward: 0.11\n",
      "Loss :  tensor(-33.7243, grad_fn=<AddBackward0>)\n",
      "Episode 1260 Average Reward: 100.36\n",
      "Loss :  tensor(24.8801, grad_fn=<AddBackward0>)\n",
      "Episode 1270 Average Reward: 180.95\n",
      "Loss :  tensor(50.9227, grad_fn=<AddBackward0>)\n",
      "Episode 1280 Average Reward: 68.52\n",
      "Loss :  tensor(13.1213, grad_fn=<AddBackward0>)\n",
      "Episode 1290 Average Reward: 161.61\n",
      "Loss :  tensor(-13.8003, grad_fn=<AddBackward0>)\n",
      "Episode 1300 Average Reward: 85.33\n",
      "Loss :  tensor(-5.3760, grad_fn=<AddBackward0>)\n",
      "Episode 1310 Average Reward: 57.02\n",
      "Loss :  tensor(19.7818, grad_fn=<AddBackward0>)\n",
      "Episode 1320 Average Reward: 150.61\n",
      "Loss :  tensor(-21.6579, grad_fn=<AddBackward0>)\n",
      "Episode 1330 Average Reward: 57.32\n",
      "Loss :  tensor(-142.1424, grad_fn=<AddBackward0>)\n",
      "Episode 1340 Average Reward: 158.64\n",
      "Loss :  tensor(1.2978, grad_fn=<AddBackward0>)\n",
      "Episode 1350 Average Reward: 85.27\n",
      "Loss :  tensor(7.3543, grad_fn=<AddBackward0>)\n",
      "Episode 1360 Average Reward: 51.63\n",
      "Loss :  tensor(11.0641, grad_fn=<AddBackward0>)\n",
      "Episode 1370 Average Reward: 99.79\n",
      "Loss :  tensor(-27.2979, grad_fn=<AddBackward0>)\n",
      "Episode 1380 Average Reward: 95.96\n",
      "Loss :  tensor(-22.2369, grad_fn=<AddBackward0>)\n",
      "Episode 1390 Average Reward: 12.18\n",
      "Loss :  tensor(-84.1698, grad_fn=<AddBackward0>)\n",
      "Episode 1400 Average Reward: 26.93\n",
      "Loss :  tensor(-15.0145, grad_fn=<AddBackward0>)\n",
      "Episode 1410 Average Reward: 140.68\n",
      "Loss :  tensor(-18.0963, grad_fn=<AddBackward0>)\n",
      "Episode 1420 Average Reward: 155.49\n",
      "Loss :  tensor(-3.4119, grad_fn=<AddBackward0>)\n",
      "Episode 1430 Average Reward: 155.47\n",
      "Loss :  tensor(-34.9495, grad_fn=<AddBackward0>)\n",
      "Episode 1440 Average Reward: 103.30\n",
      "Loss :  tensor(9.4764, grad_fn=<AddBackward0>)\n",
      "Episode 1450 Average Reward: 95.15\n",
      "Loss :  tensor(-9.4244, grad_fn=<AddBackward0>)\n",
      "Episode 1460 Average Reward: 128.94\n",
      "Loss :  tensor(-85.0014, grad_fn=<AddBackward0>)\n",
      "Episode 1470 Average Reward: 146.10\n",
      "Loss :  tensor(-6.0124, grad_fn=<AddBackward0>)\n",
      "Episode 1480 Average Reward: 94.88\n",
      "Loss :  tensor(7.5553, grad_fn=<AddBackward0>)\n",
      "Episode 1490 Average Reward: 128.40\n",
      "Loss :  tensor(1.6467, grad_fn=<AddBackward0>)\n",
      "Episode 1500 Average Reward: 59.33\n",
      "Loss :  tensor(-70.1254, grad_fn=<AddBackward0>)\n",
      "Episode 1510 Average Reward: 68.70\n",
      "Loss :  tensor(21.4300, grad_fn=<AddBackward0>)\n",
      "Episode 1520 Average Reward: 95.85\n",
      "Loss :  tensor(20.5883, grad_fn=<AddBackward0>)\n",
      "Episode 1530 Average Reward: 118.17\n",
      "Loss :  tensor(3.2605, grad_fn=<AddBackward0>)\n",
      "Episode 1540 Average Reward: 111.67\n",
      "Loss :  tensor(8.7787, grad_fn=<AddBackward0>)\n",
      "Episode 1550 Average Reward: 134.72\n",
      "Loss :  tensor(5.5047, grad_fn=<AddBackward0>)\n",
      "Episode 1560 Average Reward: 135.19\n",
      "Loss :  tensor(40.7870, grad_fn=<AddBackward0>)\n",
      "Episode 1570 Average Reward: 138.34\n",
      "Loss :  tensor(31.5859, grad_fn=<AddBackward0>)\n",
      "Episode 1580 Average Reward: 113.49\n",
      "Loss :  tensor(96.6787, grad_fn=<AddBackward0>)\n",
      "Episode 1590 Average Reward: 124.97\n",
      "Loss :  tensor(27.0684, grad_fn=<AddBackward0>)\n",
      "Episode 1600 Average Reward: 131.91\n",
      "Loss :  tensor(43.5917, grad_fn=<AddBackward0>)\n",
      "Episode 1610 Average Reward: 142.49\n",
      "Loss :  tensor(-18.7842, grad_fn=<AddBackward0>)\n",
      "Episode 1620 Average Reward: 142.40\n",
      "Loss :  tensor(27.8860, grad_fn=<AddBackward0>)\n",
      "Episode 1630 Average Reward: 107.91\n",
      "Loss :  tensor(-12.7364, grad_fn=<AddBackward0>)\n",
      "Episode 1640 Average Reward: 129.77\n",
      "Loss :  tensor(-27.5774, grad_fn=<AddBackward0>)\n",
      "Episode 1650 Average Reward: 124.57\n",
      "Loss :  tensor(-109.0557, grad_fn=<AddBackward0>)\n",
      "Episode 1660 Average Reward: 141.18\n",
      "Loss :  tensor(40.5746, grad_fn=<AddBackward0>)\n",
      "Episode 1670 Average Reward: 92.36\n",
      "Loss :  tensor(12.0254, grad_fn=<AddBackward0>)\n",
      "Episode 1680 Average Reward: 68.68\n",
      "Loss :  tensor(93.1154, grad_fn=<AddBackward0>)\n",
      "Episode 1690 Average Reward: 105.90\n",
      "Loss :  tensor(3.3778, grad_fn=<AddBackward0>)\n",
      "Episode 1700 Average Reward: 140.78\n",
      "Loss :  tensor(-49.8179, grad_fn=<AddBackward0>)\n",
      "Episode 1710 Average Reward: 130.33\n",
      "Loss :  tensor(-41.1939, grad_fn=<AddBackward0>)\n",
      "Episode 1720 Average Reward: 95.26\n",
      "Loss :  tensor(19.5036, grad_fn=<AddBackward0>)\n",
      "Episode 1730 Average Reward: 151.15\n",
      "Loss :  tensor(24.6827, grad_fn=<AddBackward0>)\n",
      "Episode 1740 Average Reward: 72.83\n",
      "Loss :  tensor(-9.3612, grad_fn=<AddBackward0>)\n",
      "Episode 1750 Average Reward: 61.24\n",
      "Loss :  tensor(-31.6303, grad_fn=<AddBackward0>)\n",
      "Episode 1760 Average Reward: 127.03\n",
      "Loss :  tensor(41.4658, grad_fn=<AddBackward0>)\n",
      "Episode 1770 Average Reward: 85.48\n",
      "Loss :  tensor(5.3543, grad_fn=<AddBackward0>)\n",
      "Episode 1780 Average Reward: 146.21\n",
      "Loss :  tensor(29.1538, grad_fn=<AddBackward0>)\n",
      "Episode 1790 Average Reward: 88.36\n",
      "Loss :  tensor(-44.1793, grad_fn=<AddBackward0>)\n",
      "Episode 1800 Average Reward: 80.85\n",
      "Loss :  tensor(15.5477, grad_fn=<AddBackward0>)\n",
      "Episode 1810 Average Reward: 135.38\n",
      "Loss :  tensor(-25.9904, grad_fn=<AddBackward0>)\n",
      "Episode 1820 Average Reward: 122.19\n",
      "Loss :  tensor(-14.3393, grad_fn=<AddBackward0>)\n",
      "Episode 1830 Average Reward: 178.59\n",
      "Loss :  tensor(8.0944, grad_fn=<AddBackward0>)\n",
      "Episode 1840 Average Reward: 106.90\n",
      "Loss :  tensor(16.1874, grad_fn=<AddBackward0>)\n",
      "Episode 1850 Average Reward: 167.65\n",
      "Loss :  tensor(44.4158, grad_fn=<AddBackward0>)\n",
      "Episode 1860 Average Reward: 187.00\n",
      "Loss :  tensor(-74.2821, grad_fn=<AddBackward0>)\n",
      "Episode 1870 Average Reward: 125.13\n",
      "Loss :  tensor(32.0502, grad_fn=<AddBackward0>)\n",
      "Episode 1880 Average Reward: 179.29\n",
      "Loss :  tensor(16.3428, grad_fn=<AddBackward0>)\n",
      "Episode 1890 Average Reward: 170.66\n",
      "Loss :  tensor(52.4160, grad_fn=<AddBackward0>)\n",
      "Episode 1900 Average Reward: 90.64\n",
      "Loss :  tensor(36.4744, grad_fn=<AddBackward0>)\n",
      "Episode 1910 Average Reward: 135.08\n",
      "Loss :  tensor(-1.3166, grad_fn=<AddBackward0>)\n",
      "Episode 1920 Average Reward: 144.36\n",
      "Loss :  tensor(20.4808, grad_fn=<AddBackward0>)\n",
      "Episode 1930 Average Reward: 119.52\n",
      "Loss :  tensor(-0.7676, grad_fn=<AddBackward0>)\n",
      "Episode 1940 Average Reward: 126.51\n",
      "Loss :  tensor(-5.5887, grad_fn=<AddBackward0>)\n",
      "Episode 1950 Average Reward: 89.17\n",
      "Loss :  tensor(11.9355, grad_fn=<AddBackward0>)\n",
      "Episode 1960 Average Reward: 71.82\n",
      "Loss :  tensor(-62.9742, grad_fn=<AddBackward0>)\n",
      "Episode 1970 Average Reward: 88.97\n",
      "Loss :  tensor(16.5810, grad_fn=<AddBackward0>)\n",
      "Episode 1980 Average Reward: 117.63\n",
      "Loss :  tensor(-101.4161, grad_fn=<AddBackward0>)\n",
      "Episode 1990 Average Reward: 57.42\n",
      "Loss :  tensor(-24.2138, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "\n",
    "#Policy Agent, Model\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "#Discounting and normalizing rewards\n",
    "def normalize_and_discount_rewards(rewards, gamma):\n",
    "    discounted_rewards=[]         #[R1, R2, R3, R4, ..... Rn]\n",
    "    G = 0\n",
    "    for i in reversed(rewards):\n",
    "        G = i + gamma*G\n",
    "        discounted_rewards.insert(0,G)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "    return (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) \n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
    "model = PolicyNetwork()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "n_episodes = 2000\n",
    "gamma = 0.98\n",
    "\n",
    "\n",
    "#Training loop\n",
    "episode_rewards = []\n",
    "episode_loss = []\n",
    "for episode in range(n_episodes):\n",
    "    \n",
    "    total_rewards = []\n",
    "    log_probs = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action_probs = model(state)\n",
    "        distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action)\n",
    "        new_state, reward, done, truncated, _ = env.step(action=action.item())\n",
    "        done = done or truncated\n",
    "        total_rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        state = new_state\n",
    "        \n",
    "    discounted_rewards = normalize_and_discount_rewards(total_rewards, gamma)\n",
    "    loss = 0\n",
    "    for log_prob, G in zip(log_probs, discounted_rewards):\n",
    "        loss += -log_prob * G #Reinforce Algorithm\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    episode_rewards.append(sum(total_rewards))\n",
    "    if episode % 10 == 0:\n",
    "        recent_rewards = episode_rewards[-10:] if len(episode_rewards) >= 100 else episode_rewards\n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        print(f\"Episode {episode} Average Reward: {avg_reward:.2f}\")\n",
    "        # recent_loss = episode_rewards[-10:] if len(episode_rewards) >= 100 else episode_rewards\n",
    "        print(\"Loss : \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rewards for EP :  1  :  170.35442957813893\n",
      "Total Rewards for EP :  2  :  165.11489393059844\n",
      "Total Rewards for EP :  3  :  208.796403278993\n",
      "Total Rewards for EP :  4  :  149.99610598673033\n",
      "Total Rewards for EP :  5  :  158.34640411410413\n",
      "Total Rewards for EP :  6  :  36.415441848172065\n",
      "Total Rewards for EP :  7  :  -45.175862911041264\n",
      "Total Rewards for EP :  8  :  52.82037163641531\n",
      "Total Rewards for EP :  9  :  163.01558822479626\n",
      "Total Rewards for EP :  10  :  -67.74137077323536\n",
      "Total Rewards for EP :  11  :  223.95715841516724\n",
      "Total Rewards for EP :  12  :  139.362018940756\n",
      "Total Rewards for EP :  13  :  250.92717772440514\n",
      "Total Rewards for EP :  14  :  79.24664969162576\n",
      "Total Rewards for EP :  15  :  32.64374273650543\n",
      "Total Rewards for EP :  16  :  135.08473036188562\n",
      "Total Rewards for EP :  17  :  -22.63212591500941\n",
      "Total Rewards for EP :  18  :  179.16582762447416\n",
      "Total Rewards for EP :  19  :  161.0527004904641\n",
      "Total Rewards for EP :  20  :  171.11727274668078\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode='human')\n",
    "    for episode in range(1, 21):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        \n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action_probs = model(state)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            # action = torch.argmax(action_probs)\n",
    "            new_state, reward, done, truncated, _ = env.step(action=action.item())\n",
    "            total_rewards+=reward\n",
    "            state=new_state\n",
    "            done = done or truncated\n",
    "            \n",
    "        print(\"Total Rewards for EP : \", episode, \" : \", total_rewards)\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHAT-GPT's\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4),\n",
    "            nn.Softmax(dim=-1)            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Initialize environment and network\n",
    "env = gym.make('LunarLander-v3')  # Corrected to standard Gymnasium environment\n",
    "# env = TimeLimit(env, max_episode_steps=200)\n",
    "policy = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "num_episodes = 10000  # Total episodes to train\n",
    "\n",
    "episode_rewards = []  # Track rewards for monitoring\n",
    "\n",
    "\n",
    "total_step = []\n",
    "# Training loop\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # Properly unpack the tuple returned by reset()\n",
    "    log_probs = []  # Store log probabilities of actions\n",
    "    rewards = []    # Store rewards\n",
    "    done = False\n",
    "\n",
    "    # Generate an episode\n",
    "    stepcount=0\n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        probs = policy(state_tensor)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Properly handle the five values returned by env.step()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated  # Episode ends if either terminated or truncated\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "    # total_step.append(stepcount)\n",
    "\n",
    "    # Compute discounted returns\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # Compute baseline and advantages\n",
    "    baseline = returns.mean()\n",
    "    advantages = returns - baseline\n",
    "    # Normalize advantages for stability\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # Compute policy loss\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    loss = -(log_probs * advantages).sum()\n",
    "\n",
    "    # Update policy\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record and print progress\n",
    "    total_reward = sum(rewards)\n",
    "    episode_rewards.append(total_reward)\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        print(f\"Episode {episode}, Reward: {total_reward:.2f}, Average Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "        print(f\" Max : {max(total_step)}, Average : {np.mean(total_step)}\")\n",
    "        total_step=[]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Looks like the agent is as clueless as I'm ;-;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is not working even after making updates for this env \n",
    "# let's visit it later(may be after a week? few weeks? some months later? all the best tho) and find out why It wouldn't work \n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "#In descrete space -> the actions will be either 0,1,2,3\n",
    "#In continous space -> the actions will be for main and side engines only with [1,1] jaha 1,1 ki values flunctuate hogi\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "               enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4),\n",
    "            nn.Softmax(dim=-1)\n",
    "            #nn.Softmax(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "model = SimpleModel()\n",
    "\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    # Convert observation to tensor\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Get action probabilities\n",
    "    action_probs = model(obs_tensor)\n",
    "    \n",
    "    # Sample action based on probabilities\n",
    "    action_dist = torch.distributions.Categorical(action_probs)\n",
    "    action = action_dist.sample()\n",
    "    \n",
    "    # Prepare target (one-hot encoded)\n",
    "    y_target = torch.zeros_like(action_probs)      # [[0,0,0,0]] \n",
    "    y_target[0, action] = 1.0                      # [[0,0,1,0]] -> one hot encoded for loss calculation\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(action_probs, y_target)\n",
    "    \n",
    "    # Compute gradient\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Take action in environment\n",
    "    action_int = int(action.item())\n",
    "    obs, reward, done, truncated, info = env.step(action_int)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = [param.grad.clone() for param in model.parameters()]\n",
    "    \n",
    "    return obs, reward, done, truncated, grads\n",
    "\n",
    "\n",
    "\n",
    "def play_multiple_episodes(env, n_max_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_max_episodes):\n",
    "        curr_rews = []\n",
    "        curr_grds = []\n",
    "        observation, info = env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            obs, reward, done, truncated, grads = play_one_step(env, observation, model, loss_fn)\n",
    "            curr_rews.append(reward)\n",
    "            curr_grds.append(grads)\n",
    "        all_rewards.append(curr_rews) \n",
    "        all_grads.append(curr_grds) \n",
    "    return all_rewards, all_grads\n",
    "\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for i in range(len(rewards)-2, -1, -1):\n",
    "        discounted[i] += discounted[i+1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(single_rew, discount_factor) for single_rew in all_rewards]\n",
    "    flattened_rewards = np.concatenate(all_discounted_rewards)\n",
    "    mean = flattened_rewards.mean()\n",
    "    std = flattened_rewards.std()\n",
    "    return [(rew - mean)/std for rew in all_discounted_rewards]\n",
    "\n",
    "\n",
    "n_iterations = 200\n",
    "n_episodes = 15\n",
    "n_steps_per_ep = 1000\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for iter in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes, n_steps_per_ep, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    \n",
    "    all_mean_grads = []\n",
    "    \n",
    "    for var_index, param in enumerate(model.parameters()):\n",
    "        mean_grad = torch.mean(\n",
    "            torch.stack([\n",
    "                final_rew * all_grads[episode_index][step][var_index]\n",
    "                for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                for step, final_rew in enumerate(final_rewards)\n",
    "            ])\n",
    "            ,dim=0\n",
    "        )\n",
    "        all_mean_grads.append(mean_grad)\n",
    "    \n",
    "    for param, mean_grads in zip(model.parameters(), all_mean_grads):\n",
    "        param.grad = mean_grads\n",
    "    optimizer.step()  # Update model parameters\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
